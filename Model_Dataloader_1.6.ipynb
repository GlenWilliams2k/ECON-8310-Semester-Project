{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "# Convert a video into a tensor\n",
    "def video_to_tensor(video_path, resize=None, frame_skip=1, return_orig_size = False):\n",
    "    print(f\"Loading video: {os.path.basename(video_path)}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "    orig_w = None\n",
    "    Orig_h = None\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if orig_w is None:\n",
    "            orig_h, orig_w = frame.shape[:2]\n",
    "        if resize:\n",
    "            frame = cv2.resize(frame, resize)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "\n",
    "    if not frames:\n",
    "        raise ValueError(f\"No frames read from {video_path}\")\n",
    "\n",
    "    frames = np.stack(frames)[::frame_skip]\n",
    "    frames = torch.from_numpy(frames).float().permute(0, 3, 1, 2) / 255.0\n",
    "    print(f\"Loaded {frame_count} frames -> kept {frames.shape[0]} after skipping\\n\")\n",
    "    \n",
    "    if return_orig_size:\n",
    "        return frames, (orig_w, orig_h)\n",
    "    else:\n",
    "        return frames  # shape: (T, 3, H, W)\n",
    "\n",
    "# Parse CVAT XML annotation file\n",
    "def parse_cvat_xml(xml_path, frame_skip=1, scale=None):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    annotations = defaultdict(list)\n",
    "\n",
    "    if scale is None:\n",
    "        sx = sy = 1.0\n",
    "    else:\n",
    "        sx, sy = scale\n",
    "\n",
    "    for track in root.findall(\"track\"):\n",
    "        label = track.attrib[\"label\"]\n",
    "        for box in track.findall(\"box\"):\n",
    "            frame = int(box.attrib[\"frame\"])\n",
    "            outside = int(box.attrib[\"outside\"])\n",
    "            if outside != 0:\n",
    "                continue\n",
    "\n",
    "            xtl = float(box.attrib[\"xtl\"])\n",
    "            ytl = float(box.attrib[\"ytl\"])\n",
    "            xbr = float(box.attrib[\"xbr\"])\n",
    "            ybr = float(box.attrib[\"ybr\"])\n",
    "\n",
    "            #scale bbox into resized frame coordinates if scale != 1\n",
    "            xtl *= sx\n",
    "            xbr *= sx\n",
    "            ytl *= sy\n",
    "            ybr *= sy\n",
    "\n",
    "            moving_attr = None\n",
    "            for attr in box.findall(\"attribute\"):\n",
    "                if attr.get(\"name\", \"\").lower() == \"moving\":\n",
    "                    moving_attr = attr\n",
    "                    break\n",
    "\n",
    "\n",
    "            moving_flag = 1 if moving_attr.text.lower() == \"true\" else 0\n",
    "            \n",
    "            #frame skip alignment\n",
    "            if frame % frame_skip == 0:\n",
    "                adjusted_frame = frame // frame_skip\n",
    "                annotations[adjusted_frame].append({\n",
    "                    \"label\": label,\n",
    "                    \"bbox\": [xtl, ytl, xbr, ybr],\n",
    "                    \"moving\": moving_flag})\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Dataset class that loads videos + XML annotations together\n",
    "class BaseballVideoDataset(Dataset):\n",
    "    def __init__(self, video_dir, xml_dir, resize=(1280, 720), frame_skip=1, scale_boxes=True):\n",
    "        self.video_dir = video_dir\n",
    "        self.xml_dir = xml_dir\n",
    "        self.resize = resize\n",
    "        self.frame_skip = frame_skip\n",
    "        self.scale_boxes = scale_boxes\n",
    "        self.video_tensors = {}\n",
    "        self.skipped_videos = []\n",
    "        self.index_map = []\n",
    "\n",
    "        # Match videos with their annotation XMLs by filename stem\n",
    "        self.samples = []\n",
    "        for vid_name in os.listdir(video_dir):\n",
    "            if vid_name.lower().endswith((\".mp4\", \".mov\", \".avi\")):\n",
    "                stem = os.path.splitext(vid_name)[0]\n",
    "                xml_path = os.path.join(xml_dir, f\"{stem}.xml\")\n",
    "                if os.path.exists(xml_path):\n",
    "                    self.samples.append((os.path.join(video_dir, vid_name), xml_path))\n",
    "                else:\n",
    "                    print(f\"No XML found for {vid_name}\")\n",
    "        print(f\"\\n Found {len(self.samples)} videos with matching XMLs in {video_dir}\\n\")\n",
    "\n",
    "        print(\"Preloading videos and indexing frames...\\n\")\n",
    "\n",
    "        # Loop through all matched video/XML pairs\n",
    "        for vid_idx, (video_path, xml_path) in enumerate(self.samples, start=1):\n",
    "            try:\n",
    "                video_tensor, (orig_w, orig_h) = video_to_tensor(video_path, resize=self.resize, frame_skip=self.frame_skip, return_orig_size = True)\n",
    "                annotations = parse_cvat_xml(xml_path, frame_skip=self.frame_skip)\n",
    "                self.video_tensors[video_path] = video_tensor\n",
    "            \n",
    "            #compute scale factors for bboxes\n",
    "                if self.resize is not None and self.scale_boxes:\n",
    "                    new_w, new_h = self.resize\n",
    "                    sx = new_w/float(orig_w)\n",
    "                    sy = new_h/float(orig_h)\n",
    "                    scale = (sx, sy)\n",
    "                else:\n",
    "                    scale = None\n",
    "\n",
    "                #parse annotations in scaled coordinates\n",
    "                annotations = parse_cvat_xml(xml_path, frame_skip = self.frame_skip, scale = scale)\n",
    "                self.video_tensors[video_path] = video_tensor\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {os.path.basename(video_path)}: {e}\")\n",
    "                self.skipped_videos.append((video_path, str(e)))\n",
    "                continue\n",
    "\n",
    "            # Build frame-by-frame index map\n",
    "            for frame_idx, ann_list in annotations.items():\n",
    "                if len(ann_list) == 0:\n",
    "                    continue\n",
    "                if frame_idx >= len(video_tensor):\n",
    "                    print(f\"Frame {frame_idx} out of range for {os.path.basename(video_path)} \"\n",
    "                          f\"(video has {len(video_tensor)} frames) — skipping\")\n",
    "                    continue\n",
    "\n",
    "                boxes = torch.tensor([a[\"bbox\"] for a in ann_list], dtype=torch.float32)\n",
    "                moving = torch.tensor([a[\"moving\"] for a in ann_list], dtype=torch.int64)\n",
    "\n",
    "                self.index_map.append((video_path, frame_idx, {\"boxes\": boxes,\"moving\": moving}))\n",
    "\n",
    "            print(f\"   [{vid_idx}/{len(self.samples)}] Loaded {os.path.basename(video_path)} \"\n",
    "                  f\"({len(video_tensor)} frames, {len(annotations)} annotated)\\n\")\n",
    "\n",
    "        print(f\"Finished indexing {len(self.index_map)} annotated frames \"\n",
    "              f\"from {len(self.video_tensors)} videos.\\n\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, frame_idx, target = self.index_map[idx]\n",
    "\n",
    "        video_tensor = self.video_tensors[video_path]\n",
    "        frame_tensor = video_tensor[frame_idx]  # (3, H, W)\n",
    "        \n",
    "        #convert flags to int, and configure labels for model interpretation\n",
    "        moving_flags = target[\"moving\"].to(torch.int64)\n",
    "        labels = moving_flags + 1\n",
    "\n",
    "        target_out = {\n",
    "            \"boxes\": target[\"boxes\"],\n",
    "            \"moving\": moving_flags,\n",
    "            \"labels\": labels,\n",
    "            \"video\": os.path.basename(video_path)}\n",
    "\n",
    "        return frame_tensor, target_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model, adjustable number of classes, not pretrained\n",
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def train_loop(model, dataloader, optimizer, device):\n",
    "    #set model to training mode\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
    "            images = [img.to(device) for img in images]\n",
    "            for t in targets:\n",
    "                t[\"boxes\"] = t[\"boxes\"].to(device)\n",
    "                t[\"labels\"] = t[\"labels\"].to(device)\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            #clear previous gradients\n",
    "            optimizer.zero_grad()\n",
    "            #compute new gradients via backpropagation\n",
    "            losses.backward()\n",
    "            #update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += losses.item()\n",
    "            if batch_idx % 5 == 0:\n",
    "                print(f\"Batch {batch_idx}/{len(dataloader)} | Loss: {losses.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss/len(dataloader)\n",
    "    print(f\"Average Training LossL: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "#evaluate the model\n",
    "@torch.no_grad() #prevent gradient updates\n",
    "def test_loop(model, dataloader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, targets in dataloader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        for t in targets:\n",
    "            t[\"boxes\"] = t[\"boxes\"].to(device)\n",
    "            t[\"labels\"] = t[\"labels\"].to(device)\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        total_loss += losses.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "#Function to measure model accuracy\n",
    "@torch.no_grad()\n",
    "def accuracy_loop(model, dataloader, device, score_thresh=0.5):\n",
    "    model.eval()\n",
    "    num_frames = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for images, targets in dataloader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        # move labels to device just in case\n",
    "        for t in targets:\n",
    "            t[\"labels\"] = t[\"labels\"].to(device)\n",
    "\n",
    "        predictions = model(images)\n",
    "\n",
    "        for pred, tgt in zip(predictions, targets):\n",
    "            # Ground truth: does this frame contain any moving object?\n",
    "            gt_any_moving = (tgt[\"labels\"] == 2).any().item()\n",
    "\n",
    "            # Predictions: keep only boxes above a confidence threshold\n",
    "            scores = pred[\"scores\"].to(device)\n",
    "            labels = pred[\"labels\"].to(device)\n",
    "            keep = scores >= score_thresh\n",
    "            pred_labels = labels[keep]\n",
    "\n",
    "            pred_any_moving = (pred_labels == 2).any().item()\n",
    "\n",
    "            num_frames += 1\n",
    "            if bool(gt_any_moving) == bool(pred_any_moving):\n",
    "                num_correct += 1\n",
    "\n",
    "    acc = num_correct / num_frames if num_frames > 0 else 0.0\n",
    "    print(f\"Frame-level moving/not-moving accuracy: {acc*100:.2f}% \"\n",
    "          f\"(threshold={score_thresh})\")\n",
    "    return acc\n",
    "\n",
    "def train_detector(train_dataset, val_dataset, num_classes=2, epochs=5, lr=1e-4, batch_size=4):\n",
    "    #use gpu if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = get_model(num_classes).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\\n----------------------------\")\n",
    "        train_loss = train_loop(model, train_loader, optimizer, device)\n",
    "        val_loss = test_loop(model, val_loader, device)\n",
    "        val_acc  = accuracy_loop(model, val_loader, device, score_thresh=0.5)\n",
    "        print(f\"Summary: train_loss={train_loss:.4f}, \" f\"val_loss={val_loss:.4f}, val_acc={val_acc*100:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No XML found for IMG_0078.mov\n",
      "No XML found for IMG_0084.mov\n",
      "\n",
      " Found 76 videos with matching XMLs in C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Raw Videos\n",
      "\n",
      "Preloading videos and indexing frames...\n",
      "\n",
      "Loading video: dusty_1.mov\n",
      "Loaded 76 frames -> kept 76 after skipping\n",
      "\n",
      "   [1/76] Loaded dusty_1.mov (76 frames, 76 annotated)\n",
      "\n",
      "Loading video: IMG_0030.mov\n",
      "Loaded 48 frames -> kept 48 after skipping\n",
      "\n",
      "   [2/76] Loaded IMG_0030.mov (48 frames, 3 annotated)\n",
      "\n",
      "Loading video: IMG_0031.mov\n",
      "Loaded 50 frames -> kept 50 after skipping\n",
      "\n",
      "   [3/76] Loaded IMG_0031.mov (50 frames, 2 annotated)\n",
      "\n",
      "Loading video: IMG_0032.mov\n",
      "Loaded 51 frames -> kept 51 after skipping\n",
      "\n",
      "   [4/76] Loaded IMG_0032.mov (51 frames, 3 annotated)\n",
      "\n",
      "Loading video: IMG_0033.mov\n",
      "Loaded 50 frames -> kept 50 after skipping\n",
      "\n",
      "   [5/76] Loaded IMG_0033.mov (50 frames, 3 annotated)\n",
      "\n",
      "Loading video: IMG_0034.mov\n",
      "Loaded 52 frames -> kept 52 after skipping\n",
      "\n",
      "   [6/76] Loaded IMG_0034.mov (52 frames, 3 annotated)\n",
      "\n",
      "Loading video: IMG_0035.mov\n",
      "Loaded 52 frames -> kept 52 after skipping\n",
      "\n",
      "   [7/76] Loaded IMG_0035.mov (52 frames, 2 annotated)\n",
      "\n",
      "Loading video: IMG_0036.mov\n",
      "Loaded 49 frames -> kept 49 after skipping\n",
      "\n",
      "   [8/76] Loaded IMG_0036.mov (49 frames, 2 annotated)\n",
      "\n",
      "Loading video: IMG_0037.mov\n",
      "Loaded 48 frames -> kept 48 after skipping\n",
      "\n",
      "   [9/76] Loaded IMG_0037.mov (48 frames, 2 annotated)\n",
      "\n",
      "Loading video: IMG_0038.mov\n",
      "Loaded 51 frames -> kept 51 after skipping\n",
      "\n",
      "   [10/76] Loaded IMG_0038.mov (51 frames, 3 annotated)\n",
      "\n",
      "Loading video: IMG_0039.mov\n",
      "Loaded 54 frames -> kept 54 after skipping\n",
      "\n",
      "   [11/76] Loaded IMG_0039.mov (54 frames, 3 annotated)\n",
      "\n",
      "Loading video: IMG_0040.mov\n",
      "Loaded 57 frames -> kept 57 after skipping\n",
      "\n",
      "   [12/76] Loaded IMG_0040.mov (57 frames, 57 annotated)\n",
      "\n",
      "Loading video: IMG_0041.mov\n",
      "Loaded 50 frames -> kept 50 after skipping\n",
      "\n",
      "   [13/76] Loaded IMG_0041.mov (50 frames, 50 annotated)\n",
      "\n",
      "Loading video: IMG_0042.mov\n",
      "Loaded 52 frames -> kept 52 after skipping\n",
      "\n",
      "   [14/76] Loaded IMG_0042.mov (52 frames, 52 annotated)\n",
      "\n",
      "Loading video: IMG_0043.mov\n",
      "Loaded 48 frames -> kept 48 after skipping\n",
      "\n",
      "   [15/76] Loaded IMG_0043.mov (48 frames, 48 annotated)\n",
      "\n",
      "Loading video: IMG_0044.mov\n",
      "Loaded 56 frames -> kept 56 after skipping\n",
      "\n",
      "   [16/76] Loaded IMG_0044.mov (56 frames, 56 annotated)\n",
      "\n",
      "Loading video: IMG_0045.mov\n",
      "Loaded 60 frames -> kept 60 after skipping\n",
      "\n",
      "   [17/76] Loaded IMG_0045.mov (60 frames, 60 annotated)\n",
      "\n",
      "Loading video: IMG_0046.mov\n",
      "Loaded 54 frames -> kept 54 after skipping\n",
      "\n",
      "   [18/76] Loaded IMG_0046.mov (54 frames, 54 annotated)\n",
      "\n",
      "Loading video: IMG_0047.mov\n",
      "Loaded 48 frames -> kept 48 after skipping\n",
      "\n",
      "   [19/76] Loaded IMG_0047.mov (48 frames, 48 annotated)\n",
      "\n",
      "Loading video: IMG_0074.mov\n",
      "Loaded 100 frames -> kept 100 after skipping\n",
      "\n",
      "   [20/76] Loaded IMG_0074.mov (100 frames, 100 annotated)\n",
      "\n",
      "Loading video: IMG_0075.mov\n",
      "Loaded 80 frames -> kept 80 after skipping\n",
      "\n",
      "   [21/76] Loaded IMG_0075.mov (80 frames, 80 annotated)\n",
      "\n",
      "Loading video: IMG_0076.mov\n",
      "Loaded 59 frames -> kept 59 after skipping\n",
      "\n",
      "   [22/76] Loaded IMG_0076.mov (59 frames, 59 annotated)\n",
      "\n",
      "Loading video: IMG_0077.mov\n",
      "Loaded 92 frames -> kept 92 after skipping\n",
      "\n",
      "   [23/76] Loaded IMG_0077.mov (92 frames, 92 annotated)\n",
      "\n",
      "Loading video: IMG_0079.mov\n",
      "Loaded 80 frames -> kept 80 after skipping\n",
      "\n",
      "   [24/76] Loaded IMG_0079.mov (80 frames, 80 annotated)\n",
      "\n",
      "Loading video: IMG_0083.mov\n",
      "Loaded 57 frames -> kept 57 after skipping\n",
      "\n",
      "   [25/76] Loaded IMG_0083.mov (57 frames, 57 annotated)\n",
      "\n",
      "Loading video: IMG_0085.mov\n",
      "Loaded 42 frames -> kept 42 after skipping\n",
      "\n",
      "   [26/76] Loaded IMG_0085.mov (42 frames, 42 annotated)\n",
      "\n",
      "Loading video: IMG_0170.mov\n",
      "Loaded 72 frames -> kept 72 after skipping\n",
      "\n",
      "   [27/76] Loaded IMG_0170.mov (72 frames, 72 annotated)\n",
      "\n",
      "Loading video: IMG_0171.mov\n",
      "Loaded 51 frames -> kept 51 after skipping\n",
      "\n",
      "   [28/76] Loaded IMG_0171.mov (51 frames, 51 annotated)\n",
      "\n",
      "Loading video: IMG_0172.mov\n",
      "Loaded 69 frames -> kept 69 after skipping\n",
      "\n",
      "   [29/76] Loaded IMG_0172.mov (69 frames, 69 annotated)\n",
      "\n",
      "Loading video: IMG_0173.mov\n",
      "Loaded 68 frames -> kept 68 after skipping\n",
      "\n",
      "   [30/76] Loaded IMG_0173.mov (68 frames, 2 annotated)\n",
      "\n",
      "Loading video: IMG_7917_dusty.mov\n",
      "Loaded 60 frames -> kept 60 after skipping\n",
      "\n",
      "   [31/76] Loaded IMG_7917_dusty.mov (60 frames, 60 annotated)\n",
      "\n",
      "Loading video: IMG_7918_dusty.mov\n",
      "Loaded 76 frames -> kept 76 after skipping\n",
      "\n",
      "   [32/76] Loaded IMG_7918_dusty.mov (76 frames, 76 annotated)\n",
      "\n",
      "Loading video: IMG_7919_dusty.mov\n",
      "Loaded 50 frames -> kept 50 after skipping\n",
      "\n",
      "   [33/76] Loaded IMG_7919_dusty.mov (50 frames, 50 annotated)\n",
      "\n",
      "Loading video: IMG_7942_dusty.mov\n",
      "Loaded 57 frames -> kept 57 after skipping\n",
      "\n",
      "   [34/76] Loaded IMG_7942_dusty.mov (57 frames, 57 annotated)\n",
      "\n",
      "Loading video: IMG_7943_khem.mov\n",
      "Loaded 51 frames -> kept 51 after skipping\n",
      "\n",
      "Frame 51 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
      "Frame 52 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
      "Frame 53 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
      "   [35/76] Loaded IMG_7943_khem.mov (51 frames, 54 annotated)\n",
      "\n",
      "Loading video: IMG_7944_khem.mov\n",
      "Loaded 54 frames -> kept 54 after skipping\n",
      "\n",
      "   [36/76] Loaded IMG_7944_khem.mov (54 frames, 37 annotated)\n",
      "\n",
      "Loading video: IMG_7997_khem.mov\n",
      "Loaded 37 frames -> kept 37 after skipping\n",
      "\n",
      "   [37/76] Loaded IMG_7997_khem.mov (37 frames, 37 annotated)\n",
      "\n",
      "Loading video: IMG_7998_khem.mov\n",
      "Loaded 55 frames -> kept 55 after skipping\n",
      "\n",
      "   [38/76] Loaded IMG_7998_khem.mov (55 frames, 55 annotated)\n",
      "\n",
      "Loading video: IMG_7999_joel.mov\n",
      "Loaded 56 frames -> kept 56 after skipping\n",
      "\n",
      "   [39/76] Loaded IMG_7999_joel.mov (56 frames, 56 annotated)\n",
      "\n",
      "Loading video: IMG_8027_joel.mov\n",
      "Loaded 54 frames -> kept 54 after skipping\n",
      "\n",
      "   [40/76] Loaded IMG_8027_joel.mov (54 frames, 54 annotated)\n",
      "\n",
      "Loading video: IMG_8028_joel.mov\n",
      "Loaded 53 frames -> kept 53 after skipping\n",
      "\n",
      "   [41/76] Loaded IMG_8028_joel.mov (53 frames, 53 annotated)\n",
      "\n",
      "Loading video: IMG_8029_joel.mov\n",
      "Loaded 54 frames -> kept 54 after skipping\n",
      "\n",
      "   [42/76] Loaded IMG_8029_joel.mov (54 frames, 54 annotated)\n",
      "\n",
      "Loading video: IMG_8030_patrick.mov\n",
      "Loaded 57 frames -> kept 57 after skipping\n",
      "\n",
      "Skipping IMG_8030_patrick.mov: 'NoneType' object has no attribute 'text'\n",
      "Loading video: IMG_8060_patrick.mov\n",
      "Loaded 58 frames -> kept 58 after skipping\n",
      "\n",
      "   [44/76] Loaded IMG_8060_patrick.mov (58 frames, 58 annotated)\n",
      "\n",
      "Loading video: IMG_8061_patrick.mov\n",
      "Loaded 54 frames -> kept 54 after skipping\n",
      "\n",
      "Skipping IMG_8061_patrick.mov: 'NoneType' object has no attribute 'text'\n",
      "Loading video: IMG_8062_patrick.mov\n",
      "Loaded 53 frames -> kept 53 after skipping\n",
      "\n",
      "Skipping IMG_8062_patrick.mov: 'NoneType' object has no attribute 'text'\n",
      "Loading video: IMG_8063_scott.mov\n",
      "Loaded 50 frames -> kept 50 after skipping\n",
      "\n",
      "   [47/76] Loaded IMG_8063_scott.mov (50 frames, 50 annotated)\n",
      "\n",
      "Loading video: IMG_8121_scott.mov\n",
      "Loaded 55 frames -> kept 55 after skipping\n",
      "\n",
      "   [48/76] Loaded IMG_8121_scott.mov (55 frames, 55 annotated)\n",
      "\n",
      "Loading video: IMG_8122_scott.mov\n",
      "Loaded 53 frames -> kept 53 after skipping\n",
      "\n",
      "   [49/76] Loaded IMG_8122_scott.mov (53 frames, 53 annotated)\n",
      "\n",
      "Loading video: IMG_8123_scott.mov\n",
      "Loaded 58 frames -> kept 58 after skipping\n",
      "\n",
      "   [50/76] Loaded IMG_8123_scott.mov (58 frames, 58 annotated)\n",
      "\n",
      "Loading video: IMG_8124_joe.mov\n",
      "Loaded 58 frames -> kept 58 after skipping\n",
      "\n",
      "   [51/76] Loaded IMG_8124_joe.mov (58 frames, 58 annotated)\n",
      "\n",
      "Loading video: IMG_8138_joe.mov\n",
      "Loaded 55 frames -> kept 55 after skipping\n",
      "\n",
      "   [52/76] Loaded IMG_8138_joe.mov (55 frames, 6 annotated)\n",
      "\n",
      "Loading video: IMG_8139_joe.mov\n",
      "Loaded 63 frames -> kept 63 after skipping\n",
      "\n",
      "   [53/76] Loaded IMG_8139_joe.mov (63 frames, 3 annotated)\n",
      "\n",
      "Loading video: IMG_8140_joe.mov\n",
      "Loaded 45 frames -> kept 45 after skipping\n",
      "\n",
      "   [54/76] Loaded IMG_8140_joe.mov (45 frames, 4 annotated)\n",
      "\n",
      "Loading video: IMG_8141_amarnath.mov\n",
      "Loaded 57 frames -> kept 57 after skipping\n",
      "\n",
      "   [55/76] Loaded IMG_8141_amarnath.mov (57 frames, 29 annotated)\n",
      "\n",
      "Loading video: IMG_8223_amarnath.mov\n",
      "Loaded 52 frames -> kept 52 after skipping\n",
      "\n",
      "   [56/76] Loaded IMG_8223_amarnath.mov (52 frames, 43 annotated)\n",
      "\n",
      "Loading video: IMG_8224_amarnath.mov\n",
      "Loaded 57 frames -> kept 57 after skipping\n",
      "\n",
      "   [57/76] Loaded IMG_8224_amarnath.mov (57 frames, 56 annotated)\n",
      "\n",
      "Loading video: IMG_8225_amarnath.mov\n",
      "Loaded 40 frames -> kept 40 after skipping\n",
      "\n",
      "   [58/76] Loaded IMG_8225_amarnath.mov (40 frames, 40 annotated)\n",
      "\n",
      "Loading video: IMG_8226_jared.mov\n",
      "Loaded 48 frames -> kept 48 after skipping\n",
      "\n",
      "   [59/76] Loaded IMG_8226_jared.mov (48 frames, 48 annotated)\n",
      "\n",
      "Loading video: IMG_8241_jared.mov\n",
      "Loaded 55 frames -> kept 55 after skipping\n",
      "\n",
      "   [60/76] Loaded IMG_8241_jared.mov (55 frames, 55 annotated)\n",
      "\n",
      "Loading video: IMG_8242_jared.mov\n",
      "Loaded 51 frames -> kept 51 after skipping\n",
      "\n",
      "   [61/76] Loaded IMG_8242_jared.mov (51 frames, 51 annotated)\n",
      "\n",
      "Loading video: IMG_8243_jared.mov\n",
      "Loaded 51 frames -> kept 51 after skipping\n",
      "\n",
      "   [62/76] Loaded IMG_8243_jared.mov (51 frames, 51 annotated)\n",
      "\n",
      "Loading video: IMG_8252_zach.mov\n",
      "Loaded 55 frames -> kept 55 after skipping\n",
      "\n",
      "Skipping IMG_8252_zach.mov: 'NoneType' object has no attribute 'text'\n",
      "Loading video: IMG_8255_zach.mov\n",
      "Loaded 44 frames -> kept 44 after skipping\n",
      "\n",
      "Skipping IMG_8255_zach.mov: 'NoneType' object has no attribute 'text'\n",
      "Loading video: IMG_8256_zach.mov\n",
      "Loaded 53 frames -> kept 53 after skipping\n",
      "\n",
      "Skipping IMG_8256_zach.mov: 'NoneType' object has no attribute 'text'\n",
      "Loading video: IMG_8257_zach.mov\n",
      "Loaded 57 frames -> kept 57 after skipping\n",
      "\n",
      "Skipping IMG_8257_zach.mov: 'NoneType' object has no attribute 'text'\n",
      "Loading video: IMG_8923_souleymane.mov\n",
      "Loaded 45 frames -> kept 45 after skipping\n",
      "\n",
      "   [67/76] Loaded IMG_8923_souleymane.mov (45 frames, 45 annotated)\n",
      "\n",
      "Loading video: IMG_8924_souleymane.mov\n",
      "Loaded 66 frames -> kept 66 after skipping\n",
      "\n",
      "   [68/76] Loaded IMG_8924_souleymane.mov (66 frames, 66 annotated)\n",
      "\n",
      "Loading video: IMG_8946_souleymane.mov\n",
      "Loaded 94 frames -> kept 94 after skipping\n",
      "\n",
      "   [69/76] Loaded IMG_8946_souleymane.mov (94 frames, 94 annotated)\n",
      "\n",
      "Loading video: IMG_8947_souleymane.mov\n",
      "Loaded 82 frames -> kept 82 after skipping\n",
      "\n",
      "   [70/76] Loaded IMG_8947_souleymane.mov (82 frames, 82 annotated)\n",
      "\n",
      "Loading video: IMG_9197_hugo.mov\n",
      "Loaded 70 frames -> kept 70 after skipping\n",
      "\n",
      "   [71/76] Loaded IMG_9197_hugo.mov (70 frames, 70 annotated)\n",
      "\n",
      "Loading video: IMG_9198_joel.mov\n",
      "Loaded 50 frames -> kept 50 after skipping\n",
      "\n",
      "   [72/76] Loaded IMG_9198_joel.mov (50 frames, 50 annotated)\n",
      "\n",
      "Loading video: IMG_9199_hugo.mov\n",
      "Loaded 81 frames -> kept 81 after skipping\n",
      "\n",
      "   [73/76] Loaded IMG_9199_hugo.mov (81 frames, 81 annotated)\n",
      "\n",
      "Loading video: IMG_9435_hugo.mov\n",
      "Loaded 66 frames -> kept 66 after skipping\n",
      "\n",
      "   [74/76] Loaded IMG_9435_hugo.mov (66 frames, 66 annotated)\n",
      "\n",
      "Loading video: IMG_9607_hugo.mov\n",
      "Loaded 43 frames -> kept 43 after skipping\n",
      "\n",
      "   [75/76] Loaded IMG_9607_hugo.mov (43 frames, 43 annotated)\n",
      "\n",
      "Loading video: IMG_9609_dusty.mov\n",
      "Loaded 102 frames -> kept 102 after skipping\n",
      "\n",
      "   [76/76] Loaded IMG_9609_dusty.mov (102 frames, 102 annotated)\n",
      "\n",
      "Finished indexing 3288 annotated frames from 69 videos.\n",
      "\n",
      "\n",
      "Dataset contains 3288 annotated frames across videos.\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Glen\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Glen\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "----------------------------\n",
      "Batch 0/658 | Loss: 2.9180\n",
      "Batch 5/658 | Loss: 0.9241\n",
      "Batch 10/658 | Loss: 0.5169\n",
      "Batch 15/658 | Loss: 0.5382\n",
      "Batch 20/658 | Loss: 0.6220\n",
      "Batch 25/658 | Loss: 0.6715\n",
      "Batch 30/658 | Loss: 0.5399\n",
      "Batch 35/658 | Loss: 0.4361\n",
      "Batch 40/658 | Loss: 1.0429\n",
      "Batch 45/658 | Loss: 0.4413\n",
      "Batch 50/658 | Loss: 0.5308\n",
      "Batch 55/658 | Loss: 0.2982\n",
      "Batch 60/658 | Loss: 0.4796\n",
      "Batch 65/658 | Loss: 0.4159\n",
      "Batch 70/658 | Loss: 0.6852\n",
      "Batch 75/658 | Loss: 0.3749\n",
      "Batch 80/658 | Loss: 0.3904\n",
      "Batch 85/658 | Loss: 0.5988\n",
      "Batch 90/658 | Loss: 0.7800\n",
      "Batch 95/658 | Loss: 0.4397\n",
      "Batch 100/658 | Loss: 0.4986\n",
      "Batch 105/658 | Loss: 0.0609\n",
      "Batch 110/658 | Loss: 0.4948\n",
      "Batch 115/658 | Loss: 0.6034\n",
      "Batch 120/658 | Loss: 0.3162\n",
      "Batch 125/658 | Loss: 0.5761\n",
      "Batch 130/658 | Loss: 0.6096\n",
      "Batch 135/658 | Loss: 0.3051\n",
      "Batch 140/658 | Loss: 0.7181\n",
      "Batch 145/658 | Loss: 0.7187\n",
      "Batch 150/658 | Loss: 0.2097\n",
      "Batch 155/658 | Loss: 0.6528\n",
      "Batch 160/658 | Loss: 0.4590\n",
      "Batch 165/658 | Loss: 0.5902\n",
      "Batch 170/658 | Loss: 0.5298\n",
      "Batch 175/658 | Loss: 0.3510\n",
      "Batch 180/658 | Loss: 0.4173\n",
      "Batch 185/658 | Loss: 0.9758\n",
      "Batch 190/658 | Loss: 0.3018\n",
      "Batch 195/658 | Loss: 0.4027\n",
      "Batch 200/658 | Loss: 0.2463\n",
      "Batch 205/658 | Loss: 0.6372\n",
      "Batch 210/658 | Loss: 0.4728\n",
      "Batch 215/658 | Loss: 0.5384\n",
      "Batch 220/658 | Loss: 0.6129\n",
      "Batch 225/658 | Loss: 0.2907\n",
      "Batch 230/658 | Loss: 0.7648\n",
      "Batch 235/658 | Loss: 0.1150\n",
      "Batch 240/658 | Loss: 0.2975\n",
      "Batch 245/658 | Loss: 0.3590\n",
      "Batch 250/658 | Loss: 0.6549\n",
      "Batch 255/658 | Loss: 0.3511\n",
      "Batch 260/658 | Loss: 0.3630\n",
      "Batch 265/658 | Loss: 0.5109\n",
      "Batch 270/658 | Loss: 0.3683\n",
      "Batch 275/658 | Loss: 0.3301\n",
      "Batch 280/658 | Loss: 0.1509\n",
      "Batch 285/658 | Loss: 0.3796\n",
      "Batch 290/658 | Loss: 0.2548\n",
      "Batch 295/658 | Loss: 0.1405\n",
      "Batch 300/658 | Loss: 0.4571\n",
      "Batch 305/658 | Loss: 0.4475\n",
      "Batch 310/658 | Loss: 0.3468\n",
      "Batch 315/658 | Loss: 0.1063\n",
      "Batch 320/658 | Loss: 0.2024\n",
      "Batch 325/658 | Loss: 0.1618\n",
      "Batch 330/658 | Loss: 0.2240\n",
      "Batch 335/658 | Loss: 0.2472\n",
      "Batch 340/658 | Loss: 0.0919\n",
      "Batch 345/658 | Loss: 0.6496\n",
      "Batch 350/658 | Loss: 0.2572\n",
      "Batch 355/658 | Loss: 0.0422\n",
      "Batch 360/658 | Loss: 0.1534\n",
      "Batch 365/658 | Loss: 0.2402\n",
      "Batch 370/658 | Loss: 0.3162\n",
      "Batch 375/658 | Loss: 0.4818\n",
      "Batch 380/658 | Loss: 0.2339\n",
      "Batch 385/658 | Loss: 0.3477\n",
      "Batch 390/658 | Loss: 0.5335\n",
      "Batch 395/658 | Loss: 0.5313\n",
      "Batch 400/658 | Loss: 0.4489\n",
      "Batch 405/658 | Loss: 0.3609\n",
      "Batch 410/658 | Loss: 0.1323\n",
      "Batch 415/658 | Loss: 0.2632\n",
      "Batch 420/658 | Loss: 0.3328\n",
      "Batch 425/658 | Loss: 0.2055\n",
      "Batch 430/658 | Loss: 0.3988\n",
      "Batch 435/658 | Loss: 0.3237\n",
      "Batch 440/658 | Loss: 0.2669\n",
      "Batch 445/658 | Loss: 0.1817\n",
      "Batch 450/658 | Loss: 0.4644\n",
      "Batch 455/658 | Loss: 0.1420\n",
      "Batch 460/658 | Loss: 0.3783\n",
      "Batch 465/658 | Loss: 0.6113\n",
      "Batch 470/658 | Loss: 0.2830\n",
      "Batch 475/658 | Loss: 0.1415\n",
      "Batch 480/658 | Loss: 0.3672\n",
      "Batch 485/658 | Loss: 0.1183\n",
      "Batch 490/658 | Loss: 0.4413\n",
      "Batch 495/658 | Loss: 0.2996\n",
      "Batch 500/658 | Loss: 0.3608\n",
      "Batch 505/658 | Loss: 0.6897\n",
      "Batch 510/658 | Loss: 0.3887\n",
      "Batch 515/658 | Loss: 0.0840\n",
      "Batch 520/658 | Loss: 0.0962\n",
      "Batch 525/658 | Loss: 0.2256\n",
      "Batch 530/658 | Loss: 0.3097\n",
      "Batch 535/658 | Loss: 0.3471\n",
      "Batch 540/658 | Loss: 0.5049\n",
      "Batch 545/658 | Loss: 0.1527\n",
      "Batch 550/658 | Loss: 0.0458\n",
      "Batch 555/658 | Loss: 0.2379\n",
      "Batch 560/658 | Loss: 0.3433\n",
      "Batch 565/658 | Loss: 0.5246\n",
      "Batch 570/658 | Loss: 0.6387\n",
      "Batch 575/658 | Loss: 0.1261\n",
      "Batch 580/658 | Loss: 0.3752\n",
      "Batch 585/658 | Loss: 0.3815\n",
      "Batch 590/658 | Loss: 0.1278\n",
      "Batch 595/658 | Loss: 0.3665\n",
      "Batch 600/658 | Loss: 0.3332\n",
      "Batch 605/658 | Loss: 0.5372\n",
      "Batch 610/658 | Loss: 0.5754\n",
      "Batch 615/658 | Loss: 0.5556\n",
      "Batch 620/658 | Loss: 0.5219\n",
      "Batch 625/658 | Loss: 0.4040\n",
      "Batch 630/658 | Loss: 0.2273\n",
      "Batch 635/658 | Loss: 0.4124\n",
      "Batch 640/658 | Loss: 0.3982\n",
      "Batch 645/658 | Loss: 0.1996\n",
      "Batch 650/658 | Loss: 0.1266\n",
      "Batch 655/658 | Loss: 0.7355\n",
      "Average Training LossL: 0.4142\n",
      "Validation Loss: 0.4090\n",
      "Frame-level moving/not-moving accuracy: 78.27% (threshold=0.5)\n",
      "Summary: train_loss=0.4142, val_loss=0.4090, val_acc=78.27%\n",
      "\n",
      "Epoch 2/10\n",
      "----------------------------\n",
      "Batch 0/658 | Loss: 0.3744\n",
      "Batch 5/658 | Loss: 0.5105\n",
      "Batch 10/658 | Loss: 0.2255\n",
      "Batch 15/658 | Loss: 0.3406\n",
      "Batch 20/658 | Loss: 0.3979\n",
      "Batch 25/658 | Loss: 0.3582\n",
      "Batch 30/658 | Loss: 0.3863\n",
      "Batch 35/658 | Loss: 0.3374\n",
      "Batch 40/658 | Loss: 0.4232\n",
      "Batch 45/658 | Loss: 0.3058\n",
      "Batch 50/658 | Loss: 0.3278\n",
      "Batch 55/658 | Loss: 0.4508\n",
      "Batch 60/658 | Loss: 0.3175\n",
      "Batch 65/658 | Loss: 0.0528\n",
      "Batch 70/658 | Loss: 0.4084\n",
      "Batch 75/658 | Loss: 0.4105\n",
      "Batch 80/658 | Loss: 0.1914\n",
      "Batch 85/658 | Loss: 0.4034\n",
      "Batch 90/658 | Loss: 0.0453\n",
      "Batch 95/658 | Loss: 0.2730\n",
      "Batch 100/658 | Loss: 0.4983\n",
      "Batch 105/658 | Loss: 0.3497\n",
      "Batch 110/658 | Loss: 0.2740\n",
      "Batch 115/658 | Loss: 0.1942\n",
      "Batch 120/658 | Loss: 0.6783\n",
      "Batch 125/658 | Loss: 0.3126\n",
      "Batch 130/658 | Loss: 0.1162\n",
      "Batch 135/658 | Loss: 0.3192\n",
      "Batch 140/658 | Loss: 0.0495\n",
      "Batch 145/658 | Loss: 0.3311\n",
      "Batch 150/658 | Loss: 0.2627\n",
      "Batch 155/658 | Loss: 0.4083\n",
      "Batch 160/658 | Loss: 0.0790\n",
      "Batch 165/658 | Loss: 0.2222\n",
      "Batch 170/658 | Loss: 0.2301\n",
      "Batch 175/658 | Loss: 0.3380\n",
      "Batch 180/658 | Loss: 0.1729\n",
      "Batch 185/658 | Loss: 0.3527\n",
      "Batch 190/658 | Loss: 0.2477\n",
      "Batch 195/658 | Loss: 0.2598\n",
      "Batch 200/658 | Loss: 0.1835\n",
      "Batch 205/658 | Loss: 0.2761\n",
      "Batch 210/658 | Loss: 0.3921\n",
      "Batch 215/658 | Loss: 0.3027\n",
      "Batch 220/658 | Loss: 0.3807\n",
      "Batch 225/658 | Loss: 0.6391\n",
      "Batch 230/658 | Loss: 0.3528\n",
      "Batch 235/658 | Loss: 0.4384\n",
      "Batch 240/658 | Loss: 0.2746\n",
      "Batch 245/658 | Loss: 0.1157\n",
      "Batch 250/658 | Loss: 0.2883\n",
      "Batch 255/658 | Loss: 0.3911\n",
      "Batch 260/658 | Loss: 0.3709\n",
      "Batch 265/658 | Loss: 0.4071\n",
      "Batch 270/658 | Loss: 0.1500\n",
      "Batch 275/658 | Loss: 0.5418\n",
      "Batch 280/658 | Loss: 0.1600\n",
      "Batch 285/658 | Loss: 0.3069\n",
      "Batch 290/658 | Loss: 0.3612\n",
      "Batch 295/658 | Loss: 0.4893\n",
      "Batch 300/658 | Loss: 0.2129\n",
      "Batch 305/658 | Loss: 0.2682\n",
      "Batch 310/658 | Loss: 0.1487\n",
      "Batch 315/658 | Loss: 0.1022\n",
      "Batch 320/658 | Loss: 0.4936\n",
      "Batch 325/658 | Loss: 0.2021\n",
      "Batch 330/658 | Loss: 0.1078\n",
      "Batch 335/658 | Loss: 0.4541\n",
      "Batch 340/658 | Loss: 0.1875\n",
      "Batch 345/658 | Loss: 0.5377\n",
      "Batch 350/658 | Loss: 0.2796\n",
      "Batch 355/658 | Loss: 0.3059\n",
      "Batch 360/658 | Loss: 0.1640\n",
      "Batch 365/658 | Loss: 0.1685\n",
      "Batch 370/658 | Loss: 0.4544\n",
      "Batch 375/658 | Loss: 0.3014\n",
      "Batch 380/658 | Loss: 0.0996\n",
      "Batch 385/658 | Loss: 0.4755\n",
      "Batch 390/658 | Loss: 0.4866\n",
      "Batch 395/658 | Loss: 0.3783\n",
      "Batch 400/658 | Loss: 0.1953\n",
      "Batch 405/658 | Loss: 0.1531\n",
      "Batch 410/658 | Loss: 0.2807\n",
      "Batch 415/658 | Loss: 0.5318\n",
      "Batch 420/658 | Loss: 0.3889\n",
      "Batch 425/658 | Loss: 0.4351\n",
      "Batch 430/658 | Loss: 0.1442\n",
      "Batch 435/658 | Loss: 0.3813\n",
      "Batch 440/658 | Loss: 0.2939\n",
      "Batch 445/658 | Loss: 0.4123\n",
      "Batch 450/658 | Loss: 0.3479\n",
      "Batch 455/658 | Loss: 0.0862\n",
      "Batch 460/658 | Loss: 0.2390\n",
      "Batch 465/658 | Loss: 0.1555\n",
      "Batch 470/658 | Loss: 0.3700\n",
      "Batch 475/658 | Loss: 0.0624\n",
      "Batch 480/658 | Loss: 0.2422\n",
      "Batch 485/658 | Loss: 0.0605\n",
      "Batch 490/658 | Loss: 0.0869\n",
      "Batch 495/658 | Loss: 0.4877\n",
      "Batch 500/658 | Loss: 0.1827\n",
      "Batch 505/658 | Loss: 0.1167\n",
      "Batch 510/658 | Loss: 0.1902\n",
      "Batch 515/658 | Loss: 0.2431\n",
      "Batch 520/658 | Loss: 0.4723\n",
      "Batch 525/658 | Loss: 0.3912\n",
      "Batch 530/658 | Loss: 0.1629\n",
      "Batch 535/658 | Loss: 0.1369\n",
      "Batch 540/658 | Loss: 0.2981\n",
      "Batch 545/658 | Loss: 0.0659\n",
      "Batch 550/658 | Loss: 0.1323\n",
      "Batch 555/658 | Loss: 0.1642\n",
      "Batch 560/658 | Loss: 0.1628\n",
      "Batch 565/658 | Loss: 0.0659\n",
      "Batch 570/658 | Loss: 0.2837\n",
      "Batch 575/658 | Loss: 0.1769\n",
      "Batch 580/658 | Loss: 0.1846\n",
      "Batch 585/658 | Loss: 0.4422\n",
      "Batch 590/658 | Loss: 0.3583\n",
      "Batch 595/658 | Loss: 0.2678\n",
      "Batch 600/658 | Loss: 0.5488\n",
      "Batch 605/658 | Loss: 0.1130\n",
      "Batch 610/658 | Loss: 0.3128\n",
      "Batch 615/658 | Loss: 0.1716\n",
      "Batch 620/658 | Loss: 0.1499\n",
      "Batch 625/658 | Loss: 0.2037\n",
      "Batch 630/658 | Loss: 0.0558\n",
      "Batch 635/658 | Loss: 0.2733\n",
      "Batch 640/658 | Loss: 0.5051\n",
      "Batch 645/658 | Loss: 0.0726\n",
      "Batch 650/658 | Loss: 0.2948\n",
      "Batch 655/658 | Loss: 0.4109\n",
      "Average Training LossL: 0.2963\n",
      "Validation Loss: 0.2710\n",
      "Frame-level moving/not-moving accuracy: 80.70% (threshold=0.5)\n",
      "Summary: train_loss=0.2963, val_loss=0.2710, val_acc=80.70%\n",
      "\n",
      "Epoch 3/10\n",
      "----------------------------\n",
      "Batch 0/658 | Loss: 0.2352\n",
      "Batch 5/658 | Loss: 0.2725\n",
      "Batch 10/658 | Loss: 0.2870\n",
      "Batch 15/658 | Loss: 0.2182\n",
      "Batch 20/658 | Loss: 0.3695\n",
      "Batch 25/658 | Loss: 0.0868\n",
      "Batch 30/658 | Loss: 0.3710\n",
      "Batch 35/658 | Loss: 0.2882\n",
      "Batch 40/658 | Loss: 0.2051\n",
      "Batch 45/658 | Loss: 0.0753\n",
      "Batch 50/658 | Loss: 0.1257\n",
      "Batch 55/658 | Loss: 0.3408\n",
      "Batch 60/658 | Loss: 0.4497\n",
      "Batch 65/658 | Loss: 0.4131\n",
      "Batch 70/658 | Loss: 0.3215\n",
      "Batch 75/658 | Loss: 0.2883\n",
      "Batch 80/658 | Loss: 0.2002\n",
      "Batch 85/658 | Loss: 0.4158\n",
      "Batch 90/658 | Loss: 0.3070\n",
      "Batch 95/658 | Loss: 0.3047\n",
      "Batch 100/658 | Loss: 0.2462\n",
      "Batch 105/658 | Loss: 0.0839\n",
      "Batch 110/658 | Loss: 0.4197\n",
      "Batch 115/658 | Loss: 0.1507\n",
      "Batch 120/658 | Loss: 0.3186\n",
      "Batch 125/658 | Loss: 0.3001\n",
      "Batch 130/658 | Loss: 0.1240\n",
      "Batch 135/658 | Loss: 0.3042\n",
      "Batch 140/658 | Loss: 0.3578\n",
      "Batch 145/658 | Loss: 0.0925\n",
      "Batch 150/658 | Loss: 0.3672\n",
      "Batch 155/658 | Loss: 0.1210\n",
      "Batch 160/658 | Loss: 0.2313\n",
      "Batch 165/658 | Loss: 0.2144\n",
      "Batch 170/658 | Loss: 0.2277\n",
      "Batch 175/658 | Loss: 0.2551\n",
      "Batch 180/658 | Loss: 0.1286\n",
      "Batch 185/658 | Loss: 0.1127\n",
      "Batch 190/658 | Loss: 0.2337\n",
      "Batch 195/658 | Loss: 0.1144\n",
      "Batch 200/658 | Loss: 0.2263\n",
      "Batch 205/658 | Loss: 0.2277\n",
      "Batch 210/658 | Loss: 0.1976\n",
      "Batch 215/658 | Loss: 0.1758\n",
      "Batch 220/658 | Loss: 0.3386\n",
      "Batch 225/658 | Loss: 0.1440\n",
      "Batch 230/658 | Loss: 0.5970\n",
      "Batch 235/658 | Loss: 0.3114\n",
      "Batch 240/658 | Loss: 0.2073\n",
      "Batch 245/658 | Loss: 0.2952\n",
      "Batch 250/658 | Loss: 0.2288\n",
      "Batch 255/658 | Loss: 0.3095\n",
      "Batch 260/658 | Loss: 0.1619\n",
      "Batch 265/658 | Loss: 0.0642\n",
      "Batch 270/658 | Loss: 0.2162\n",
      "Batch 275/658 | Loss: 0.2203\n",
      "Batch 280/658 | Loss: 0.2796\n",
      "Batch 285/658 | Loss: 0.3261\n",
      "Batch 290/658 | Loss: 0.1567\n",
      "Batch 295/658 | Loss: 0.2672\n",
      "Batch 300/658 | Loss: 0.2212\n",
      "Batch 305/658 | Loss: 0.3364\n",
      "Batch 310/658 | Loss: 0.3979\n",
      "Batch 315/658 | Loss: 0.1601\n",
      "Batch 320/658 | Loss: 0.2579\n",
      "Batch 325/658 | Loss: 0.2883\n",
      "Batch 330/658 | Loss: 0.3026\n",
      "Batch 335/658 | Loss: 0.3775\n",
      "Batch 340/658 | Loss: 0.0387\n",
      "Batch 345/658 | Loss: 0.3369\n",
      "Batch 350/658 | Loss: 0.2145\n",
      "Batch 355/658 | Loss: 0.3636\n",
      "Batch 360/658 | Loss: 0.2225\n",
      "Batch 365/658 | Loss: 0.2134\n",
      "Batch 370/658 | Loss: 0.2678\n",
      "Batch 375/658 | Loss: 0.5278\n",
      "Batch 380/658 | Loss: 0.1298\n",
      "Batch 385/658 | Loss: 0.0769\n",
      "Batch 390/658 | Loss: 0.1018\n",
      "Batch 395/658 | Loss: 0.3106\n",
      "Batch 400/658 | Loss: 0.1584\n",
      "Batch 405/658 | Loss: 0.2200\n",
      "Batch 410/658 | Loss: 0.3044\n",
      "Batch 415/658 | Loss: 0.3314\n",
      "Batch 420/658 | Loss: 0.1302\n",
      "Batch 425/658 | Loss: 0.3283\n",
      "Batch 430/658 | Loss: 0.1196\n",
      "Batch 435/658 | Loss: 0.3099\n",
      "Batch 440/658 | Loss: 0.1412\n",
      "Batch 445/658 | Loss: 0.3258\n",
      "Batch 450/658 | Loss: 0.2787\n",
      "Batch 455/658 | Loss: 0.2687\n",
      "Batch 460/658 | Loss: 0.2916\n",
      "Batch 465/658 | Loss: 0.0639\n",
      "Batch 470/658 | Loss: 0.2930\n",
      "Batch 475/658 | Loss: 0.2927\n",
      "Batch 480/658 | Loss: 0.1295\n",
      "Batch 485/658 | Loss: 0.2733\n",
      "Batch 490/658 | Loss: 0.0478\n",
      "Batch 495/658 | Loss: 0.2202\n",
      "Batch 500/658 | Loss: 0.0313\n",
      "Batch 505/658 | Loss: 0.3528\n",
      "Batch 510/658 | Loss: 0.2654\n",
      "Batch 515/658 | Loss: 0.1129\n",
      "Batch 520/658 | Loss: 0.2472\n",
      "Batch 525/658 | Loss: 0.1989\n",
      "Batch 530/658 | Loss: 0.2214\n",
      "Batch 535/658 | Loss: 0.2301\n",
      "Batch 540/658 | Loss: 0.3843\n",
      "Batch 545/658 | Loss: 0.3345\n",
      "Batch 550/658 | Loss: 0.1255\n",
      "Batch 555/658 | Loss: 0.2476\n",
      "Batch 560/658 | Loss: 0.0581\n",
      "Batch 565/658 | Loss: 0.1482\n",
      "Batch 570/658 | Loss: 0.2568\n",
      "Batch 575/658 | Loss: 0.2920\n",
      "Batch 580/658 | Loss: 0.1311\n",
      "Batch 585/658 | Loss: 0.2543\n",
      "Batch 590/658 | Loss: 0.3656\n",
      "Batch 595/658 | Loss: 0.1043\n",
      "Batch 600/658 | Loss: 0.1993\n",
      "Batch 605/658 | Loss: 0.1339\n",
      "Batch 610/658 | Loss: 0.1841\n",
      "Batch 615/658 | Loss: 0.1932\n",
      "Batch 620/658 | Loss: 0.4139\n",
      "Batch 625/658 | Loss: 0.3132\n",
      "Batch 630/658 | Loss: 0.2059\n",
      "Batch 635/658 | Loss: 0.1943\n",
      "Batch 640/658 | Loss: 0.3449\n",
      "Batch 645/658 | Loss: 0.3108\n",
      "Batch 650/658 | Loss: 0.4012\n",
      "Batch 655/658 | Loss: 0.5210\n",
      "Average Training LossL: 0.2479\n",
      "Validation Loss: 0.2735\n",
      "Frame-level moving/not-moving accuracy: 77.66% (threshold=0.5)\n",
      "Summary: train_loss=0.2479, val_loss=0.2735, val_acc=77.66%\n",
      "\n",
      "Epoch 4/10\n",
      "----------------------------\n",
      "Batch 0/658 | Loss: 0.2265\n",
      "Batch 5/658 | Loss: 0.2404\n",
      "Batch 10/658 | Loss: 0.2389\n",
      "Batch 15/658 | Loss: 0.3349\n",
      "Batch 20/658 | Loss: 0.1693\n",
      "Batch 25/658 | Loss: 0.1518\n",
      "Batch 30/658 | Loss: 0.2337\n",
      "Batch 35/658 | Loss: 0.3830\n",
      "Batch 40/658 | Loss: 0.1178\n",
      "Batch 45/658 | Loss: 0.2480\n",
      "Batch 50/658 | Loss: 0.0872\n",
      "Batch 55/658 | Loss: 0.0832\n",
      "Batch 60/658 | Loss: 0.3583\n",
      "Batch 65/658 | Loss: 0.3321\n",
      "Batch 70/658 | Loss: 0.3185\n",
      "Batch 75/658 | Loss: 0.1853\n",
      "Batch 80/658 | Loss: 0.2033\n",
      "Batch 85/658 | Loss: 0.2819\n",
      "Batch 90/658 | Loss: 0.1048\n",
      "Batch 95/658 | Loss: 0.1202\n",
      "Batch 100/658 | Loss: 0.3753\n",
      "Batch 105/658 | Loss: 0.1941\n",
      "Batch 110/658 | Loss: 0.3527\n",
      "Batch 115/658 | Loss: 0.1325\n",
      "Batch 120/658 | Loss: 0.0573\n",
      "Batch 125/658 | Loss: 0.1420\n",
      "Batch 130/658 | Loss: 0.0545\n",
      "Batch 135/658 | Loss: 0.0930\n",
      "Batch 140/658 | Loss: 0.4034\n",
      "Batch 145/658 | Loss: 0.1011\n",
      "Batch 150/658 | Loss: 0.1998\n",
      "Batch 155/658 | Loss: 0.2454\n",
      "Batch 160/658 | Loss: 0.3098\n",
      "Batch 165/658 | Loss: 0.2142\n",
      "Batch 170/658 | Loss: 0.0798\n",
      "Batch 175/658 | Loss: 0.2098\n",
      "Batch 180/658 | Loss: 0.2374\n",
      "Batch 185/658 | Loss: 0.5572\n",
      "Batch 190/658 | Loss: 0.2601\n",
      "Batch 195/658 | Loss: 0.1105\n",
      "Batch 200/658 | Loss: 0.0480\n",
      "Batch 205/658 | Loss: 0.1867\n",
      "Batch 210/658 | Loss: 0.1354\n",
      "Batch 215/658 | Loss: 0.3847\n",
      "Batch 220/658 | Loss: 0.2136\n",
      "Batch 225/658 | Loss: 0.1885\n",
      "Batch 230/658 | Loss: 0.0600\n",
      "Batch 235/658 | Loss: 0.1148\n",
      "Batch 240/658 | Loss: 0.2604\n",
      "Batch 245/658 | Loss: 0.1303\n",
      "Batch 250/658 | Loss: 0.3087\n",
      "Batch 255/658 | Loss: 0.2476\n",
      "Batch 260/658 | Loss: 0.2260\n",
      "Batch 265/658 | Loss: 0.1471\n",
      "Batch 270/658 | Loss: 0.1420\n",
      "Batch 275/658 | Loss: 0.1825\n",
      "Batch 280/658 | Loss: 0.3416\n",
      "Batch 285/658 | Loss: 0.0641\n",
      "Batch 290/658 | Loss: 0.2824\n",
      "Batch 295/658 | Loss: 0.1795\n",
      "Batch 300/658 | Loss: 0.2759\n",
      "Batch 305/658 | Loss: 0.1785\n",
      "Batch 310/658 | Loss: 0.2660\n",
      "Batch 315/658 | Loss: 0.2080\n",
      "Batch 320/658 | Loss: 0.1300\n",
      "Batch 325/658 | Loss: 0.1037\n",
      "Batch 330/658 | Loss: 0.4239\n",
      "Batch 335/658 | Loss: 0.2477\n",
      "Batch 340/658 | Loss: 0.2070\n",
      "Batch 345/658 | Loss: 0.0920\n",
      "Batch 350/658 | Loss: 0.2922\n",
      "Batch 355/658 | Loss: 0.0537\n",
      "Batch 360/658 | Loss: 0.2670\n",
      "Batch 365/658 | Loss: 0.2335\n",
      "Batch 370/658 | Loss: 0.0832\n",
      "Batch 375/658 | Loss: 0.2351\n",
      "Batch 380/658 | Loss: 0.2512\n",
      "Batch 385/658 | Loss: 0.1805\n",
      "Batch 390/658 | Loss: 0.1840\n",
      "Batch 395/658 | Loss: 0.1610\n",
      "Batch 400/658 | Loss: 0.1965\n",
      "Batch 405/658 | Loss: 0.2519\n",
      "Batch 410/658 | Loss: 0.1816\n",
      "Batch 415/658 | Loss: 0.3925\n",
      "Batch 420/658 | Loss: 0.0938\n",
      "Batch 425/658 | Loss: 0.2921\n",
      "Batch 430/658 | Loss: 0.3201\n",
      "Batch 435/658 | Loss: 0.1115\n",
      "Batch 440/658 | Loss: 0.1895\n",
      "Batch 445/658 | Loss: 0.2445\n",
      "Batch 450/658 | Loss: 0.0863\n",
      "Batch 455/658 | Loss: 0.1606\n",
      "Batch 460/658 | Loss: 0.3655\n",
      "Batch 465/658 | Loss: 0.1901\n",
      "Batch 470/658 | Loss: 0.2106\n",
      "Batch 475/658 | Loss: 0.0860\n",
      "Batch 480/658 | Loss: 0.1402\n",
      "Batch 485/658 | Loss: 0.3662\n",
      "Batch 490/658 | Loss: 0.2230\n",
      "Batch 495/658 | Loss: 0.2025\n",
      "Batch 500/658 | Loss: 0.1407\n",
      "Batch 505/658 | Loss: 0.3602\n",
      "Batch 510/658 | Loss: 0.0984\n",
      "Batch 515/658 | Loss: 0.3341\n",
      "Batch 520/658 | Loss: 0.1269\n",
      "Batch 525/658 | Loss: 0.2240\n",
      "Batch 530/658 | Loss: 0.3321\n",
      "Batch 535/658 | Loss: 0.2020\n",
      "Batch 540/658 | Loss: 0.2057\n",
      "Batch 545/658 | Loss: 0.3903\n",
      "Batch 550/658 | Loss: 0.2342\n",
      "Batch 555/658 | Loss: 0.3586\n",
      "Batch 560/658 | Loss: 0.2502\n",
      "Batch 565/658 | Loss: 0.4873\n",
      "Batch 570/658 | Loss: 0.3088\n",
      "Batch 575/658 | Loss: 0.3185\n",
      "Batch 580/658 | Loss: 0.1650\n",
      "Batch 585/658 | Loss: 0.0674\n",
      "Batch 590/658 | Loss: 0.2821\n",
      "Batch 595/658 | Loss: 0.0974\n",
      "Batch 600/658 | Loss: 0.3418\n",
      "Batch 605/658 | Loss: 0.2202\n",
      "Batch 610/658 | Loss: 0.2305\n",
      "Batch 615/658 | Loss: 0.2569\n",
      "Batch 620/658 | Loss: 0.1517\n",
      "Batch 625/658 | Loss: 0.2188\n",
      "Batch 630/658 | Loss: 0.2449\n",
      "Batch 635/658 | Loss: 0.0619\n",
      "Batch 640/658 | Loss: 0.1338\n",
      "Batch 645/658 | Loss: 0.1392\n",
      "Batch 650/658 | Loss: 0.2629\n",
      "Batch 655/658 | Loss: 0.2558\n",
      "Average Training LossL: 0.2151\n",
      "Validation Loss: 0.1986\n",
      "Frame-level moving/not-moving accuracy: 80.85% (threshold=0.5)\n",
      "Summary: train_loss=0.2151, val_loss=0.1986, val_acc=80.85%\n",
      "\n",
      "Epoch 5/10\n",
      "----------------------------\n",
      "Batch 0/658 | Loss: 0.0539\n",
      "Batch 5/658 | Loss: 0.2401\n",
      "Batch 10/658 | Loss: 0.4309\n",
      "Batch 15/658 | Loss: 0.0637\n",
      "Batch 20/658 | Loss: 0.1404\n",
      "Batch 25/658 | Loss: 0.4353\n",
      "Batch 30/658 | Loss: 0.2929\n",
      "Batch 35/658 | Loss: 0.0798\n",
      "Batch 40/658 | Loss: 0.1329\n",
      "Batch 45/658 | Loss: 0.1277\n",
      "Batch 50/658 | Loss: 0.1890\n",
      "Batch 55/658 | Loss: 0.1984\n",
      "Batch 60/658 | Loss: 0.1475\n",
      "Batch 65/658 | Loss: 0.0607\n",
      "Batch 70/658 | Loss: 0.1954\n",
      "Batch 75/658 | Loss: 0.2349\n",
      "Batch 80/658 | Loss: 0.0985\n",
      "Batch 85/658 | Loss: 0.2900\n",
      "Batch 90/658 | Loss: 0.1266\n",
      "Batch 95/658 | Loss: 0.2830\n",
      "Batch 100/658 | Loss: 0.0673\n",
      "Batch 105/658 | Loss: 0.0340\n",
      "Batch 110/658 | Loss: 0.0597\n",
      "Batch 115/658 | Loss: 0.1995\n",
      "Batch 120/658 | Loss: 0.0968\n",
      "Batch 125/658 | Loss: 0.0793\n",
      "Batch 130/658 | Loss: 0.2175\n",
      "Batch 135/658 | Loss: 0.0729\n",
      "Batch 140/658 | Loss: 0.1397\n",
      "Batch 145/658 | Loss: 0.0982\n",
      "Batch 150/658 | Loss: 0.2126\n",
      "Batch 155/658 | Loss: 0.3902\n",
      "Batch 160/658 | Loss: 0.1734\n",
      "Batch 165/658 | Loss: 0.2971\n",
      "Batch 170/658 | Loss: 0.1057\n",
      "Batch 175/658 | Loss: 0.0847\n",
      "Batch 180/658 | Loss: 0.0945\n",
      "Batch 185/658 | Loss: 0.2668\n",
      "Batch 190/658 | Loss: 0.3570\n",
      "Batch 195/658 | Loss: 0.2959\n",
      "Batch 200/658 | Loss: 0.1426\n",
      "Batch 205/658 | Loss: 0.4324\n",
      "Batch 210/658 | Loss: 0.1478\n",
      "Batch 215/658 | Loss: 0.3335\n",
      "Batch 220/658 | Loss: 0.1813\n",
      "Batch 225/658 | Loss: 0.2074\n",
      "Batch 230/658 | Loss: 0.0696\n",
      "Batch 235/658 | Loss: 0.2357\n",
      "Batch 240/658 | Loss: 0.2899\n",
      "Batch 245/658 | Loss: 0.2163\n",
      "Batch 250/658 | Loss: 0.0730\n",
      "Batch 255/658 | Loss: 0.1026\n",
      "Batch 260/658 | Loss: 0.2726\n",
      "Batch 265/658 | Loss: 0.0963\n",
      "Batch 270/658 | Loss: 0.1576\n",
      "Batch 275/658 | Loss: 0.1676\n",
      "Batch 280/658 | Loss: 0.2422\n",
      "Batch 285/658 | Loss: 0.1214\n",
      "Batch 290/658 | Loss: 0.1402\n",
      "Batch 295/658 | Loss: 0.2260\n",
      "Batch 300/658 | Loss: 0.2653\n",
      "Batch 305/658 | Loss: 0.1323\n",
      "Batch 310/658 | Loss: 0.2469\n",
      "Batch 315/658 | Loss: 0.2017\n",
      "Batch 320/658 | Loss: 0.0904\n",
      "Batch 325/658 | Loss: 0.0733\n",
      "Batch 330/658 | Loss: 0.1379\n",
      "Batch 335/658 | Loss: 0.2509\n",
      "Batch 340/658 | Loss: 0.2380\n",
      "Batch 345/658 | Loss: 0.0573\n",
      "Batch 350/658 | Loss: 0.1888\n",
      "Batch 355/658 | Loss: 0.1256\n",
      "Batch 360/658 | Loss: 0.1695\n",
      "Batch 365/658 | Loss: 0.1981\n",
      "Batch 370/658 | Loss: 0.3850\n",
      "Batch 375/658 | Loss: 0.2482\n",
      "Batch 380/658 | Loss: 0.2191\n",
      "Batch 385/658 | Loss: 0.1060\n",
      "Batch 390/658 | Loss: 0.3268\n",
      "Batch 395/658 | Loss: 0.2166\n",
      "Batch 400/658 | Loss: 0.0974\n",
      "Batch 405/658 | Loss: 0.2658\n",
      "Batch 410/658 | Loss: 0.2056\n",
      "Batch 415/658 | Loss: 0.1079\n",
      "Batch 420/658 | Loss: 0.3754\n",
      "Batch 425/658 | Loss: 0.1006\n",
      "Batch 430/658 | Loss: 0.2240\n",
      "Batch 435/658 | Loss: 0.4194\n",
      "Batch 440/658 | Loss: 0.0592\n",
      "Batch 445/658 | Loss: 0.1497\n",
      "Batch 450/658 | Loss: 0.1318\n",
      "Batch 455/658 | Loss: 0.1842\n",
      "Batch 460/658 | Loss: 0.1830\n",
      "Batch 465/658 | Loss: 0.3788\n",
      "Batch 470/658 | Loss: 0.0857\n",
      "Batch 475/658 | Loss: 0.2604\n",
      "Batch 480/658 | Loss: 0.1073\n",
      "Batch 485/658 | Loss: 0.1536\n",
      "Batch 490/658 | Loss: 0.1066\n",
      "Batch 495/658 | Loss: 0.1659\n",
      "Batch 500/658 | Loss: 0.1278\n",
      "Batch 505/658 | Loss: 0.0888\n",
      "Batch 510/658 | Loss: 0.4564\n",
      "Batch 515/658 | Loss: 0.2540\n",
      "Batch 520/658 | Loss: 0.3510\n",
      "Batch 525/658 | Loss: 0.1007\n",
      "Batch 530/658 | Loss: 0.2972\n",
      "Batch 535/658 | Loss: 0.1576\n",
      "Batch 540/658 | Loss: 0.3286\n",
      "Batch 545/658 | Loss: 0.2092\n",
      "Batch 550/658 | Loss: 0.2039\n",
      "Batch 555/658 | Loss: 0.1924\n",
      "Batch 560/658 | Loss: 0.1295\n",
      "Batch 565/658 | Loss: 0.0951\n",
      "Batch 570/658 | Loss: 0.1429\n",
      "Batch 575/658 | Loss: 0.1247\n",
      "Batch 580/658 | Loss: 0.1182\n",
      "Batch 585/658 | Loss: 0.0523\n",
      "Batch 590/658 | Loss: 0.1724\n",
      "Batch 595/658 | Loss: 0.2698\n",
      "Batch 600/658 | Loss: 0.1397\n",
      "Batch 605/658 | Loss: 0.0720\n",
      "Batch 610/658 | Loss: 0.1936\n",
      "Batch 615/658 | Loss: 0.1959\n",
      "Batch 620/658 | Loss: 0.1232\n",
      "Batch 625/658 | Loss: 0.1743\n",
      "Batch 630/658 | Loss: 0.2664\n",
      "Batch 635/658 | Loss: 0.1454\n",
      "Batch 640/658 | Loss: 0.0530\n",
      "Batch 645/658 | Loss: 0.1395\n",
      "Batch 650/658 | Loss: 0.2281\n",
      "Batch 655/658 | Loss: 0.1262\n",
      "Average Training LossL: 0.1861\n",
      "Validation Loss: 0.2892\n",
      "Frame-level moving/not-moving accuracy: 86.78% (threshold=0.5)\n",
      "Summary: train_loss=0.1861, val_loss=0.2892, val_acc=86.78%\n",
      "\n",
      "Epoch 6/10\n",
      "----------------------------\n",
      "Batch 0/658 | Loss: 0.0940\n",
      "Batch 5/658 | Loss: 0.2870\n",
      "Batch 10/658 | Loss: 0.1794\n",
      "Batch 15/658 | Loss: 0.2043\n",
      "Batch 20/658 | Loss: 0.2676\n",
      "Batch 25/658 | Loss: 0.0900\n",
      "Batch 30/658 | Loss: 0.3704\n",
      "Batch 35/658 | Loss: 0.1424\n",
      "Batch 40/658 | Loss: 0.4568\n",
      "Batch 45/658 | Loss: 0.2078\n",
      "Batch 50/658 | Loss: 0.2653\n",
      "Batch 55/658 | Loss: 0.1822\n",
      "Batch 60/658 | Loss: 0.0936\n",
      "Batch 65/658 | Loss: 0.4149\n",
      "Batch 70/658 | Loss: 0.1664\n",
      "Batch 75/658 | Loss: 0.3798\n",
      "Batch 80/658 | Loss: 0.5977\n",
      "Batch 85/658 | Loss: 0.3960\n",
      "Batch 90/658 | Loss: 0.1170\n",
      "Batch 95/658 | Loss: 0.1118\n",
      "Batch 100/658 | Loss: 0.0377\n",
      "Batch 105/658 | Loss: 0.2476\n",
      "Batch 110/658 | Loss: 0.2719\n",
      "Batch 115/658 | Loss: 0.1441\n",
      "Batch 120/658 | Loss: 0.1300\n",
      "Batch 125/658 | Loss: 0.1441\n",
      "Batch 130/658 | Loss: 0.0409\n",
      "Batch 135/658 | Loss: 0.1973\n",
      "Batch 140/658 | Loss: 0.1551\n",
      "Batch 145/658 | Loss: 0.2047\n",
      "Batch 150/658 | Loss: 0.3767\n",
      "Batch 155/658 | Loss: 0.0304\n",
      "Batch 160/658 | Loss: 0.0354\n",
      "Batch 165/658 | Loss: 0.0831\n",
      "Batch 170/658 | Loss: 0.0347\n",
      "Batch 175/658 | Loss: 0.2514\n",
      "Batch 180/658 | Loss: 0.1928\n",
      "Batch 185/658 | Loss: 0.2859\n",
      "Batch 190/658 | Loss: 0.1082\n",
      "Batch 195/658 | Loss: 0.0980\n",
      "Batch 200/658 | Loss: 0.1604\n",
      "Batch 205/658 | Loss: 0.2075\n",
      "Batch 210/658 | Loss: 0.0447\n",
      "Batch 215/658 | Loss: 0.1461\n",
      "Batch 220/658 | Loss: 0.1518\n",
      "Batch 225/658 | Loss: 0.2057\n",
      "Batch 230/658 | Loss: 0.2591\n",
      "Batch 235/658 | Loss: 0.3686\n",
      "Batch 240/658 | Loss: 0.1531\n",
      "Batch 245/658 | Loss: 0.1263\n",
      "Batch 250/658 | Loss: 0.0507\n",
      "Batch 255/658 | Loss: 0.0790\n",
      "Batch 260/658 | Loss: 0.0903\n",
      "Batch 265/658 | Loss: 0.3207\n",
      "Batch 270/658 | Loss: 0.3076\n",
      "Batch 275/658 | Loss: 0.2342\n",
      "Batch 280/658 | Loss: 0.0550\n",
      "Batch 285/658 | Loss: 0.0908\n",
      "Batch 290/658 | Loss: 0.1298\n",
      "Batch 295/658 | Loss: 0.0849\n",
      "Batch 300/658 | Loss: 0.1755\n",
      "Batch 305/658 | Loss: 0.0384\n",
      "Batch 310/658 | Loss: 0.1641\n",
      "Batch 315/658 | Loss: 0.2752\n",
      "Batch 320/658 | Loss: 0.1683\n",
      "Batch 325/658 | Loss: 0.1050\n",
      "Batch 330/658 | Loss: 0.2208\n",
      "Batch 335/658 | Loss: 0.1629\n",
      "Batch 340/658 | Loss: 0.1156\n",
      "Batch 345/658 | Loss: 0.2577\n",
      "Batch 350/658 | Loss: 0.0522\n",
      "Batch 355/658 | Loss: 0.2278\n",
      "Batch 360/658 | Loss: 0.1104\n",
      "Batch 365/658 | Loss: 0.0908\n",
      "Batch 370/658 | Loss: 0.0412\n",
      "Batch 375/658 | Loss: 0.2964\n",
      "Batch 380/658 | Loss: 0.1604\n",
      "Batch 385/658 | Loss: 0.2091\n",
      "Batch 390/658 | Loss: 0.0942\n",
      "Batch 395/658 | Loss: 0.2412\n",
      "Batch 400/658 | Loss: 0.1538\n",
      "Batch 405/658 | Loss: 0.2121\n",
      "Batch 410/658 | Loss: 0.1784\n",
      "Batch 415/658 | Loss: 0.0749\n",
      "Batch 420/658 | Loss: 0.2120\n",
      "Batch 425/658 | Loss: 0.2464\n",
      "Batch 430/658 | Loss: 0.2830\n",
      "Batch 435/658 | Loss: 0.1856\n",
      "Batch 440/658 | Loss: 0.0296\n",
      "Batch 445/658 | Loss: 0.1179\n",
      "Batch 450/658 | Loss: 0.1952\n",
      "Batch 455/658 | Loss: 0.1479\n",
      "Batch 460/658 | Loss: 0.0466\n",
      "Batch 465/658 | Loss: 0.0675\n",
      "Batch 470/658 | Loss: 0.1376\n",
      "Batch 475/658 | Loss: 0.0402\n",
      "Batch 480/658 | Loss: 0.3440\n",
      "Batch 485/658 | Loss: 0.1476\n",
      "Batch 490/658 | Loss: 0.1563\n",
      "Batch 495/658 | Loss: 0.0267\n",
      "Batch 500/658 | Loss: 0.1054\n",
      "Batch 505/658 | Loss: 0.2252\n",
      "Batch 510/658 | Loss: 0.1079\n",
      "Batch 515/658 | Loss: 0.1432\n",
      "Batch 520/658 | Loss: 0.1919\n",
      "Batch 525/658 | Loss: 0.1898\n",
      "Batch 530/658 | Loss: 0.1881\n",
      "Batch 535/658 | Loss: 0.1807\n",
      "Batch 540/658 | Loss: 0.1191\n",
      "Batch 545/658 | Loss: 0.1988\n",
      "Batch 550/658 | Loss: 0.0779\n",
      "Batch 555/658 | Loss: 0.1193\n",
      "Batch 560/658 | Loss: 0.1881\n",
      "Batch 565/658 | Loss: 0.0407\n",
      "Batch 570/658 | Loss: 0.0411\n",
      "Batch 575/658 | Loss: 0.1859\n",
      "Batch 580/658 | Loss: 0.1510\n",
      "Batch 585/658 | Loss: 0.1461\n",
      "Batch 590/658 | Loss: 0.1053\n",
      "Batch 595/658 | Loss: 0.1158\n",
      "Batch 600/658 | Loss: 0.1567\n",
      "Batch 605/658 | Loss: 0.2029\n",
      "Batch 610/658 | Loss: 0.2642\n",
      "Batch 615/658 | Loss: 0.1626\n",
      "Batch 620/658 | Loss: 0.1633\n",
      "Batch 625/658 | Loss: 0.0428\n",
      "Batch 630/658 | Loss: 0.0809\n",
      "Batch 635/658 | Loss: 0.2598\n",
      "Batch 640/658 | Loss: 0.1614\n",
      "Batch 645/658 | Loss: 0.2594\n",
      "Batch 650/658 | Loss: 0.0701\n",
      "Batch 655/658 | Loss: 0.4435\n",
      "Average Training LossL: 0.1728\n",
      "Validation Loss: 0.1815\n",
      "Frame-level moving/not-moving accuracy: 81.31% (threshold=0.5)\n",
      "Summary: train_loss=0.1728, val_loss=0.1815, val_acc=81.31%\n",
      "\n",
      "Epoch 7/10\n",
      "----------------------------\n",
      "Batch 0/658 | Loss: 0.1014\n",
      "Batch 5/658 | Loss: 0.0902\n",
      "Batch 10/658 | Loss: 0.2685\n",
      "Batch 15/658 | Loss: 0.2049\n",
      "Batch 20/658 | Loss: 0.2112\n",
      "Batch 25/658 | Loss: 0.1506\n",
      "Batch 30/658 | Loss: 0.2019\n",
      "Batch 35/658 | Loss: 0.0989\n",
      "Batch 40/658 | Loss: 0.1729\n",
      "Batch 45/658 | Loss: 0.1671\n",
      "Batch 50/658 | Loss: 0.0915\n",
      "Batch 55/658 | Loss: 0.2094\n",
      "Batch 60/658 | Loss: 0.2079\n",
      "Batch 65/658 | Loss: 0.1170\n",
      "Batch 70/658 | Loss: 0.0934\n",
      "Batch 75/658 | Loss: 0.0445\n",
      "Batch 80/658 | Loss: 0.1212\n",
      "Batch 85/658 | Loss: 0.2013\n",
      "Batch 90/658 | Loss: 0.1409\n",
      "Batch 95/658 | Loss: 0.1123\n",
      "Batch 100/658 | Loss: 0.1355\n",
      "Batch 105/658 | Loss: 0.1129\n",
      "Batch 110/658 | Loss: 0.1112\n",
      "Batch 115/658 | Loss: 0.2333\n",
      "Batch 120/658 | Loss: 0.0924\n",
      "Batch 125/658 | Loss: 0.1196\n",
      "Batch 130/658 | Loss: 0.0853\n",
      "Batch 135/658 | Loss: 0.0397\n",
      "Batch 140/658 | Loss: 0.4198\n",
      "Batch 145/658 | Loss: 0.0674\n",
      "Batch 150/658 | Loss: 0.0688\n",
      "Batch 155/658 | Loss: 0.1561\n",
      "Batch 160/658 | Loss: 0.1510\n",
      "Batch 165/658 | Loss: 0.1659\n",
      "Batch 170/658 | Loss: 0.2407\n",
      "Batch 175/658 | Loss: 0.1220\n",
      "Batch 180/658 | Loss: 0.0408\n",
      "Batch 185/658 | Loss: 0.3239\n",
      "Batch 190/658 | Loss: 0.1815\n",
      "Batch 195/658 | Loss: 0.0746\n",
      "Batch 200/658 | Loss: 0.1105\n",
      "Batch 205/658 | Loss: 0.1368\n",
      "Batch 210/658 | Loss: 0.0720\n",
      "Batch 215/658 | Loss: 0.1403\n",
      "Batch 220/658 | Loss: 0.1098\n",
      "Batch 225/658 | Loss: 0.0747\n",
      "Batch 230/658 | Loss: 0.0939\n",
      "Batch 235/658 | Loss: 0.1463\n",
      "Batch 240/658 | Loss: 0.0951\n",
      "Batch 245/658 | Loss: 0.0746\n",
      "Batch 250/658 | Loss: 0.1643\n",
      "Batch 255/658 | Loss: 0.0301\n",
      "Batch 260/658 | Loss: 0.0687\n",
      "Batch 265/658 | Loss: 0.1217\n",
      "Batch 270/658 | Loss: 0.0539\n",
      "Batch 275/658 | Loss: 0.0180\n",
      "Batch 280/658 | Loss: 0.0751\n",
      "Batch 285/658 | Loss: 0.2630\n",
      "Batch 290/658 | Loss: 0.1731\n",
      "Batch 295/658 | Loss: 0.1905\n",
      "Batch 300/658 | Loss: 0.0338\n",
      "Batch 305/658 | Loss: 0.0334\n",
      "Batch 310/658 | Loss: 0.1296\n",
      "Batch 315/658 | Loss: 0.3062\n",
      "Batch 320/658 | Loss: 0.1709\n",
      "Batch 325/658 | Loss: 0.1494\n",
      "Batch 330/658 | Loss: 0.0935\n",
      "Batch 335/658 | Loss: 0.1467\n",
      "Batch 340/658 | Loss: 0.3431\n",
      "Batch 345/658 | Loss: 0.0860\n",
      "Batch 350/658 | Loss: 0.2329\n",
      "Batch 355/658 | Loss: 0.2537\n",
      "Batch 360/658 | Loss: 0.3567\n",
      "Batch 365/658 | Loss: 0.2220\n",
      "Batch 370/658 | Loss: 0.2798\n",
      "Batch 375/658 | Loss: 0.2357\n",
      "Batch 380/658 | Loss: 0.1971\n",
      "Batch 385/658 | Loss: 0.3188\n",
      "Batch 390/658 | Loss: 0.1048\n",
      "Batch 395/658 | Loss: 0.1960\n",
      "Batch 400/658 | Loss: 0.1972\n",
      "Batch 405/658 | Loss: 0.1654\n",
      "Batch 410/658 | Loss: 0.2018\n",
      "Batch 415/658 | Loss: 0.1727\n",
      "Batch 420/658 | Loss: 0.0965\n",
      "Batch 425/658 | Loss: 0.2610\n",
      "Batch 430/658 | Loss: 0.1087\n",
      "Batch 435/658 | Loss: 0.2113\n",
      "Batch 440/658 | Loss: 0.1498\n",
      "Batch 445/658 | Loss: 0.0235\n",
      "Batch 450/658 | Loss: 0.0419\n",
      "Batch 455/658 | Loss: 0.1284\n",
      "Batch 460/658 | Loss: 0.1102\n",
      "Batch 465/658 | Loss: 0.1078\n",
      "Batch 470/658 | Loss: 0.2227\n",
      "Batch 475/658 | Loss: 0.1184\n",
      "Batch 480/658 | Loss: 0.0532\n",
      "Batch 485/658 | Loss: 0.2673\n",
      "Batch 490/658 | Loss: 0.0462\n",
      "Batch 495/658 | Loss: 0.1507\n",
      "Batch 500/658 | Loss: 0.2956\n",
      "Batch 505/658 | Loss: 0.0833\n",
      "Batch 510/658 | Loss: 0.1223\n",
      "Batch 515/658 | Loss: 0.0479\n",
      "Batch 520/658 | Loss: 0.1014\n",
      "Batch 525/658 | Loss: 0.2531\n",
      "Batch 530/658 | Loss: 0.1959\n",
      "Batch 535/658 | Loss: 0.0206\n",
      "Batch 540/658 | Loss: 0.1229\n",
      "Batch 545/658 | Loss: 0.1589\n",
      "Batch 550/658 | Loss: 0.1822\n",
      "Batch 555/658 | Loss: 0.0334\n",
      "Batch 560/658 | Loss: 0.0883\n",
      "Batch 565/658 | Loss: 0.2353\n",
      "Batch 570/658 | Loss: 0.1555\n",
      "Batch 575/658 | Loss: 0.1152\n",
      "Batch 580/658 | Loss: 0.1378\n",
      "Batch 585/658 | Loss: 0.0665\n",
      "Batch 590/658 | Loss: 0.0244\n",
      "Batch 595/658 | Loss: 0.0821\n",
      "Batch 600/658 | Loss: 0.1470\n",
      "Batch 605/658 | Loss: 0.0375\n",
      "Batch 610/658 | Loss: 0.1681\n",
      "Batch 615/658 | Loss: 0.1123\n",
      "Batch 620/658 | Loss: 0.1192\n",
      "Batch 625/658 | Loss: 0.2296\n",
      "Batch 630/658 | Loss: 0.3538\n",
      "Batch 635/658 | Loss: 0.3082\n",
      "Batch 640/658 | Loss: 0.0825\n",
      "Batch 645/658 | Loss: 0.2186\n",
      "Batch 650/658 | Loss: 0.3498\n",
      "Batch 655/658 | Loss: 0.1722\n",
      "Average Training LossL: 0.1477\n",
      "Validation Loss: 0.1637\n",
      "Frame-level moving/not-moving accuracy: 87.23% (threshold=0.5)\n",
      "Summary: train_loss=0.1477, val_loss=0.1637, val_acc=87.23%\n",
      "\n",
      "Epoch 8/10\n",
      "----------------------------\n",
      "Batch 0/658 | Loss: 0.2774\n",
      "Batch 5/658 | Loss: 0.2262\n",
      "Batch 10/658 | Loss: 0.3561\n",
      "Batch 15/658 | Loss: 0.2294\n",
      "Batch 20/658 | Loss: 0.1194\n",
      "Batch 25/658 | Loss: 0.1453\n",
      "Batch 30/658 | Loss: 0.1143\n",
      "Batch 35/658 | Loss: 0.1430\n",
      "Batch 40/658 | Loss: 0.1549\n",
      "Batch 45/658 | Loss: 0.0790\n",
      "Batch 50/658 | Loss: 0.0784\n",
      "Batch 55/658 | Loss: 0.0725\n",
      "Batch 60/658 | Loss: 0.0248\n",
      "Batch 65/658 | Loss: 0.0240\n",
      "Batch 70/658 | Loss: 0.1563\n",
      "Batch 75/658 | Loss: 0.0890\n",
      "Batch 80/658 | Loss: 0.0939\n",
      "Batch 85/658 | Loss: 0.1486\n",
      "Batch 90/658 | Loss: 0.1016\n",
      "Batch 95/658 | Loss: 0.0609\n",
      "Batch 100/658 | Loss: 0.1280\n",
      "Batch 105/658 | Loss: 0.1980\n",
      "Batch 110/658 | Loss: 0.1492\n",
      "Batch 115/658 | Loss: 0.1472\n",
      "Batch 120/658 | Loss: 0.0969\n",
      "Batch 125/658 | Loss: 0.0910\n",
      "Batch 130/658 | Loss: 0.0506\n",
      "Batch 135/658 | Loss: 0.1520\n",
      "Batch 140/658 | Loss: 0.2536\n",
      "Batch 145/658 | Loss: 0.1399\n",
      "Batch 150/658 | Loss: 0.0961\n",
      "Batch 155/658 | Loss: 0.0241\n",
      "Batch 160/658 | Loss: 0.0308\n",
      "Batch 165/658 | Loss: 0.2912\n",
      "Batch 170/658 | Loss: 0.0908\n",
      "Batch 175/658 | Loss: 0.0792\n",
      "Batch 180/658 | Loss: 0.1353\n",
      "Batch 185/658 | Loss: 0.0329\n",
      "Batch 190/658 | Loss: 0.0708\n",
      "Batch 195/658 | Loss: 0.1840\n",
      "Batch 200/658 | Loss: 0.0715\n",
      "Batch 205/658 | Loss: 0.2399\n",
      "Batch 210/658 | Loss: 0.2147\n",
      "Batch 215/658 | Loss: 0.2039\n",
      "Batch 220/658 | Loss: 0.1939\n",
      "Batch 225/658 | Loss: 0.1158\n",
      "Batch 230/658 | Loss: 0.1115\n",
      "Batch 235/658 | Loss: 0.0856\n",
      "Batch 240/658 | Loss: 0.1176\n",
      "Batch 245/658 | Loss: 0.1322\n",
      "Batch 250/658 | Loss: 0.2385\n",
      "Batch 255/658 | Loss: 0.0798\n",
      "Batch 260/658 | Loss: 0.1728\n",
      "Batch 265/658 | Loss: 0.2164\n",
      "Batch 270/658 | Loss: 0.1503\n",
      "Batch 275/658 | Loss: 0.1817\n",
      "Batch 280/658 | Loss: 0.2955\n",
      "Batch 285/658 | Loss: 0.0471\n",
      "Batch 290/658 | Loss: 0.1133\n",
      "Batch 295/658 | Loss: 0.1099\n",
      "Batch 300/658 | Loss: 0.1312\n",
      "Batch 305/658 | Loss: 0.1357\n",
      "Batch 310/658 | Loss: 0.0249\n",
      "Batch 315/658 | Loss: 0.1672\n",
      "Batch 320/658 | Loss: 0.0261\n",
      "Batch 325/658 | Loss: 0.1445\n",
      "Batch 330/658 | Loss: 0.1089\n",
      "Batch 335/658 | Loss: 0.0852\n",
      "Batch 340/658 | Loss: 0.1677\n",
      "Batch 345/658 | Loss: 0.0780\n",
      "Batch 350/658 | Loss: 0.0351\n",
      "Batch 355/658 | Loss: 0.1603\n",
      "Batch 360/658 | Loss: 0.1143\n",
      "Batch 365/658 | Loss: 0.1934\n",
      "Batch 370/658 | Loss: 0.0655\n",
      "Batch 375/658 | Loss: 0.0618\n",
      "Batch 380/658 | Loss: 0.1552\n",
      "Batch 385/658 | Loss: 0.1112\n",
      "Batch 390/658 | Loss: 0.1340\n",
      "Batch 395/658 | Loss: 0.1283\n",
      "Batch 400/658 | Loss: 0.1914\n",
      "Batch 405/658 | Loss: 0.1669\n",
      "Batch 410/658 | Loss: 0.1051\n",
      "Batch 415/658 | Loss: 0.2057\n",
      "Batch 420/658 | Loss: 0.0476\n",
      "Batch 425/658 | Loss: 0.0752\n",
      "Batch 430/658 | Loss: 0.0873\n",
      "Batch 435/658 | Loss: 0.0726\n",
      "Batch 440/658 | Loss: 0.0857\n",
      "Batch 445/658 | Loss: 0.1387\n",
      "Batch 450/658 | Loss: 0.0842\n",
      "Batch 455/658 | Loss: 0.0693\n",
      "Batch 460/658 | Loss: 0.2180\n",
      "Batch 465/658 | Loss: 0.1103\n",
      "Batch 470/658 | Loss: 0.0875\n",
      "Batch 475/658 | Loss: 0.1338\n",
      "Batch 480/658 | Loss: 0.1227\n",
      "Batch 485/658 | Loss: 0.0931\n",
      "Batch 490/658 | Loss: 0.1302\n",
      "Batch 495/658 | Loss: 0.0955\n",
      "Batch 500/658 | Loss: 0.1102\n",
      "Batch 505/658 | Loss: 0.1816\n",
      "Batch 510/658 | Loss: 0.0897\n",
      "Batch 515/658 | Loss: 0.2875\n",
      "Batch 520/658 | Loss: 0.1022\n",
      "Batch 525/658 | Loss: 0.1064\n",
      "Batch 530/658 | Loss: 0.1199\n",
      "Batch 535/658 | Loss: 0.0726\n",
      "Batch 540/658 | Loss: 0.1026\n",
      "Batch 545/658 | Loss: 0.1561\n",
      "Batch 550/658 | Loss: 0.1035\n",
      "Batch 555/658 | Loss: 0.1709\n",
      "Batch 560/658 | Loss: 0.1950\n",
      "Batch 565/658 | Loss: 0.0705\n",
      "Batch 570/658 | Loss: 0.0172\n",
      "Batch 575/658 | Loss: 0.1298\n",
      "Batch 580/658 | Loss: 0.2266\n",
      "Batch 585/658 | Loss: 0.1192\n",
      "Batch 590/658 | Loss: 0.1354\n",
      "Batch 595/658 | Loss: 0.1142\n",
      "Batch 600/658 | Loss: 0.0646\n",
      "Batch 605/658 | Loss: 0.1556\n",
      "Batch 610/658 | Loss: 0.1621\n",
      "Batch 615/658 | Loss: 0.0935\n",
      "Batch 620/658 | Loss: 0.0531\n",
      "Batch 625/658 | Loss: 0.0393\n",
      "Batch 630/658 | Loss: 0.2533\n",
      "Batch 635/658 | Loss: 0.1236\n",
      "Batch 640/658 | Loss: 0.1598\n",
      "Batch 645/658 | Loss: 0.1181\n",
      "Batch 650/658 | Loss: 0.1309\n",
      "Batch 655/658 | Loss: 0.1530\n",
      "Average Training LossL: 0.1294\n",
      "Validation Loss: 0.1375\n",
      "Frame-level moving/not-moving accuracy: 87.99% (threshold=0.5)\n",
      "Summary: train_loss=0.1294, val_loss=0.1375, val_acc=87.99%\n",
      "\n",
      "Epoch 9/10\n",
      "----------------------------\n",
      "Batch 0/658 | Loss: 0.1552\n",
      "Batch 5/658 | Loss: 0.1304\n",
      "Batch 10/658 | Loss: 0.0552\n",
      "Batch 15/658 | Loss: 0.1384\n",
      "Batch 20/658 | Loss: 0.1310\n",
      "Batch 25/658 | Loss: 0.1268\n",
      "Batch 30/658 | Loss: 0.1425\n",
      "Batch 35/658 | Loss: 0.1757\n",
      "Batch 40/658 | Loss: 0.0775\n",
      "Batch 45/658 | Loss: 0.2577\n",
      "Batch 50/658 | Loss: 0.1263\n",
      "Batch 55/658 | Loss: 0.2929\n",
      "Batch 60/658 | Loss: 0.1529\n",
      "Batch 65/658 | Loss: 0.1235\n",
      "Batch 70/658 | Loss: 0.1468\n",
      "Batch 75/658 | Loss: 0.2107\n",
      "Batch 80/658 | Loss: 0.1941\n",
      "Batch 85/658 | Loss: 0.1072\n",
      "Batch 90/658 | Loss: 0.0345\n",
      "Batch 95/658 | Loss: 0.0590\n",
      "Batch 100/658 | Loss: 0.1186\n",
      "Batch 105/658 | Loss: 0.2780\n",
      "Batch 110/658 | Loss: 0.0944\n",
      "Batch 115/658 | Loss: 0.1943\n",
      "Batch 120/658 | Loss: 0.2119\n",
      "Batch 125/658 | Loss: 0.1185\n",
      "Batch 130/658 | Loss: 0.1666\n",
      "Batch 135/658 | Loss: 0.2490\n",
      "Batch 140/658 | Loss: 0.0511\n",
      "Batch 145/658 | Loss: 0.1835\n",
      "Batch 150/658 | Loss: 0.0439\n",
      "Batch 155/658 | Loss: 0.0521\n",
      "Batch 160/658 | Loss: 0.0914\n",
      "Batch 165/658 | Loss: 0.2043\n",
      "Batch 170/658 | Loss: 0.1233\n",
      "Batch 175/658 | Loss: 0.1872\n",
      "Batch 180/658 | Loss: 0.1323\n",
      "Batch 185/658 | Loss: 0.3210\n",
      "Batch 190/658 | Loss: 0.1193\n",
      "Batch 195/658 | Loss: 0.1082\n",
      "Batch 200/658 | Loss: 0.2027\n",
      "Batch 205/658 | Loss: 0.1192\n",
      "Batch 210/658 | Loss: 0.1532\n",
      "Batch 215/658 | Loss: 0.1781\n",
      "Batch 220/658 | Loss: 0.0317\n",
      "Batch 225/658 | Loss: 0.0132\n",
      "Batch 230/658 | Loss: 0.0870\n",
      "Batch 235/658 | Loss: 0.1561\n",
      "Batch 240/658 | Loss: 0.0820\n",
      "Batch 245/658 | Loss: 0.1867\n",
      "Batch 250/658 | Loss: 0.2243\n",
      "Batch 255/658 | Loss: 0.1794\n",
      "Batch 260/658 | Loss: 0.1785\n",
      "Batch 265/658 | Loss: 0.1520\n",
      "Batch 270/658 | Loss: 0.1729\n",
      "Batch 275/658 | Loss: 0.1806\n",
      "Batch 280/658 | Loss: 0.0478\n",
      "Batch 285/658 | Loss: 0.2319\n",
      "Batch 290/658 | Loss: 0.0353\n",
      "Batch 295/658 | Loss: 0.0180\n",
      "Batch 300/658 | Loss: 0.1449\n",
      "Batch 305/658 | Loss: 0.0824\n",
      "Batch 310/658 | Loss: 0.2038\n",
      "Batch 315/658 | Loss: 0.1545\n",
      "Batch 320/658 | Loss: 0.0634\n",
      "Batch 325/658 | Loss: 0.0996\n",
      "Batch 330/658 | Loss: 0.1012\n",
      "Batch 335/658 | Loss: 0.1976\n",
      "Batch 340/658 | Loss: 0.0777\n",
      "Batch 345/658 | Loss: 0.2187\n",
      "Batch 350/658 | Loss: 0.1127\n",
      "Batch 355/658 | Loss: 0.0453\n",
      "Batch 360/658 | Loss: 0.0610\n",
      "Batch 365/658 | Loss: 0.0404\n",
      "Batch 370/658 | Loss: 0.0715\n",
      "Batch 375/658 | Loss: 0.0350\n",
      "Batch 380/658 | Loss: 0.1151\n",
      "Batch 385/658 | Loss: 0.0429\n",
      "Batch 390/658 | Loss: 0.0774\n",
      "Batch 395/658 | Loss: 0.1561\n",
      "Batch 400/658 | Loss: 0.0219\n",
      "Batch 405/658 | Loss: 0.0614\n",
      "Batch 410/658 | Loss: 0.0843\n",
      "Batch 415/658 | Loss: 0.0563\n",
      "Batch 420/658 | Loss: 0.0724\n",
      "Batch 425/658 | Loss: 0.0750\n",
      "Batch 430/658 | Loss: 0.0707\n",
      "Batch 435/658 | Loss: 0.0977\n",
      "Batch 440/658 | Loss: 0.0957\n",
      "Batch 445/658 | Loss: 0.1341\n",
      "Batch 450/658 | Loss: 0.0979\n",
      "Batch 455/658 | Loss: 0.1372\n",
      "Batch 460/658 | Loss: 0.0471\n",
      "Batch 465/658 | Loss: 0.0762\n",
      "Batch 470/658 | Loss: 0.0903\n",
      "Batch 475/658 | Loss: 0.1320\n",
      "Batch 480/658 | Loss: 0.0869\n",
      "Batch 485/658 | Loss: 0.1611\n",
      "Batch 490/658 | Loss: 0.0371\n",
      "Batch 495/658 | Loss: 0.1121\n",
      "Batch 500/658 | Loss: 0.0813\n",
      "Batch 505/658 | Loss: 0.1826\n",
      "Batch 510/658 | Loss: 0.3483\n",
      "Batch 515/658 | Loss: 0.1516\n",
      "Batch 520/658 | Loss: 0.1599\n",
      "Batch 525/658 | Loss: 0.1562\n",
      "Batch 530/658 | Loss: 0.1151\n",
      "Batch 535/658 | Loss: 0.1349\n",
      "Batch 540/658 | Loss: 0.1182\n",
      "Batch 545/658 | Loss: 0.1239\n",
      "Batch 550/658 | Loss: 0.0674\n",
      "Batch 555/658 | Loss: 0.0955\n",
      "Batch 560/658 | Loss: 0.0967\n",
      "Batch 565/658 | Loss: 0.1777\n",
      "Batch 570/658 | Loss: 0.1921\n",
      "Batch 575/658 | Loss: 0.1149\n",
      "Batch 580/658 | Loss: 0.2601\n",
      "Batch 585/658 | Loss: 0.1766\n",
      "Batch 590/658 | Loss: 0.1539\n",
      "Batch 595/658 | Loss: 0.0598\n",
      "Batch 600/658 | Loss: 0.0593\n",
      "Batch 605/658 | Loss: 0.0986\n",
      "Batch 610/658 | Loss: 0.0787\n",
      "Batch 615/658 | Loss: 0.1962\n",
      "Batch 620/658 | Loss: 0.1776\n",
      "Batch 625/658 | Loss: 0.0344\n",
      "Batch 630/658 | Loss: 0.1608\n",
      "Batch 635/658 | Loss: 0.0231\n",
      "Batch 640/658 | Loss: 0.1514\n",
      "Batch 645/658 | Loss: 0.1957\n",
      "Batch 650/658 | Loss: 0.0886\n",
      "Batch 655/658 | Loss: 0.2184\n",
      "Average Training LossL: 0.1221\n",
      "Validation Loss: 0.1484\n",
      "Frame-level moving/not-moving accuracy: 88.60% (threshold=0.5)\n",
      "Summary: train_loss=0.1221, val_loss=0.1484, val_acc=88.60%\n",
      "\n",
      "Epoch 10/10\n",
      "----------------------------\n",
      "Batch 0/658 | Loss: 0.1996\n",
      "Batch 5/658 | Loss: 0.1396\n",
      "Batch 10/658 | Loss: 0.0841\n",
      "Batch 15/658 | Loss: 0.1081\n",
      "Batch 20/658 | Loss: 0.0582\n",
      "Batch 25/658 | Loss: 0.0190\n",
      "Batch 30/658 | Loss: 0.1434\n",
      "Batch 35/658 | Loss: 0.1414\n",
      "Batch 40/658 | Loss: 0.0588\n",
      "Batch 45/658 | Loss: 0.1251\n",
      "Batch 50/658 | Loss: 0.1788\n",
      "Batch 55/658 | Loss: 0.0972\n",
      "Batch 60/658 | Loss: 0.0678\n",
      "Batch 65/658 | Loss: 0.1066\n",
      "Batch 70/658 | Loss: 0.1446\n",
      "Batch 75/658 | Loss: 0.1072\n",
      "Batch 80/658 | Loss: 0.0414\n",
      "Batch 85/658 | Loss: 0.1046\n",
      "Batch 90/658 | Loss: 0.1466\n",
      "Batch 95/658 | Loss: 0.0483\n",
      "Batch 100/658 | Loss: 0.1564\n",
      "Batch 105/658 | Loss: 0.1714\n",
      "Batch 110/658 | Loss: 0.0987\n",
      "Batch 115/658 | Loss: 0.0362\n",
      "Batch 120/658 | Loss: 0.0423\n",
      "Batch 125/658 | Loss: 0.0245\n",
      "Batch 130/658 | Loss: 0.0086\n",
      "Batch 135/658 | Loss: 0.1400\n",
      "Batch 140/658 | Loss: 0.1158\n",
      "Batch 145/658 | Loss: 0.0420\n",
      "Batch 150/658 | Loss: 0.1112\n",
      "Batch 155/658 | Loss: 0.1181\n",
      "Batch 160/658 | Loss: 0.1116\n",
      "Batch 165/658 | Loss: 0.0246\n",
      "Batch 170/658 | Loss: 0.1044\n",
      "Batch 175/658 | Loss: 0.2321\n",
      "Batch 180/658 | Loss: 0.0727\n",
      "Batch 185/658 | Loss: 0.2185\n",
      "Batch 190/658 | Loss: 0.1461\n",
      "Batch 195/658 | Loss: 0.0738\n",
      "Batch 200/658 | Loss: 0.1426\n",
      "Batch 205/658 | Loss: 0.1081\n",
      "Batch 210/658 | Loss: 0.1472\n",
      "Batch 215/658 | Loss: 0.0989\n",
      "Batch 220/658 | Loss: 0.2127\n",
      "Batch 225/658 | Loss: 0.1370\n",
      "Batch 230/658 | Loss: 0.1205\n",
      "Batch 235/658 | Loss: 0.1468\n",
      "Batch 240/658 | Loss: 0.0906\n",
      "Batch 245/658 | Loss: 0.0613\n",
      "Batch 250/658 | Loss: 0.0489\n",
      "Batch 255/658 | Loss: 0.0792\n",
      "Batch 260/658 | Loss: 0.1583\n",
      "Batch 265/658 | Loss: 0.1090\n",
      "Batch 270/658 | Loss: 0.1079\n",
      "Batch 275/658 | Loss: 0.0343\n",
      "Batch 280/658 | Loss: 0.0784\n",
      "Batch 285/658 | Loss: 0.1191\n",
      "Batch 290/658 | Loss: 0.1043\n",
      "Batch 295/658 | Loss: 0.0859\n",
      "Batch 300/658 | Loss: 0.0670\n",
      "Batch 305/658 | Loss: 0.0591\n",
      "Batch 310/658 | Loss: 0.0867\n",
      "Batch 315/658 | Loss: 0.0339\n",
      "Batch 320/658 | Loss: 0.0914\n",
      "Batch 325/658 | Loss: 0.1196\n",
      "Batch 330/658 | Loss: 0.0146\n",
      "Batch 335/658 | Loss: 0.1157\n",
      "Batch 340/658 | Loss: 0.0137\n",
      "Batch 345/658 | Loss: 0.1621\n",
      "Batch 350/658 | Loss: 0.1829\n",
      "Batch 355/658 | Loss: 0.0621\n",
      "Batch 360/658 | Loss: 0.0370\n",
      "Batch 365/658 | Loss: 0.1012\n",
      "Batch 370/658 | Loss: 0.0866\n",
      "Batch 375/658 | Loss: 0.1129\n",
      "Batch 380/658 | Loss: 0.1199\n",
      "Batch 385/658 | Loss: 0.1276\n",
      "Batch 390/658 | Loss: 0.1520\n",
      "Batch 395/658 | Loss: 0.1511\n",
      "Batch 400/658 | Loss: 0.0846\n",
      "Batch 405/658 | Loss: 0.1051\n",
      "Batch 410/658 | Loss: 0.1640\n",
      "Batch 415/658 | Loss: 0.1406\n",
      "Batch 420/658 | Loss: 0.1627\n",
      "Batch 425/658 | Loss: 0.0778\n",
      "Batch 430/658 | Loss: 0.0632\n",
      "Batch 435/658 | Loss: 0.1511\n",
      "Batch 440/658 | Loss: 0.2404\n",
      "Batch 445/658 | Loss: 0.1767\n",
      "Batch 450/658 | Loss: 0.0328\n",
      "Batch 455/658 | Loss: 0.1961\n",
      "Batch 460/658 | Loss: 0.0806\n",
      "Batch 465/658 | Loss: 0.0410\n",
      "Batch 470/658 | Loss: 0.0724\n",
      "Batch 475/658 | Loss: 0.1012\n",
      "Batch 480/658 | Loss: 0.1406\n",
      "Batch 485/658 | Loss: 0.2672\n",
      "Batch 490/658 | Loss: 0.1269\n",
      "Batch 495/658 | Loss: 0.2400\n",
      "Batch 500/658 | Loss: 0.0849\n",
      "Batch 505/658 | Loss: 0.1643\n",
      "Batch 510/658 | Loss: 0.1282\n",
      "Batch 515/658 | Loss: 0.2354\n",
      "Batch 520/658 | Loss: 0.0996\n",
      "Batch 525/658 | Loss: 0.0822\n",
      "Batch 530/658 | Loss: 0.1289\n",
      "Batch 535/658 | Loss: 0.0656\n",
      "Batch 540/658 | Loss: 0.0230\n",
      "Batch 545/658 | Loss: 0.1668\n",
      "Batch 550/658 | Loss: 0.1374\n",
      "Batch 555/658 | Loss: 0.1496\n",
      "Batch 560/658 | Loss: 0.0686\n",
      "Batch 565/658 | Loss: 0.1051\n",
      "Batch 570/658 | Loss: 0.0430\n",
      "Batch 575/658 | Loss: 0.2202\n",
      "Batch 580/658 | Loss: 0.0239\n",
      "Batch 585/658 | Loss: 0.0941\n",
      "Batch 590/658 | Loss: 0.1470\n",
      "Batch 595/658 | Loss: 0.0954\n",
      "Batch 600/658 | Loss: 0.0293\n",
      "Batch 605/658 | Loss: 0.1856\n",
      "Batch 610/658 | Loss: 0.1596\n",
      "Batch 615/658 | Loss: 0.1157\n",
      "Batch 620/658 | Loss: 0.0753\n",
      "Batch 625/658 | Loss: 0.1317\n",
      "Batch 630/658 | Loss: 0.0669\n",
      "Batch 635/658 | Loss: 0.1812\n",
      "Batch 640/658 | Loss: 0.1069\n",
      "Batch 645/658 | Loss: 0.0598\n",
      "Batch 650/658 | Loss: 0.1416\n",
      "Batch 655/658 | Loss: 0.1220\n",
      "Average Training LossL: 0.1129\n",
      "Validation Loss: 0.1343\n",
      "Frame-level moving/not-moving accuracy: 87.08% (threshold=0.5)\n",
      "Summary: train_loss=0.1129, val_loss=0.1343, val_acc=87.08%\n",
      "\n",
      "Model saved as 'fasterrcnn_moving_detector_2.pth'.\n"
     ]
    }
   ],
   "source": [
    "#train and save trained model\n",
    "if __name__ == \"__main__\":\n",
    "    video_folder = r\"C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Raw Videos\"\n",
    "    xml_folder   = r\"C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Annotations\"\n",
    "\n",
    "    #Collate function for object detection\n",
    "    def collate_fn(batch):\n",
    "        batch = [b for b in batch if b is not None]\n",
    "        frames = [b[0] for b in batch]\n",
    "        targets = [b[1] for b in batch]\n",
    "        return frames, targets\n",
    "\n",
    "    #Load full dataset (videos + XMLs)\n",
    "    full_dataset = BaseballVideoDataset(video_folder, xml_folder, resize=(1280, 720), frame_skip=1, scale_boxes=True)\n",
    "\n",
    "    #print a quick summary\n",
    "    print(f\"\\nDataset contains {len(full_dataset)} annotated frames across videos.\")\n",
    "\n",
    "    #Split train/test\n",
    "    n = len(full_dataset)\n",
    "    split = int(0.8 * n)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [split, n - split])\n",
    "\n",
    "    #Train the model\n",
    "    trained_model = train_detector(\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        num_classes=3,\n",
    "        epochs=10,\n",
    "        lr=1e-4,\n",
    "        batch_size=4)\n",
    "\n",
    "    #Save trained model\n",
    "    torch.save(trained_model.state_dict(), \"fasterrcnn_moving_detector_2.0.pth\")\n",
    "    print(\"\\nModel saved as 'fasterrcnn_moving_detector_2.pth'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link to .pth file in one drive, was too large to upload to github\n",
    "#https://uofnebraska-my.sharepoint.com/:u:/g/personal/51628718_nebraska_edu/IQCJ0x1cdpvdQJXmvP5vf1XLAcN6AUHbCMoGgel_l8_oEMM?e=qqaj9l\n",
    "\n",
    "#import script for trained model\n",
    "def load_trained_model(weights_path, num_classes=3, device=None):\n",
    "\n",
    "    # Select device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Loading model on device: {device}\")\n",
    "\n",
    "    # Recreate model architecture\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Load saved weights\n",
    "    state_dict = torch.load(weights_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Move model to device and set eval mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Model loaded successfully from '{weights_path}'\")\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
