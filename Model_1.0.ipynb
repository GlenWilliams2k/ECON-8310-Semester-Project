{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to correct scaling\n",
    "- scaling videos but not annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "# Convert a video into a tensor\n",
    "def video_to_tensor(video_path, resize=None, frame_skip=1):\n",
    "    print(f\"Loading video: {os.path.basename(video_path)}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if resize:\n",
    "            frame = cv2.resize(frame, resize)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "\n",
    "    if not frames:\n",
    "        raise ValueError(f\"No frames read from {video_path}\")\n",
    "\n",
    "    frames = np.stack(frames)[::frame_skip]\n",
    "    frames = torch.from_numpy(frames).float().permute(0, 3, 1, 2) / 255.0\n",
    "    print(f\"Loaded {frame_count} frames -> kept {frames.shape[0]} after skipping\\n\")\n",
    "    return frames  # shape: (T, 3, H, W)\n",
    "\n",
    "# Parse CVAT XML annotation file\n",
    "def parse_cvat_xml(xml_path, frame_skip=1):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    annotations = defaultdict(list)\n",
    "\n",
    "    for track in root.findall(\"track\"):\n",
    "        label = track.attrib[\"label\"]\n",
    "        for box in track.findall(\"box\"):\n",
    "            frame = int(box.attrib[\"frame\"])\n",
    "            outside = int(box.attrib[\"outside\"])\n",
    "            if outside != 0:\n",
    "                continue\n",
    "\n",
    "            xtl = float(box.attrib[\"xtl\"])\n",
    "            ytl = float(box.attrib[\"ytl\"])\n",
    "            xbr = float(box.attrib[\"xbr\"])\n",
    "            ybr = float(box.attrib[\"ybr\"])\n",
    "\n",
    "            moving_attr = box.find(\"attribute[@name='moving']\")\n",
    "            if moving_attr is None:\n",
    "                raise ValueError(f\"Missing 'moving' attribute in file {xml_path}, track '{label}', frame {frame}\")\n",
    "\n",
    "            moving_flag = 1 if moving_attr.text.lower() == \"true\" else 0\n",
    "\n",
    "            if frame % frame_skip == 0:\n",
    "                adjusted_frame = frame // frame_skip\n",
    "                annotations[adjusted_frame].append({\n",
    "                    \"label\": label,\n",
    "                    \"bbox\": [xtl, ytl, xbr, ybr],\n",
    "                    \"moving\": moving_flag})\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Dataset class that loads videos + XML annotations together\n",
    "class BaseballVideoDataset(Dataset):\n",
    "    def __init__(self, video_dir, xml_dir, resize=(1280, 720), frame_skip=1):\n",
    "        self.video_dir = video_dir\n",
    "        self.xml_dir = xml_dir\n",
    "        self.resize = resize\n",
    "        self.frame_skip = frame_skip\n",
    "        self.video_tensors = {}\n",
    "        self.skipped_videos = []\n",
    "        self.index_map = []\n",
    "\n",
    "        # Match videos with their annotation XMLs by filename stem\n",
    "        self.samples = []\n",
    "        for vid_name in os.listdir(video_dir):\n",
    "            if vid_name.lower().endswith((\".mp4\", \".mov\", \".avi\")):\n",
    "                stem = os.path.splitext(vid_name)[0]\n",
    "                xml_path = os.path.join(xml_dir, f\"{stem}.xml\")\n",
    "                if os.path.exists(xml_path):\n",
    "                    self.samples.append((os.path.join(video_dir, vid_name), xml_path))\n",
    "                else:\n",
    "                    print(f\"No XML found for {vid_name}\")\n",
    "        print(f\"\\n Found {len(self.samples)} videos with matching XMLs in {video_dir}\\n\")\n",
    "\n",
    "        print(\"Preloading videos and indexing frames...\\n\")\n",
    "\n",
    "        # Loop through all matched video/XML pairs\n",
    "        for vid_idx, (video_path, xml_path) in enumerate(self.samples, start=1):\n",
    "            try:\n",
    "                video_tensor = video_to_tensor(video_path, resize=self.resize, frame_skip=self.frame_skip)\n",
    "                annotations = parse_cvat_xml(xml_path, frame_skip=self.frame_skip)\n",
    "                self.video_tensors[video_path] = video_tensor\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {os.path.basename(video_path)}: {e}\")\n",
    "                self.skipped_videos.append((video_path, str(e)))\n",
    "                continue\n",
    "\n",
    "            # Build index map (frame-by-frame)\n",
    "            for frame_idx, ann_list in annotations.items():\n",
    "                if len(ann_list) == 0:\n",
    "                    continue\n",
    "                if frame_idx >= len(video_tensor):\n",
    "                    print(f\"Frame {frame_idx} out of range for {os.path.basename(video_path)} \"\n",
    "                          f\"(video has {len(video_tensor)} frames) — skipping\")\n",
    "                    continue\n",
    "\n",
    "                boxes = torch.tensor([a[\"bbox\"] for a in ann_list], dtype=torch.float32)\n",
    "                moving = torch.tensor([a[\"moving\"] for a in ann_list], dtype=torch.int64)\n",
    "\n",
    "                self.index_map.append((video_path, frame_idx, {\n",
    "                    \"boxes\": boxes,\n",
    "                    \"moving\": moving}))\n",
    "\n",
    "            print(f\"   [{vid_idx}/{len(self.samples)}] Loaded {os.path.basename(video_path)} \"\n",
    "                  f\"({len(video_tensor)} frames, {len(annotations)} annotated)\\n\")\n",
    "\n",
    "        print(f\"Finished indexing {len(self.index_map)} annotated frames \"\n",
    "              f\"from {len(self.video_tensors)} videos.\\n\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, frame_idx, target = self.index_map[idx]\n",
    "\n",
    "        video_tensor = self.video_tensors[video_path]\n",
    "        frame_tensor = video_tensor[frame_idx]  # (3, H, W)\n",
    "\n",
    "        moving_flags = target[\"moving\"]\n",
    "        labels = moving_flags.clone()\n",
    "        labels[labels == 0] = 1\n",
    "        labels[labels == 1] = 2\n",
    "\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"video\"] = os.path.basename(video_path)\n",
    "\n",
    "        return frame_tensor, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model, adjustable number of classes, not pretrained\n",
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = False)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def train_loop(model, dataloader, optimizer, device):\n",
    "    #set model to training mode\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
    "            images = [img.to(device) for img in images]\n",
    "            for t in targets:\n",
    "                t[\"boxes\"] = t[\"boxes\"].to(device)\n",
    "                t[\"labels\"] = t[\"labels\"].to(device)\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            #clear previous gradients\n",
    "            optimizer.zero_grad()\n",
    "            #compute new gradients via backpropagation\n",
    "            losses.backward()\n",
    "            #update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += losses.item()\n",
    "            if batch_idx % 5 == 0:\n",
    "                print(f\"Batch {batch_idx}/{len(dataloader)} | Loss: {losses.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss/len(dataloader)\n",
    "    print(f\"Average Training LossL: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "#evaluate the model\n",
    "@torch.no_grad() #prevent gradient updates\n",
    "def test_loop(model, dataloader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, targets in dataloader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        for t in targets:\n",
    "            t[\"boxes\"] = t[\"boxes\"].to(device)\n",
    "            t[\"labels\"] = t[\"labels\"].to(device)\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        total_loss += losses.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "#Function to measure model accuracy\n",
    "@torch.no_grad()\n",
    "def accuracy_loop(model, dataloader, device, score_thresh=0.5):\n",
    "    model.eval()\n",
    "    num_frames = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for images, targets in dataloader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        # move labels to device just in case\n",
    "        for t in targets:\n",
    "            t[\"labels\"] = t[\"labels\"].to(device)\n",
    "\n",
    "        predictions = model(images)\n",
    "\n",
    "        for pred, tgt in zip(predictions, targets):\n",
    "            # Ground truth: does this frame contain any moving object?\n",
    "            gt_any_moving = (tgt[\"labels\"] == 2).any().item()\n",
    "\n",
    "            # Predictions: keep only boxes above a confidence threshold\n",
    "            scores = pred[\"scores\"].to(device)\n",
    "            labels = pred[\"labels\"].to(device)\n",
    "            keep = scores >= score_thresh\n",
    "            pred_labels = labels[keep]\n",
    "\n",
    "            pred_any_moving = (pred_labels == 2).any().item()\n",
    "\n",
    "            num_frames += 1\n",
    "            if bool(gt_any_moving) == bool(pred_any_moving):\n",
    "                num_correct += 1\n",
    "\n",
    "    acc = num_correct / num_frames if num_frames > 0 else 0.0\n",
    "    print(f\"Frame-level moving/not-moving accuracy: {acc*100:.2f}% \"\n",
    "          f\"(threshold={score_thresh})\")\n",
    "    return acc\n",
    "\n",
    "def train_detector(train_dataset, val_dataset, num_classes=2, epochs=5, lr=1e-4, batch_size=4):\n",
    "    #use gpu if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = get_model(num_classes).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\\n----------------------------\")\n",
    "        train_loss = train_loop(model, train_loader, optimizer, device)\n",
    "        val_loss = test_loop(model, val_loader, device)\n",
    "        val_acc  = accuracy_loop(model, val_loader, device, score_thresh=0.5)\n",
    "        print(f\"Summary: train_loss={train_loss:.4f}, \" f\"val_loss={val_loss:.4f}, val_acc={val_acc*100:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Found 47 videos with matching XMLs in C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Raw Videos\n",
      "\n",
      "Preloading videos and indexing frames...\n",
      "\n",
      "Loading video: dusty_1.mov\n",
      "Loaded 76 frames -> kept 76 after skipping\n",
      "\n",
      "   [1/47] Loaded dusty_1.mov (76 frames, 76 annotated)\n",
      "\n",
      "Loading video: IMG_7917_dusty.mov\n",
      "Loaded 60 frames -> kept 60 after skipping\n",
      "\n",
      "   [2/47] Loaded IMG_7917_dusty.mov (60 frames, 60 annotated)\n",
      "\n",
      "Loading video: IMG_7918_dusty.mov\n",
      "Loaded 76 frames -> kept 76 after skipping\n",
      "\n",
      "   [3/47] Loaded IMG_7918_dusty.mov (76 frames, 76 annotated)\n",
      "\n",
      "Loading video: IMG_7919_dusty.mov\n",
      "Loaded 50 frames -> kept 50 after skipping\n",
      "\n",
      "   [4/47] Loaded IMG_7919_dusty.mov (50 frames, 50 annotated)\n",
      "\n",
      "Loading video: IMG_7942_dusty.mov\n",
      "Loaded 57 frames -> kept 57 after skipping\n",
      "\n",
      "   [5/47] Loaded IMG_7942_dusty.mov (57 frames, 57 annotated)\n",
      "\n",
      "Loading video: IMG_7943_khem.mov\n",
      "Loaded 51 frames -> kept 51 after skipping\n",
      "\n",
      "Frame 51 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
      "Frame 52 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
      "Frame 53 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
      "   [6/47] Loaded IMG_7943_khem.mov (51 frames, 54 annotated)\n",
      "\n",
      "Loading video: IMG_7944_khem.mov\n",
      "Loaded 54 frames -> kept 54 after skipping\n",
      "\n",
      "   [7/47] Loaded IMG_7944_khem.mov (54 frames, 37 annotated)\n",
      "\n",
      "Loading video: IMG_7997_khem.mov\n",
      "Loaded 37 frames -> kept 37 after skipping\n",
      "\n",
      "   [8/47] Loaded IMG_7997_khem.mov (37 frames, 37 annotated)\n",
      "\n",
      "Loading video: IMG_7998_khem.mov\n",
      "Loaded 55 frames -> kept 55 after skipping\n",
      "\n",
      "   [9/47] Loaded IMG_7998_khem.mov (55 frames, 55 annotated)\n",
      "\n",
      "Loading video: IMG_7999_joel.mov\n",
      "Loaded 56 frames -> kept 56 after skipping\n",
      "\n",
      "   [10/47] Loaded IMG_7999_joel.mov (56 frames, 56 annotated)\n",
      "\n",
      "Loading video: IMG_8027_joel.mov\n",
      "Loaded 54 frames -> kept 54 after skipping\n",
      "\n",
      "   [11/47] Loaded IMG_8027_joel.mov (54 frames, 54 annotated)\n",
      "\n",
      "Loading video: IMG_8028_joel.mov\n",
      "Loaded 53 frames -> kept 53 after skipping\n",
      "\n",
      "   [12/47] Loaded IMG_8028_joel.mov (53 frames, 53 annotated)\n",
      "\n",
      "Loading video: IMG_8029_joel.mov\n",
      "Loaded 54 frames -> kept 54 after skipping\n",
      "\n",
      "   [13/47] Loaded IMG_8029_joel.mov (54 frames, 54 annotated)\n",
      "\n",
      "Loading video: IMG_8030_patrick.mov\n",
      "Loaded 57 frames -> kept 57 after skipping\n",
      "\n",
      "Skipping IMG_8030_patrick.mov: Missing 'moving' attribute in file C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Annotations\\IMG_8030_patrick.xml, track 'Baseball', frame 30\n",
      "Loading video: IMG_8060_patrick.mov\n",
      "Loaded 58 frames -> kept 58 after skipping\n",
      "\n",
      "Skipping IMG_8060_patrick.mov: Missing 'moving' attribute in file C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Annotations\\IMG_8060_patrick.xml, track 'Baseball', frame 33\n",
      "Loading video: IMG_8061_patrick.mov\n",
      "Loaded 54 frames -> kept 54 after skipping\n",
      "\n",
      "Skipping IMG_8061_patrick.mov: Missing 'moving' attribute in file C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Annotations\\IMG_8061_patrick.xml, track 'Baseball', frame 32\n",
      "Loading video: IMG_8062_patrick.mov\n",
      "Loaded 53 frames -> kept 53 after skipping\n",
      "\n",
      "Skipping IMG_8062_patrick.mov: Missing 'moving' attribute in file C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Annotations\\IMG_8062_patrick.xml, track 'Baseball', frame 30\n",
      "Loading video: IMG_8063_scott.mov\n",
      "Loaded 50 frames -> kept 50 after skipping\n",
      "\n",
      "   [18/47] Loaded IMG_8063_scott.mov (50 frames, 50 annotated)\n",
      "\n",
      "Loading video: IMG_8121_scott.mov\n",
      "Loaded 55 frames -> kept 55 after skipping\n",
      "\n",
      "   [19/47] Loaded IMG_8121_scott.mov (55 frames, 55 annotated)\n",
      "\n",
      "Loading video: IMG_8122_scott.mov\n",
      "Loaded 53 frames -> kept 53 after skipping\n",
      "\n",
      "   [20/47] Loaded IMG_8122_scott.mov (53 frames, 53 annotated)\n",
      "\n",
      "Loading video: IMG_8123_scott.mov\n",
      "Loaded 58 frames -> kept 58 after skipping\n",
      "\n",
      "   [21/47] Loaded IMG_8123_scott.mov (58 frames, 58 annotated)\n",
      "\n",
      "Loading video: IMG_8124_joe.mov\n",
      "Loaded 58 frames -> kept 58 after skipping\n",
      "\n",
      "   [22/47] Loaded IMG_8124_joe.mov (58 frames, 58 annotated)\n",
      "\n",
      "Loading video: IMG_8138_joe.mov\n",
      "Loaded 55 frames -> kept 55 after skipping\n",
      "\n",
      "   [23/47] Loaded IMG_8138_joe.mov (55 frames, 6 annotated)\n",
      "\n",
      "Loading video: IMG_8139_joe.mov\n",
      "Loaded 63 frames -> kept 63 after skipping\n",
      "\n",
      "   [24/47] Loaded IMG_8139_joe.mov (63 frames, 3 annotated)\n",
      "\n",
      "Loading video: IMG_8140_joe.mov\n",
      "Loaded 45 frames -> kept 45 after skipping\n",
      "\n",
      "   [25/47] Loaded IMG_8140_joe.mov (45 frames, 4 annotated)\n",
      "\n",
      "Loading video: IMG_8141_amarnath.mov\n",
      "Loaded 57 frames -> kept 57 after skipping\n",
      "\n",
      "Skipping IMG_8141_amarnath.mov: Missing 'moving' attribute in file C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Annotations\\IMG_8141_amarnath.xml, track 'Baseball', frame 27\n",
      "Loading video: IMG_8223_amarnath.mov\n",
      "Loaded 52 frames -> kept 52 after skipping\n",
      "\n",
      "Skipping IMG_8223_amarnath.mov: Missing 'moving' attribute in file C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Annotations\\IMG_8223_amarnath.xml, track 'Baseball', frame 9\n",
      "Loading video: IMG_8224_amarnath.mov\n",
      "Loaded 57 frames -> kept 57 after skipping\n",
      "\n",
      "Skipping IMG_8224_amarnath.mov: Missing 'moving' attribute in file C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Annotations\\IMG_8224_amarnath.xml, track 'Baseball', frame 15\n",
      "Loading video: IMG_8225_amarnath.mov\n",
      "Loaded 40 frames -> kept 40 after skipping\n",
      "\n",
      "Skipping IMG_8225_amarnath.mov: Missing 'moving' attribute in file C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Annotations\\IMG_8225_amarnath.xml, track 'Baseball', frame 10\n",
      "Loading video: IMG_8226_jared.mov\n",
      "Loaded 48 frames -> kept 48 after skipping\n",
      "\n",
      "   [30/47] Loaded IMG_8226_jared.mov (48 frames, 48 annotated)\n",
      "\n",
      "Loading video: IMG_8241_jared.mov\n",
      "Loaded 55 frames -> kept 55 after skipping\n",
      "\n",
      "   [31/47] Loaded IMG_8241_jared.mov (55 frames, 55 annotated)\n",
      "\n",
      "Loading video: IMG_8242_jared.mov\n",
      "Loaded 51 frames -> kept 51 after skipping\n",
      "\n",
      "   [32/47] Loaded IMG_8242_jared.mov (51 frames, 51 annotated)\n",
      "\n",
      "Loading video: IMG_8243_jared.mov\n",
      "Loaded 51 frames -> kept 51 after skipping\n",
      "\n",
      "   [33/47] Loaded IMG_8243_jared.mov (51 frames, 51 annotated)\n",
      "\n",
      "Loading video: IMG_8252_zach.mov\n",
      "Loaded 55 frames -> kept 55 after skipping\n",
      "\n",
      "Skipping IMG_8252_zach.mov: Missing 'moving' attribute in file C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Annotations\\IMG_8252_zach.xml, track 'Baseball', frame 18\n",
      "Loading video: IMG_8255_zach.mov\n",
      "Loaded 44 frames -> kept 44 after skipping\n",
      "\n",
      "Skipping IMG_8255_zach.mov: Missing 'moving' attribute in file C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Annotations\\IMG_8255_zach.xml, track 'Baseball', frame 14\n",
      "Loading video: IMG_8256_zach.mov\n",
      "Loaded 53 frames -> kept 53 after skipping\n",
      "\n",
      "Skipping IMG_8256_zach.mov: Missing 'moving' attribute in file C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Annotations\\IMG_8256_zach.xml, track 'Baseball', frame 29\n",
      "Loading video: IMG_8257_zach.mov\n",
      "Loaded 57 frames -> kept 57 after skipping\n",
      "\n",
      "Skipping IMG_8257_zach.mov: Missing 'moving' attribute in file C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Annotations\\IMG_8257_zach.xml, track 'Baseball', frame 13\n",
      "Loading video: IMG_8923_souleymane.mov\n",
      "Loaded 45 frames -> kept 45 after skipping\n",
      "\n",
      "   [38/47] Loaded IMG_8923_souleymane.mov (45 frames, 45 annotated)\n",
      "\n",
      "Loading video: IMG_8924_souleymane.mov\n",
      "Loaded 66 frames -> kept 66 after skipping\n",
      "\n",
      "   [39/47] Loaded IMG_8924_souleymane.mov (66 frames, 66 annotated)\n",
      "\n",
      "Loading video: IMG_8946_souleymane.mov\n",
      "Loaded 94 frames -> kept 94 after skipping\n",
      "\n",
      "   [40/47] Loaded IMG_8946_souleymane.mov (94 frames, 94 annotated)\n",
      "\n",
      "Loading video: IMG_8947_souleymane.mov\n",
      "Loaded 82 frames -> kept 82 after skipping\n",
      "\n",
      "   [41/47] Loaded IMG_8947_souleymane.mov (82 frames, 82 annotated)\n",
      "\n",
      "Loading video: IMG_9197_hugo.mov\n",
      "Loaded 70 frames -> kept 70 after skipping\n",
      "\n",
      "   [42/47] Loaded IMG_9197_hugo.mov (70 frames, 70 annotated)\n",
      "\n",
      "Loading video: IMG_9198_joel.mov\n",
      "Loaded 50 frames -> kept 50 after skipping\n",
      "\n",
      "   [43/47] Loaded IMG_9198_joel.mov (50 frames, 50 annotated)\n",
      "\n",
      "Loading video: IMG_9199_hugo.mov\n",
      "Loaded 81 frames -> kept 81 after skipping\n",
      "\n",
      "   [44/47] Loaded IMG_9199_hugo.mov (81 frames, 81 annotated)\n",
      "\n",
      "Loading video: IMG_9435_hugo.mov\n",
      "Loaded 66 frames -> kept 66 after skipping\n",
      "\n",
      "   [45/47] Loaded IMG_9435_hugo.mov (66 frames, 66 annotated)\n",
      "\n",
      "Loading video: IMG_9607_hugo.mov\n",
      "Loaded 43 frames -> kept 43 after skipping\n",
      "\n",
      "   [46/47] Loaded IMG_9607_hugo.mov (43 frames, 43 annotated)\n",
      "\n",
      "Loading video: IMG_9609_dusty.mov\n",
      "Loaded 102 frames -> kept 102 after skipping\n",
      "\n",
      "   [47/47] Loaded IMG_9609_dusty.mov (102 frames, 102 annotated)\n",
      "\n",
      "Finished indexing 1907 annotated frames from 35 videos.\n",
      "\n",
      "\n",
      "Dataset contains 1907 annotated frames across videos.\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Glen\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Glen\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "----------------------------\n",
      "Batch 0/382 | Loss: 68.7677\n",
      "Batch 5/382 | Loss: 36.7203\n",
      "Batch 10/382 | Loss: 33.4778\n",
      "Batch 15/382 | Loss: 35.0697\n",
      "Batch 20/382 | Loss: 29.4796\n",
      "Batch 25/382 | Loss: 29.9194\n",
      "Batch 30/382 | Loss: 22.6410\n",
      "Batch 35/382 | Loss: 24.2589\n",
      "Batch 40/382 | Loss: 26.1701\n",
      "Batch 45/382 | Loss: 48.6600\n",
      "Batch 50/382 | Loss: 42.5242\n",
      "Batch 55/382 | Loss: 33.1072\n",
      "Batch 60/382 | Loss: 29.1607\n",
      "Batch 65/382 | Loss: 23.9734\n",
      "Batch 70/382 | Loss: 34.9886\n",
      "Batch 75/382 | Loss: 37.9772\n",
      "Batch 80/382 | Loss: 26.1650\n",
      "Batch 85/382 | Loss: 35.5376\n",
      "Batch 90/382 | Loss: 17.6388\n",
      "Batch 95/382 | Loss: 22.9503\n",
      "Batch 100/382 | Loss: 29.1248\n",
      "Batch 105/382 | Loss: 29.8221\n",
      "Batch 110/382 | Loss: 31.2881\n",
      "Batch 115/382 | Loss: 16.8766\n",
      "Batch 120/382 | Loss: 28.4048\n",
      "Batch 125/382 | Loss: 27.9400\n",
      "Batch 130/382 | Loss: 36.5510\n",
      "Batch 135/382 | Loss: 27.2610\n",
      "Batch 140/382 | Loss: 27.2815\n",
      "Batch 145/382 | Loss: 24.9686\n",
      "Batch 150/382 | Loss: 23.9547\n",
      "Batch 155/382 | Loss: 20.9938\n",
      "Batch 160/382 | Loss: 32.1563\n",
      "Batch 165/382 | Loss: 30.6232\n",
      "Batch 170/382 | Loss: 32.4390\n",
      "Batch 175/382 | Loss: 14.0786\n",
      "Batch 180/382 | Loss: 42.2898\n",
      "Batch 185/382 | Loss: 40.8139\n",
      "Batch 190/382 | Loss: 27.5973\n",
      "Batch 195/382 | Loss: 25.2747\n",
      "Batch 200/382 | Loss: 21.9130\n",
      "Batch 205/382 | Loss: 26.2412\n",
      "Batch 210/382 | Loss: 26.0538\n",
      "Batch 215/382 | Loss: 25.1259\n",
      "Batch 220/382 | Loss: 29.0815\n",
      "Batch 225/382 | Loss: 25.8447\n",
      "Batch 230/382 | Loss: 31.4088\n",
      "Batch 235/382 | Loss: 47.2908\n",
      "Batch 240/382 | Loss: 28.0438\n",
      "Batch 245/382 | Loss: 32.8464\n",
      "Batch 250/382 | Loss: 23.4662\n",
      "Batch 255/382 | Loss: 40.5994\n",
      "Batch 260/382 | Loss: 40.0949\n",
      "Batch 265/382 | Loss: 14.4883\n",
      "Batch 270/382 | Loss: 34.8724\n",
      "Batch 275/382 | Loss: 26.0537\n",
      "Batch 280/382 | Loss: 29.5462\n",
      "Batch 285/382 | Loss: 22.5934\n",
      "Batch 290/382 | Loss: 30.3907\n",
      "Batch 295/382 | Loss: 27.0164\n",
      "Batch 300/382 | Loss: 19.6533\n",
      "Batch 305/382 | Loss: 29.9371\n",
      "Batch 310/382 | Loss: 43.8486\n",
      "Batch 315/382 | Loss: 29.0196\n",
      "Batch 320/382 | Loss: 23.6397\n",
      "Batch 325/382 | Loss: 16.6226\n",
      "Batch 330/382 | Loss: 21.3984\n",
      "Batch 335/382 | Loss: 34.2579\n",
      "Batch 340/382 | Loss: 6.5086\n",
      "Batch 345/382 | Loss: 32.7049\n",
      "Batch 350/382 | Loss: 23.2402\n",
      "Batch 355/382 | Loss: 83.3349\n",
      "Batch 360/382 | Loss: 26.4751\n",
      "Batch 365/382 | Loss: 30.6384\n",
      "Batch 370/382 | Loss: 18.6598\n",
      "Batch 375/382 | Loss: 22.1766\n",
      "Batch 380/382 | Loss: 32.6945\n",
      "Average Training LossL: 29.6682\n",
      "Validation Loss: 28.2842\n",
      "Frame-level moving/not-moving accuracy: 0.00% (threshold=0.5)\n",
      "Summary: train_loss=29.6682, val_loss=28.2842, val_acc=0.00%\n",
      "\n",
      "Epoch 2/5\n",
      "----------------------------\n",
      "Batch 0/382 | Loss: 25.3196\n",
      "Batch 5/382 | Loss: 38.4325\n",
      "Batch 10/382 | Loss: 32.6959\n",
      "Batch 15/382 | Loss: 22.9045\n",
      "Batch 20/382 | Loss: 8.6607\n",
      "Batch 25/382 | Loss: 25.1737\n",
      "Batch 30/382 | Loss: 24.4331\n",
      "Batch 35/382 | Loss: 25.5744\n",
      "Batch 40/382 | Loss: 36.2154\n",
      "Batch 45/382 | Loss: 22.3593\n",
      "Batch 50/382 | Loss: 21.1851\n",
      "Batch 55/382 | Loss: 25.8283\n",
      "Batch 60/382 | Loss: 26.6391\n",
      "Batch 65/382 | Loss: 15.9443\n",
      "Batch 70/382 | Loss: 30.0395\n",
      "Batch 75/382 | Loss: 33.8635\n",
      "Batch 80/382 | Loss: 24.2687\n",
      "Batch 85/382 | Loss: 23.7595\n",
      "Batch 90/382 | Loss: 16.4202\n",
      "Batch 95/382 | Loss: 26.0259\n",
      "Batch 100/382 | Loss: 13.8005\n",
      "Batch 105/382 | Loss: 29.3489\n",
      "Batch 110/382 | Loss: 10.5173\n",
      "Batch 115/382 | Loss: 28.1926\n",
      "Batch 120/382 | Loss: 23.7650\n",
      "Batch 125/382 | Loss: 31.6041\n",
      "Batch 130/382 | Loss: 27.2406\n",
      "Batch 135/382 | Loss: 25.9702\n",
      "Batch 140/382 | Loss: 36.9762\n",
      "Batch 145/382 | Loss: 27.9709\n",
      "Batch 150/382 | Loss: 37.9792\n",
      "Batch 155/382 | Loss: 34.8426\n",
      "Batch 160/382 | Loss: 38.0475\n",
      "Batch 165/382 | Loss: 30.4151\n",
      "Batch 170/382 | Loss: 12.9298\n",
      "Batch 175/382 | Loss: 16.5984\n",
      "Batch 180/382 | Loss: 28.0028\n",
      "Batch 185/382 | Loss: 32.8069\n",
      "Batch 190/382 | Loss: 26.8859\n",
      "Batch 195/382 | Loss: 39.1435\n",
      "Batch 200/382 | Loss: 34.7924\n",
      "Batch 205/382 | Loss: 32.1962\n",
      "Batch 210/382 | Loss: 20.4486\n",
      "Batch 215/382 | Loss: 21.3207\n",
      "Batch 220/382 | Loss: 25.1839\n",
      "Batch 225/382 | Loss: 25.4782\n",
      "Batch 230/382 | Loss: 24.8481\n",
      "Batch 235/382 | Loss: 25.8647\n",
      "Batch 240/382 | Loss: 24.7234\n",
      "Batch 245/382 | Loss: 22.2937\n",
      "Batch 250/382 | Loss: 25.6963\n",
      "Batch 255/382 | Loss: 14.2677\n",
      "Batch 260/382 | Loss: 22.1143\n",
      "Batch 265/382 | Loss: 27.2405\n",
      "Batch 270/382 | Loss: 24.8496\n",
      "Batch 275/382 | Loss: 20.9990\n",
      "Batch 280/382 | Loss: 27.5723\n",
      "Batch 285/382 | Loss: 13.1242\n",
      "Batch 290/382 | Loss: 12.0138\n",
      "Batch 295/382 | Loss: 26.1202\n",
      "Batch 300/382 | Loss: 29.4282\n",
      "Batch 305/382 | Loss: 24.5894\n",
      "Batch 310/382 | Loss: 29.0941\n",
      "Batch 315/382 | Loss: 28.0757\n",
      "Batch 320/382 | Loss: 21.2853\n",
      "Batch 325/382 | Loss: 32.2178\n",
      "Batch 330/382 | Loss: 28.1594\n",
      "Batch 335/382 | Loss: 34.9507\n",
      "Batch 340/382 | Loss: 16.7549\n",
      "Batch 345/382 | Loss: 31.4765\n",
      "Batch 350/382 | Loss: 19.5434\n",
      "Batch 355/382 | Loss: 31.7793\n",
      "Batch 360/382 | Loss: 24.3451\n",
      "Batch 365/382 | Loss: 24.7334\n",
      "Batch 370/382 | Loss: 19.9130\n",
      "Batch 375/382 | Loss: 17.4248\n",
      "Batch 380/382 | Loss: 33.4458\n",
      "Average Training LossL: 26.6873\n",
      "Validation Loss: 30.8082\n",
      "Frame-level moving/not-moving accuracy: 15.45% (threshold=0.5)\n",
      "Summary: train_loss=26.6873, val_loss=30.8082, val_acc=15.45%\n",
      "\n",
      "Epoch 3/5\n",
      "----------------------------\n",
      "Batch 0/382 | Loss: 29.7809\n",
      "Batch 5/382 | Loss: 30.3801\n",
      "Batch 10/382 | Loss: 30.9753\n",
      "Batch 15/382 | Loss: 39.0696\n",
      "Batch 20/382 | Loss: 26.0359\n",
      "Batch 25/382 | Loss: 26.9942\n",
      "Batch 30/382 | Loss: 22.6881\n",
      "Batch 35/382 | Loss: 32.8828\n",
      "Batch 40/382 | Loss: 26.3055\n",
      "Batch 45/382 | Loss: 33.3467\n",
      "Batch 50/382 | Loss: 32.2420\n",
      "Batch 55/382 | Loss: 25.4960\n",
      "Batch 60/382 | Loss: 16.9063\n",
      "Batch 65/382 | Loss: 33.3895\n",
      "Batch 70/382 | Loss: 20.5336\n",
      "Batch 75/382 | Loss: 29.9775\n",
      "Batch 80/382 | Loss: 16.7813\n",
      "Batch 85/382 | Loss: 19.4237\n",
      "Batch 90/382 | Loss: 26.3087\n",
      "Batch 95/382 | Loss: 23.2438\n",
      "Batch 100/382 | Loss: 27.2671\n",
      "Batch 105/382 | Loss: 17.0359\n",
      "Batch 110/382 | Loss: 23.6082\n",
      "Batch 115/382 | Loss: 41.7020\n",
      "Batch 120/382 | Loss: 22.0790\n",
      "Batch 125/382 | Loss: 16.9065\n",
      "Batch 130/382 | Loss: 32.6904\n",
      "Batch 135/382 | Loss: 18.3790\n",
      "Batch 140/382 | Loss: 22.4759\n",
      "Batch 145/382 | Loss: 16.1569\n",
      "Batch 150/382 | Loss: 21.6094\n",
      "Batch 155/382 | Loss: 15.4151\n",
      "Batch 160/382 | Loss: 28.6894\n",
      "Batch 165/382 | Loss: 28.4414\n",
      "Batch 170/382 | Loss: 27.8091\n",
      "Batch 175/382 | Loss: 15.2483\n",
      "Batch 180/382 | Loss: 15.3678\n",
      "Batch 185/382 | Loss: 14.0035\n",
      "Batch 190/382 | Loss: 23.0357\n",
      "Batch 195/382 | Loss: 9.7247\n",
      "Batch 200/382 | Loss: 15.1540\n",
      "Batch 205/382 | Loss: 28.4802\n",
      "Batch 210/382 | Loss: 25.0871\n",
      "Batch 215/382 | Loss: 7.6387\n",
      "Batch 220/382 | Loss: 22.8366\n",
      "Batch 225/382 | Loss: 16.7304\n",
      "Batch 230/382 | Loss: 18.7374\n",
      "Batch 235/382 | Loss: 12.9841\n",
      "Batch 240/382 | Loss: 33.8070\n",
      "Batch 245/382 | Loss: 47.6942\n",
      "Batch 250/382 | Loss: 21.2117\n",
      "Batch 255/382 | Loss: 25.9481\n",
      "Batch 260/382 | Loss: 15.6333\n",
      "Batch 265/382 | Loss: 18.2661\n",
      "Batch 270/382 | Loss: 30.2840\n",
      "Batch 275/382 | Loss: 15.6142\n",
      "Batch 280/382 | Loss: 25.8987\n",
      "Batch 285/382 | Loss: 9.8766\n",
      "Batch 290/382 | Loss: 17.0712\n",
      "Batch 295/382 | Loss: 26.7960\n",
      "Batch 300/382 | Loss: 35.4774\n",
      "Batch 305/382 | Loss: 39.8901\n",
      "Batch 310/382 | Loss: 23.4370\n",
      "Batch 315/382 | Loss: 28.6932\n",
      "Batch 320/382 | Loss: 6.5147\n",
      "Batch 325/382 | Loss: 16.0612\n",
      "Batch 330/382 | Loss: 19.2815\n",
      "Batch 335/382 | Loss: 25.3831\n",
      "Batch 340/382 | Loss: 17.5238\n",
      "Batch 345/382 | Loss: 26.4649\n",
      "Batch 350/382 | Loss: 20.1666\n",
      "Batch 355/382 | Loss: 14.2077\n",
      "Batch 360/382 | Loss: 23.1725\n",
      "Batch 365/382 | Loss: 12.3431\n",
      "Batch 370/382 | Loss: 28.5845\n",
      "Batch 375/382 | Loss: 31.4349\n",
      "Batch 380/382 | Loss: 27.9329\n",
      "Average Training LossL: 23.7472\n",
      "Validation Loss: 22.6485\n",
      "Frame-level moving/not-moving accuracy: 0.00% (threshold=0.5)\n",
      "Summary: train_loss=23.7472, val_loss=22.6485, val_acc=0.00%\n",
      "\n",
      "Epoch 4/5\n",
      "----------------------------\n",
      "Batch 0/382 | Loss: 12.7946\n",
      "Batch 5/382 | Loss: 26.3389\n",
      "Batch 10/382 | Loss: 15.7031\n",
      "Batch 15/382 | Loss: 29.3910\n",
      "Batch 20/382 | Loss: 14.3969\n",
      "Batch 25/382 | Loss: 26.6707\n",
      "Batch 30/382 | Loss: 30.8342\n",
      "Batch 35/382 | Loss: 8.4226\n",
      "Batch 40/382 | Loss: 13.6002\n",
      "Batch 45/382 | Loss: 19.0749\n",
      "Batch 50/382 | Loss: 11.0908\n",
      "Batch 55/382 | Loss: 14.0859\n",
      "Batch 60/382 | Loss: 11.5886\n",
      "Batch 65/382 | Loss: 18.0373\n",
      "Batch 70/382 | Loss: 13.5099\n",
      "Batch 75/382 | Loss: 15.2018\n",
      "Batch 80/382 | Loss: 13.7230\n",
      "Batch 85/382 | Loss: 12.8475\n",
      "Batch 90/382 | Loss: 29.8413\n",
      "Batch 95/382 | Loss: 22.8692\n",
      "Batch 100/382 | Loss: 14.6988\n",
      "Batch 105/382 | Loss: 12.5850\n",
      "Batch 110/382 | Loss: 15.8879\n",
      "Batch 115/382 | Loss: 14.5641\n",
      "Batch 120/382 | Loss: 7.2930\n",
      "Batch 125/382 | Loss: 19.3165\n",
      "Batch 130/382 | Loss: 19.4252\n",
      "Batch 135/382 | Loss: 26.4652\n",
      "Batch 140/382 | Loss: 12.2446\n",
      "Batch 145/382 | Loss: 10.9703\n",
      "Batch 150/382 | Loss: 8.6352\n",
      "Batch 155/382 | Loss: 14.9764\n",
      "Batch 160/382 | Loss: 10.5356\n",
      "Batch 165/382 | Loss: 28.3183\n",
      "Batch 170/382 | Loss: 24.9654\n",
      "Batch 175/382 | Loss: 21.7627\n",
      "Batch 180/382 | Loss: 3.7778\n",
      "Batch 185/382 | Loss: 28.8223\n",
      "Batch 190/382 | Loss: 27.6411\n",
      "Batch 195/382 | Loss: 23.9215\n",
      "Batch 200/382 | Loss: 19.1704\n",
      "Batch 205/382 | Loss: 18.6891\n",
      "Batch 210/382 | Loss: 21.1724\n",
      "Batch 215/382 | Loss: 46.8378\n",
      "Batch 220/382 | Loss: 11.4965\n",
      "Batch 225/382 | Loss: 25.5893\n",
      "Batch 230/382 | Loss: 9.4015\n",
      "Batch 235/382 | Loss: 38.9399\n",
      "Batch 240/382 | Loss: 13.5653\n",
      "Batch 245/382 | Loss: 29.2681\n",
      "Batch 250/382 | Loss: 19.7783\n",
      "Batch 255/382 | Loss: 16.0472\n",
      "Batch 260/382 | Loss: 7.7231\n",
      "Batch 265/382 | Loss: 23.4118\n",
      "Batch 270/382 | Loss: 18.5828\n",
      "Batch 275/382 | Loss: 9.2006\n",
      "Batch 280/382 | Loss: 16.5303\n",
      "Batch 285/382 | Loss: 24.7846\n",
      "Batch 290/382 | Loss: 28.0758\n",
      "Batch 295/382 | Loss: 18.8801\n",
      "Batch 300/382 | Loss: 16.6599\n",
      "Batch 305/382 | Loss: 29.3165\n",
      "Batch 310/382 | Loss: 25.7995\n",
      "Batch 315/382 | Loss: 19.2703\n",
      "Batch 320/382 | Loss: 15.6228\n",
      "Batch 325/382 | Loss: 13.2669\n",
      "Batch 330/382 | Loss: 26.8633\n",
      "Batch 335/382 | Loss: 20.9960\n",
      "Batch 340/382 | Loss: 10.8993\n",
      "Batch 345/382 | Loss: 17.4019\n",
      "Batch 350/382 | Loss: 19.1058\n",
      "Batch 355/382 | Loss: 10.1378\n",
      "Batch 360/382 | Loss: 28.1783\n",
      "Batch 365/382 | Loss: 22.5496\n",
      "Batch 370/382 | Loss: 7.6789\n",
      "Batch 375/382 | Loss: 14.8441\n",
      "Batch 380/382 | Loss: 6.5126\n",
      "Average Training LossL: 19.9570\n",
      "Validation Loss: 21.2204\n",
      "Frame-level moving/not-moving accuracy: 0.79% (threshold=0.5)\n",
      "Summary: train_loss=19.9570, val_loss=21.2204, val_acc=0.79%\n",
      "\n",
      "Epoch 5/5\n",
      "----------------------------\n",
      "Batch 0/382 | Loss: 11.8918\n",
      "Batch 5/382 | Loss: 15.7662\n",
      "Batch 10/382 | Loss: 21.9339\n",
      "Batch 15/382 | Loss: 22.3309\n",
      "Batch 20/382 | Loss: 22.3251\n",
      "Batch 25/382 | Loss: 13.9479\n",
      "Batch 30/382 | Loss: 7.8784\n",
      "Batch 35/382 | Loss: 25.5434\n",
      "Batch 40/382 | Loss: 14.2787\n",
      "Batch 45/382 | Loss: 6.6242\n",
      "Batch 50/382 | Loss: 24.0665\n",
      "Batch 55/382 | Loss: 40.8413\n",
      "Batch 60/382 | Loss: 20.5544\n",
      "Batch 65/382 | Loss: 12.2080\n",
      "Batch 70/382 | Loss: 17.0472\n",
      "Batch 75/382 | Loss: 19.3752\n",
      "Batch 80/382 | Loss: 9.0253\n",
      "Batch 85/382 | Loss: 21.2460\n",
      "Batch 90/382 | Loss: 15.3481\n",
      "Batch 95/382 | Loss: 17.9146\n",
      "Batch 100/382 | Loss: 33.9764\n",
      "Batch 105/382 | Loss: 19.5223\n",
      "Batch 110/382 | Loss: 10.9628\n",
      "Batch 115/382 | Loss: 17.3271\n",
      "Batch 120/382 | Loss: 17.4542\n",
      "Batch 125/382 | Loss: 15.3722\n",
      "Batch 130/382 | Loss: 9.1345\n",
      "Batch 135/382 | Loss: 10.8023\n",
      "Batch 140/382 | Loss: 20.2852\n",
      "Batch 145/382 | Loss: 21.7666\n",
      "Batch 150/382 | Loss: 23.3832\n",
      "Batch 155/382 | Loss: 11.7279\n",
      "Batch 160/382 | Loss: 32.0656\n",
      "Batch 165/382 | Loss: 7.9844\n",
      "Batch 170/382 | Loss: 17.8725\n",
      "Batch 175/382 | Loss: 5.8093\n",
      "Batch 180/382 | Loss: 9.7323\n",
      "Batch 185/382 | Loss: 21.8089\n",
      "Batch 190/382 | Loss: 15.1714\n",
      "Batch 195/382 | Loss: 26.1975\n",
      "Batch 200/382 | Loss: 21.2361\n",
      "Batch 205/382 | Loss: 22.2205\n",
      "Batch 210/382 | Loss: 10.4828\n",
      "Batch 215/382 | Loss: 9.8127\n",
      "Batch 220/382 | Loss: 12.3929\n",
      "Batch 225/382 | Loss: 19.4200\n",
      "Batch 230/382 | Loss: 31.2973\n",
      "Batch 235/382 | Loss: 11.3586\n",
      "Batch 240/382 | Loss: 18.6488\n",
      "Batch 245/382 | Loss: 24.1657\n",
      "Batch 250/382 | Loss: 15.6219\n",
      "Batch 255/382 | Loss: 24.2526\n",
      "Batch 260/382 | Loss: 19.7689\n",
      "Batch 265/382 | Loss: 13.2024\n",
      "Batch 270/382 | Loss: 6.7453\n",
      "Batch 275/382 | Loss: 26.9219\n",
      "Batch 280/382 | Loss: 10.9299\n",
      "Batch 285/382 | Loss: 10.9281\n",
      "Batch 290/382 | Loss: 24.4234\n",
      "Batch 295/382 | Loss: 24.6577\n",
      "Batch 300/382 | Loss: 16.0480\n",
      "Batch 305/382 | Loss: 17.0149\n",
      "Batch 310/382 | Loss: 20.1118\n",
      "Batch 315/382 | Loss: 7.0432\n",
      "Batch 320/382 | Loss: 9.8630\n",
      "Batch 325/382 | Loss: 23.1122\n",
      "Batch 330/382 | Loss: 14.0837\n",
      "Batch 335/382 | Loss: 13.1673\n",
      "Batch 340/382 | Loss: 30.2889\n",
      "Batch 345/382 | Loss: 23.1056\n",
      "Batch 350/382 | Loss: 32.5343\n",
      "Batch 355/382 | Loss: 13.5339\n",
      "Batch 360/382 | Loss: 17.3552\n",
      "Batch 365/382 | Loss: 15.7393\n",
      "Batch 370/382 | Loss: 19.6836\n",
      "Batch 375/382 | Loss: 5.7227\n",
      "Batch 380/382 | Loss: 17.0441\n",
      "Average Training LossL: 17.8999\n",
      "Validation Loss: 19.6558\n",
      "Frame-level moving/not-moving accuracy: 0.52% (threshold=0.5)\n",
      "Summary: train_loss=17.8999, val_loss=19.6558, val_acc=0.52%\n"
     ]
    }
   ],
   "source": [
    "#train and save trained model\n",
    "if __name__ == \"__main__\":\n",
    "    video_folder = r\"C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Raw Videos\"\n",
    "    xml_folder   = r\"C:\\Users\\Glen\\Documents\\School\\BusForecasting\\Final Project Folder\\Annotations\"\n",
    "\n",
    "    #Collate function for object detection\n",
    "    def collate_fn(batch):\n",
    "        batch = [b for b in batch if b is not None]\n",
    "        frames = [b[0] for b in batch]\n",
    "        targets = [b[1] for b in batch]\n",
    "        return frames, targets\n",
    "\n",
    "    #Load full dataset (videos + XMLs)\n",
    "    full_dataset = BaseballVideoDataset(video_folder, xml_folder, frame_skip=1)\n",
    "\n",
    "    #print a quick summary\n",
    "    print(f\"\\nDataset contains {len(full_dataset)} annotated frames across videos.\")\n",
    "\n",
    "    #Split train/test\n",
    "    n = len(full_dataset)\n",
    "    split = int(0.8 * n)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [split, n - split])\n",
    "\n",
    "    #Train the model\n",
    "    trained_model = train_detector(\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        num_classes=3,\n",
    "        epochs=5,\n",
    "        lr=1e-4,\n",
    "        batch_size=4)\n",
    "\n",
    "    #Save trained model\n",
    "    #torch.save(trained_model.state_dict(), \"fasterrcnn_moving_detector.pth\")\n",
    "    #print(\"\\nModel saved as 'fasterrcnn_moving_detector.pth'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link to .pth file in one drive, was too large to upload to github\n",
    "#https://uofnebraska-my.sharepoint.com/:u:/g/personal/51628718_nebraska_edu/EeMMKq6VaAdLqhyOtLBWLckB7RRR-rMLfBkvvxsgIL99PA?e=HWLXBi\n",
    "\n",
    "#import script for trained model\n",
    "def load_trained_model(weights_path, num_classes=3, device=None):\n",
    "\n",
    "    # Select device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Loading model on device: {device}\")\n",
    "\n",
    "    # Recreate model architecture\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Load saved weights\n",
    "    state_dict = torch.load(weights_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Move model to device and set eval mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Model loaded successfully from '{weights_path}'\")\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
