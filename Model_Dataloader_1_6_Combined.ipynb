{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87usWDMP8KdN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "\n",
        "# Convert a video into a tensor\n",
        "def video_to_tensor(video_path, resize=None, frame_skip=1, return_orig_size = False):\n",
        "    print(f\"Loading video: {os.path.basename(video_path)}\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    orig_w = None\n",
        "    Orig_h = None\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if orig_w is None:\n",
        "            orig_h, orig_w = frame.shape[:2]\n",
        "        if resize:\n",
        "            frame = cv2.resize(frame, resize)\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "\n",
        "    if not frames:\n",
        "        raise ValueError(f\"No frames read from {video_path}\")\n",
        "\n",
        "    frames = np.stack(frames)[::frame_skip]\n",
        "    frames = torch.from_numpy(frames).float().permute(0, 3, 1, 2) / 255.0\n",
        "    print(f\"Loaded {frame_count} frames -> kept {frames.shape[0]} after skipping\\n\")\n",
        "\n",
        "    if return_orig_size:\n",
        "        return frames, (orig_w, orig_h)\n",
        "    else:\n",
        "        return frames  # shape: (T, 3, H, W)\n",
        "\n",
        "# Parse CVAT XML annotation file\n",
        "def parse_cvat_xml(xml_path, frame_skip=1, scale=None):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    annotations = defaultdict(list)\n",
        "\n",
        "    if scale is None:\n",
        "        sx = sy = 1.0\n",
        "    else:\n",
        "        sx, sy = scale\n",
        "\n",
        "    for track in root.findall(\"track\"):\n",
        "        label = track.attrib[\"label\"]\n",
        "        for box in track.findall(\"box\"):\n",
        "            frame = int(box.attrib[\"frame\"])\n",
        "            outside = int(box.attrib[\"outside\"])\n",
        "            if outside != 0:\n",
        "                continue\n",
        "\n",
        "            xtl = float(box.attrib[\"xtl\"])\n",
        "            ytl = float(box.attrib[\"ytl\"])\n",
        "            xbr = float(box.attrib[\"xbr\"])\n",
        "            ybr = float(box.attrib[\"ybr\"])\n",
        "\n",
        "            #scale bbox into resized frame coordinates if scale != 1\n",
        "            xtl *= sx\n",
        "            xbr *= sx\n",
        "            ytl *= sy\n",
        "            ybr *= sy\n",
        "\n",
        "            moving_attr = None\n",
        "            for attr in box.findall(\"attribute\"):\n",
        "                if attr.attrib.get(\"name\", \"\").lower() == \"moving\":\n",
        "                    moving_attr = attr\n",
        "                    break\n",
        "\n",
        "            if moving_attr is None:\n",
        "                raise ValueError(f\"Missing 'moving' attribute in file {xml_path}, track '{label}', frame {frame}\")\n",
        "\n",
        "            moving_flag = 1 if moving_attr.text.lower() == \"true\" else 0\n",
        "\n",
        "            #frame skip alignment\n",
        "            if frame % frame_skip == 0:\n",
        "                adjusted_frame = frame // frame_skip\n",
        "                annotations[adjusted_frame].append({\n",
        "                    \"label\": label,\n",
        "                    \"bbox\": [xtl, ytl, xbr, ybr],\n",
        "                    \"moving\": moving_flag})\n",
        "\n",
        "    return annotations\n",
        "\n",
        "# Dataset class that loads videos + XML annotations together\n",
        "class BaseballVideoDataset(Dataset):\n",
        "    def __init__(self, video_dir, xml_dir, resize=(1280, 720), frame_skip=1, scale_boxes=True):\n",
        "        self.video_dir = video_dir\n",
        "        self.xml_dir = xml_dir\n",
        "        self.resize = resize\n",
        "        self.frame_skip = frame_skip\n",
        "        self.scale_boxes = scale_boxes\n",
        "        self.video_tensors = {}\n",
        "        self.skipped_videos = []\n",
        "        self.index_map = []\n",
        "\n",
        "        # Match videos with their annotation XMLs by filename stem\n",
        "        self.samples = []\n",
        "        for vid_name in os.listdir(video_dir):\n",
        "            if vid_name.lower().endswith((\".mp4\", \".mov\", \".avi\")):\n",
        "                stem = os.path.splitext(vid_name)[0]\n",
        "                xml_path = os.path.join(xml_dir, f\"{stem}.xml\")\n",
        "                if os.path.exists(xml_path):\n",
        "                    self.samples.append((os.path.join(video_dir, vid_name), xml_path))\n",
        "                else:\n",
        "                    print(f\"No XML found for {vid_name}\")\n",
        "        print(f\"\\n Found {len(self.samples)} videos with matching XMLs in {video_dir}\\n\")\n",
        "\n",
        "        print(\"Preloading videos and indexing frames...\\n\")\n",
        "\n",
        "        # Loop through all matched video/XML pairs\n",
        "        for vid_idx, (video_path, xml_path) in enumerate(self.samples, start=1):\n",
        "            try:\n",
        "                video_tensor, (orig_w, orig_h) = video_to_tensor(video_path, resize=self.resize, frame_skip=self.frame_skip, return_orig_size = True)\n",
        "\n",
        "            #compute scale factors for bboxes\n",
        "                if self.resize is not None and self.scale_boxes:\n",
        "                    new_w, new_h = self.resize\n",
        "                    sx = new_w/float(orig_w)\n",
        "                    sy = new_h/float(orig_h)\n",
        "                    scale = (sx, sy)\n",
        "                else:\n",
        "                    scale = None\n",
        "\n",
        "                #parse annotations in scaled coordinates\n",
        "                annotations = parse_cvat_xml(xml_path, frame_skip = self.frame_skip, scale = scale)\n",
        "                self.video_tensors[video_path] = video_tensor\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {os.path.basename(video_path)}: {e}\")\n",
        "                self.skipped_videos.append((video_path, str(e)))\n",
        "                continue\n",
        "\n",
        "            # Build frame-by-frame index map\n",
        "            for frame_idx, ann_list in annotations.items():\n",
        "                if len(ann_list) == 0:\n",
        "                    continue\n",
        "                if frame_idx >= len(video_tensor):\n",
        "                    print(f\"Frame {frame_idx} out of range for {os.path.basename(video_path)} \"\n",
        "                          f\"(video has {len(video_tensor)} frames) — skipping\")\n",
        "                    continue\n",
        "\n",
        "                boxes = torch.tensor([a[\"bbox\"] for a in ann_list], dtype=torch.float32)\n",
        "                moving = torch.tensor([a[\"moving\"] for a in ann_list], dtype=torch.int64)\n",
        "\n",
        "                self.index_map.append((video_path, frame_idx, {\"boxes\": boxes,\"moving\": moving}))\n",
        "\n",
        "            print(f\"   [{vid_idx}/{len(self.samples)}] Loaded {os.path.basename(video_path)} \"\n",
        "                  f\"({len(video_tensor)} frames, {len(annotations)} annotated)\\n\")\n",
        "\n",
        "        print(f\"Finished indexing {len(self.index_map)} annotated frames \"\n",
        "              f\"from {len(self.video_tensors)} videos.\\n\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, frame_idx, target = self.index_map[idx]\n",
        "\n",
        "        video_tensor = self.video_tensors[video_path]\n",
        "        frame_tensor = video_tensor[frame_idx]  # (3, H, W)\n",
        "\n",
        "        #convert flags to int, and configure labels for model interpretation\n",
        "        moving_flags = target[\"moving\"].to(torch.int64)\n",
        "        labels = moving_flags + 1\n",
        "\n",
        "        target_out = {\n",
        "            \"boxes\": target[\"boxes\"],\n",
        "            \"moving\": moving_flags,\n",
        "            \"labels\": labels,\n",
        "            \"video\": os.path.basename(video_path)}\n",
        "\n",
        "        return frame_tensor, target_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BDMDKlW8Uz_",
        "outputId": "d69e1c36-339c-4ecf-aa9e-9c13dfd85ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS7s2MNm8KdQ"
      },
      "outputs": [],
      "source": [
        "#define the model, adjustable number of classes, not pretrained\n",
        "def get_model(num_classes):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def train_loop(model, dataloader, optimizer, device):\n",
        "    #set model to training mode\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
        "            images = [img.to(device) for img in images]\n",
        "            for t in targets:\n",
        "                t[\"boxes\"] = t[\"boxes\"].to(device)\n",
        "                t[\"labels\"] = t[\"labels\"].to(device)\n",
        "\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            #clear previous gradients\n",
        "            optimizer.zero_grad()\n",
        "            #compute new gradients via backpropagation\n",
        "            losses.backward()\n",
        "            #update model weights\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += losses.item()\n",
        "            if batch_idx % 5 == 0:\n",
        "                print(f\"Batch {batch_idx}/{len(dataloader)} | Loss: {losses.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss/len(dataloader)\n",
        "    print(f\"Average Training LossL: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "#evaluate the model\n",
        "@torch.no_grad() #prevent gradient updates\n",
        "def test_loop(model, dataloader, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for images, targets in dataloader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        for t in targets:\n",
        "            t[\"boxes\"] = t[\"boxes\"].to(device)\n",
        "            t[\"labels\"] = t[\"labels\"].to(device)\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        total_loss += losses.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "#Function to measure model accuracy\n",
        "@torch.no_grad()\n",
        "def accuracy_loop(model, dataloader, device, score_thresh=0.5):\n",
        "    model.eval()\n",
        "    num_frames = 0\n",
        "    num_correct = 0\n",
        "\n",
        "    for images, targets in dataloader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        # move labels to device just in case\n",
        "        for t in targets:\n",
        "            t[\"labels\"] = t[\"labels\"].to(device)\n",
        "\n",
        "        predictions = model(images)\n",
        "\n",
        "        for pred, tgt in zip(predictions, targets):\n",
        "            # Ground truth: does this frame contain any moving object?\n",
        "            gt_any_moving = (tgt[\"labels\"] == 2).any().item()\n",
        "\n",
        "            # Predictions: keep only boxes above a confidence threshold\n",
        "            scores = pred[\"scores\"].to(device)\n",
        "            labels = pred[\"labels\"].to(device)\n",
        "            keep = scores >= score_thresh\n",
        "            pred_labels = labels[keep]\n",
        "\n",
        "            pred_any_moving = (pred_labels == 2).any().item()\n",
        "\n",
        "            num_frames += 1\n",
        "            if bool(gt_any_moving) == bool(pred_any_moving):\n",
        "                num_correct += 1\n",
        "\n",
        "    acc = num_correct / num_frames if num_frames > 0 else 0.0\n",
        "    print(f\"Frame-level moving/not-moving accuracy: {acc*100:.2f}% \"\n",
        "          f\"(threshold={score_thresh})\")\n",
        "    return acc\n",
        "\n",
        "def train_detector(train_dataset, val_dataset, num_classes=2, epochs=5, lr=1e-4, batch_size=4):\n",
        "    #use gpu if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    model = get_model(num_classes).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\\n----------------------------\")\n",
        "        train_loss = train_loop(model, train_loader, optimizer, device)\n",
        "        val_loss = test_loop(model, val_loader, device)\n",
        "        val_acc  = accuracy_loop(model, val_loader, device, score_thresh=0.5)\n",
        "        print(f\"Summary: train_loss={train_loss:.4f}, \" f\"val_loss={val_loss:.4f}, val_acc={val_acc*100:.2f}%\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQpLICef8KdR",
        "outputId": "5cba8c6c-63d7-4229-d627-61b17571ca4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No XML found for IMG_8226_jared.mov\n",
            "No XML found for IMG_8124_joe.mov\n",
            "No XML found for IMG_8028_joel.mov\n",
            "No XML found for IMG_8139_joe.mov\n",
            "No XML found for IMG_8123_scott.mov\n",
            "No XML found for IMG_8027_joel.mov\n",
            "No XML found for IMG_8243_jared.mov\n",
            "No XML found for IMG_8063_scott.mov\n",
            "No XML found for IMG_8030_patrick.mov\n",
            "No XML found for IMG_8255_zach.mov\n",
            "No XML found for IMG_8121_scott.mov\n",
            "No XML found for IMG_8138_joe.mov\n",
            "No XML found for IMG_8242_jared.mov\n",
            "No XML found for IMG_8241_jared.mov\n",
            "No XML found for IMG_8140_joe.mov\n",
            "No XML found for IMG_8122_scott.mov\n",
            "No XML found for IMG_8029_joel.mov\n",
            "No XML found for IMG_7999_joel.mov\n",
            "No XML found for IMG_8252_zach.mov\n",
            "No XML found for IMG_8923_souleymane.mov\n",
            "No XML found for IMG_8947_souleymane.mov\n",
            "No XML found for IMG_8257_zach.mov\n",
            "No XML found for IMG_9198_joel.mov\n",
            "No XML found for IMG_8256_zach.mov\n",
            "No XML found for IMG_8924_souleymane.mov\n",
            "No XML found for IMG_8946_souleymane.mov\n",
            "\n",
            " Found 21 videos with matching XMLs in /content/drive/MyDrive/Raw Videos\n",
            "\n",
            "Preloading videos and indexing frames...\n",
            "\n",
            "Loading video: IMG_7943_khem.mov\n",
            "Loaded 51 frames -> kept 51 after skipping\n",
            "\n",
            "Frame 51 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
            "Frame 52 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
            "Frame 53 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
            "   [1/21] Loaded IMG_7943_khem.mov (51 frames, 54 annotated)\n",
            "\n",
            "Loading video: IMG_7919_dusty.mov\n",
            "Loaded 50 frames -> kept 50 after skipping\n",
            "\n",
            "   [2/21] Loaded IMG_7919_dusty.mov (50 frames, 50 annotated)\n",
            "\n",
            "Loading video: dusty_1.mov\n",
            "Loaded 76 frames -> kept 76 after skipping\n",
            "\n",
            "   [3/21] Loaded dusty_1.mov (76 frames, 76 annotated)\n",
            "\n",
            "Loading video: IMG_7997_khem.mov\n",
            "Loaded 37 frames -> kept 37 after skipping\n",
            "\n",
            "   [4/21] Loaded IMG_7997_khem.mov (37 frames, 37 annotated)\n",
            "\n",
            "Loading video: IMG_8061_patrick.mov\n",
            "Loaded 54 frames -> kept 54 after skipping\n",
            "\n",
            "   [5/21] Loaded IMG_8061_patrick.mov (54 frames, 54 annotated)\n",
            "\n",
            "Loading video: IMG_7998_khem.mov\n",
            "Loaded 55 frames -> kept 55 after skipping\n",
            "\n",
            "   [6/21] Loaded IMG_7998_khem.mov (55 frames, 55 annotated)\n",
            "\n",
            "Loading video: IMG_8141_amarnath.mov\n",
            "Loaded 57 frames -> kept 57 after skipping\n",
            "\n",
            "   [7/21] Loaded IMG_8141_amarnath.mov (57 frames, 29 annotated)\n",
            "\n",
            "Loading video: IMG_8223_amarnath.mov\n",
            "Loaded 52 frames -> kept 52 after skipping\n",
            "\n",
            "   [8/21] Loaded IMG_8223_amarnath.mov (52 frames, 43 annotated)\n",
            "\n",
            "Loading video: IMG_7944_khem.mov\n",
            "Loaded 54 frames -> kept 54 after skipping\n",
            "\n",
            "   [9/21] Loaded IMG_7944_khem.mov (54 frames, 37 annotated)\n",
            "\n",
            "Loading video: IMG_7942_dusty.mov\n",
            "Loaded 57 frames -> kept 57 after skipping\n",
            "\n",
            "   [10/21] Loaded IMG_7942_dusty.mov (57 frames, 57 annotated)\n",
            "\n",
            "Loading video: IMG_7917_dusty.mov\n",
            "Loaded 60 frames -> kept 60 after skipping\n",
            "\n",
            "   [11/21] Loaded IMG_7917_dusty.mov (60 frames, 60 annotated)\n",
            "\n",
            "Loading video: IMG_8225_amarnath.mov\n",
            "Loaded 40 frames -> kept 40 after skipping\n",
            "\n",
            "   [12/21] Loaded IMG_8225_amarnath.mov (40 frames, 40 annotated)\n",
            "\n",
            "Loading video: IMG_8060_patrick.mov\n",
            "Loaded 58 frames -> kept 58 after skipping\n",
            "\n",
            "   [13/21] Loaded IMG_8060_patrick.mov (58 frames, 58 annotated)\n",
            "\n",
            "Loading video: IMG_8062_patrick.mov\n",
            "Loaded 53 frames -> kept 53 after skipping\n",
            "\n",
            "   [14/21] Loaded IMG_8062_patrick.mov (53 frames, 53 annotated)\n",
            "\n",
            "Loading video: IMG_8224_amarnath.mov\n",
            "Loaded 57 frames -> kept 57 after skipping\n",
            "\n",
            "   [15/21] Loaded IMG_8224_amarnath.mov (57 frames, 56 annotated)\n",
            "\n",
            "Loading video: IMG_7918_dusty.mov\n",
            "Loaded 76 frames -> kept 76 after skipping\n",
            "\n",
            "   [16/21] Loaded IMG_7918_dusty.mov (76 frames, 76 annotated)\n",
            "\n",
            "Loading video: IMG_9199_hugo.mov\n",
            "Loaded 81 frames -> kept 81 after skipping\n",
            "\n",
            "   [17/21] Loaded IMG_9199_hugo.mov (81 frames, 81 annotated)\n",
            "\n",
            "Loading video: IMG_9197_hugo.mov\n",
            "Loaded 70 frames -> kept 70 after skipping\n",
            "\n",
            "   [18/21] Loaded IMG_9197_hugo.mov (70 frames, 70 annotated)\n",
            "\n",
            "Loading video: IMG_9607_hugo.mov\n",
            "Loaded 43 frames -> kept 43 after skipping\n",
            "\n",
            "   [19/21] Loaded IMG_9607_hugo.mov (43 frames, 43 annotated)\n",
            "\n",
            "Loading video: IMG_9609_dusty.mov\n",
            "Loaded 102 frames -> kept 102 after skipping\n",
            "\n",
            "   [20/21] Loaded IMG_9609_dusty.mov (102 frames, 102 annotated)\n",
            "\n",
            "Loading video: IMG_9435_hugo.mov\n",
            "Loaded 66 frames -> kept 66 after skipping\n",
            "\n",
            "   [21/21] Loaded IMG_9435_hugo.mov (66 frames, 66 annotated)\n",
            "\n",
            "Finished indexing 1194 annotated frames from 21 videos.\n",
            "\n",
            "\n",
            "Dataset contains 1194 annotated frames across videos.\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 160M/160M [00:00<00:00, 215MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 4.2637\n",
            "Batch 5/239 | Loss: 0.6494\n",
            "Batch 10/239 | Loss: 0.8681\n",
            "Batch 15/239 | Loss: 0.4860\n",
            "Batch 20/239 | Loss: 0.6403\n",
            "Batch 25/239 | Loss: 0.4103\n",
            "Batch 30/239 | Loss: 0.5687\n",
            "Batch 35/239 | Loss: 0.5541\n",
            "Batch 40/239 | Loss: 0.2250\n",
            "Batch 45/239 | Loss: 0.3853\n",
            "Batch 50/239 | Loss: 0.2741\n",
            "Batch 55/239 | Loss: 0.4839\n",
            "Batch 60/239 | Loss: 0.3406\n",
            "Batch 65/239 | Loss: 0.8450\n",
            "Batch 70/239 | Loss: 0.1846\n",
            "Batch 75/239 | Loss: 0.5465\n",
            "Batch 80/239 | Loss: 0.6376\n",
            "Batch 85/239 | Loss: 0.4792\n",
            "Batch 90/239 | Loss: 0.4808\n",
            "Batch 95/239 | Loss: 0.3720\n",
            "Batch 100/239 | Loss: 0.5374\n",
            "Batch 105/239 | Loss: 0.5006\n",
            "Batch 110/239 | Loss: 0.1673\n",
            "Batch 115/239 | Loss: 0.5370\n",
            "Batch 120/239 | Loss: 0.4840\n",
            "Batch 125/239 | Loss: 0.5427\n",
            "Batch 130/239 | Loss: 0.4162\n",
            "Batch 135/239 | Loss: 0.5092\n",
            "Batch 140/239 | Loss: 0.3625\n",
            "Batch 145/239 | Loss: 0.2212\n",
            "Batch 150/239 | Loss: 0.4685\n",
            "Batch 155/239 | Loss: 0.3553\n",
            "Batch 160/239 | Loss: 0.5177\n",
            "Batch 165/239 | Loss: 0.5091\n",
            "Batch 170/239 | Loss: 0.4798\n",
            "Batch 175/239 | Loss: 0.3629\n",
            "Batch 180/239 | Loss: 0.3427\n",
            "Batch 185/239 | Loss: 0.4238\n",
            "Batch 190/239 | Loss: 0.3886\n",
            "Batch 195/239 | Loss: 0.3278\n",
            "Batch 200/239 | Loss: 0.5772\n",
            "Batch 205/239 | Loss: 0.5172\n",
            "Batch 210/239 | Loss: 0.6017\n",
            "Batch 215/239 | Loss: 0.4532\n",
            "Batch 220/239 | Loss: 0.4787\n",
            "Batch 225/239 | Loss: 0.8664\n",
            "Batch 230/239 | Loss: 0.5109\n",
            "Batch 235/239 | Loss: 0.5085\n",
            "Average Training LossL: 0.5098\n",
            "Validation Loss: 0.4220\n",
            "Frame-level moving/not-moving accuracy: 74.90% (threshold=0.5)\n",
            "Summary: train_loss=0.5098, val_loss=0.4220, val_acc=74.90%\n",
            "\n",
            "Epoch 2/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.3555\n",
            "Batch 5/239 | Loss: 0.1399\n",
            "Batch 10/239 | Loss: 0.3075\n",
            "Batch 15/239 | Loss: 0.4084\n",
            "Batch 20/239 | Loss: 0.2107\n",
            "Batch 25/239 | Loss: 0.3618\n",
            "Batch 30/239 | Loss: 0.5036\n",
            "Batch 35/239 | Loss: 0.3018\n",
            "Batch 40/239 | Loss: 0.4460\n",
            "Batch 45/239 | Loss: 0.2276\n",
            "Batch 50/239 | Loss: 0.2407\n",
            "Batch 55/239 | Loss: 0.4885\n",
            "Batch 60/239 | Loss: 0.4398\n",
            "Batch 65/239 | Loss: 0.2358\n",
            "Batch 70/239 | Loss: 0.4188\n",
            "Batch 75/239 | Loss: 0.3330\n",
            "Batch 80/239 | Loss: 0.4451\n",
            "Batch 85/239 | Loss: 0.4220\n",
            "Batch 90/239 | Loss: 0.1454\n",
            "Batch 95/239 | Loss: 0.1896\n",
            "Batch 100/239 | Loss: 0.3122\n",
            "Batch 105/239 | Loss: 0.3576\n",
            "Batch 110/239 | Loss: 0.5488\n",
            "Batch 115/239 | Loss: 0.0676\n",
            "Batch 120/239 | Loss: 0.6167\n",
            "Batch 125/239 | Loss: 0.3187\n",
            "Batch 130/239 | Loss: 0.2493\n",
            "Batch 135/239 | Loss: 0.6379\n",
            "Batch 140/239 | Loss: 0.4254\n",
            "Batch 145/239 | Loss: 0.1544\n",
            "Batch 150/239 | Loss: 0.3168\n",
            "Batch 155/239 | Loss: 0.3811\n",
            "Batch 160/239 | Loss: 0.2450\n",
            "Batch 165/239 | Loss: 0.3268\n",
            "Batch 170/239 | Loss: 0.3094\n",
            "Batch 175/239 | Loss: 0.2611\n",
            "Batch 180/239 | Loss: 0.3694\n",
            "Batch 185/239 | Loss: 0.3546\n",
            "Batch 190/239 | Loss: 0.2567\n",
            "Batch 195/239 | Loss: 0.3692\n",
            "Batch 200/239 | Loss: 0.4356\n",
            "Batch 205/239 | Loss: 0.3019\n",
            "Batch 210/239 | Loss: 0.2370\n",
            "Batch 215/239 | Loss: 0.5888\n",
            "Batch 220/239 | Loss: 0.4578\n",
            "Batch 225/239 | Loss: 0.4938\n",
            "Batch 230/239 | Loss: 0.2490\n",
            "Batch 235/239 | Loss: 0.5619\n",
            "Average Training LossL: 0.3539\n",
            "Validation Loss: 0.3863\n",
            "Frame-level moving/not-moving accuracy: 67.36% (threshold=0.5)\n",
            "Summary: train_loss=0.3539, val_loss=0.3863, val_acc=67.36%\n",
            "\n",
            "Epoch 3/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.2822\n",
            "Batch 5/239 | Loss: 0.1752\n",
            "Batch 10/239 | Loss: 0.4136\n",
            "Batch 15/239 | Loss: 0.5525\n",
            "Batch 20/239 | Loss: 0.3287\n",
            "Batch 25/239 | Loss: 0.2341\n",
            "Batch 30/239 | Loss: 0.3634\n",
            "Batch 35/239 | Loss: 0.2906\n",
            "Batch 40/239 | Loss: 0.3547\n",
            "Batch 45/239 | Loss: 0.4841\n",
            "Batch 50/239 | Loss: 0.3665\n",
            "Batch 55/239 | Loss: 0.3430\n",
            "Batch 60/239 | Loss: 0.3129\n",
            "Batch 65/239 | Loss: 0.1678\n",
            "Batch 70/239 | Loss: 0.4911\n",
            "Batch 75/239 | Loss: 0.2947\n",
            "Batch 80/239 | Loss: 0.1773\n",
            "Batch 85/239 | Loss: 0.3056\n",
            "Batch 90/239 | Loss: 0.4160\n",
            "Batch 95/239 | Loss: 0.2972\n",
            "Batch 100/239 | Loss: 0.2813\n",
            "Batch 105/239 | Loss: 0.4035\n",
            "Batch 110/239 | Loss: 0.5452\n",
            "Batch 115/239 | Loss: 0.2478\n",
            "Batch 120/239 | Loss: 0.5669\n",
            "Batch 125/239 | Loss: 0.2609\n",
            "Batch 130/239 | Loss: 0.4482\n",
            "Batch 135/239 | Loss: 0.2617\n",
            "Batch 140/239 | Loss: 0.1748\n",
            "Batch 145/239 | Loss: 0.1038\n",
            "Batch 150/239 | Loss: 0.0535\n",
            "Batch 155/239 | Loss: 0.7146\n",
            "Batch 160/239 | Loss: 0.0814\n",
            "Batch 165/239 | Loss: 0.0921\n",
            "Batch 170/239 | Loss: 0.2373\n",
            "Batch 175/239 | Loss: 0.4281\n",
            "Batch 180/239 | Loss: 0.2833\n",
            "Batch 185/239 | Loss: 0.4173\n",
            "Batch 190/239 | Loss: 0.2724\n",
            "Batch 195/239 | Loss: 0.0556\n",
            "Batch 200/239 | Loss: 0.2126\n",
            "Batch 205/239 | Loss: 0.1158\n",
            "Batch 210/239 | Loss: 0.1420\n",
            "Batch 215/239 | Loss: 0.6115\n",
            "Batch 220/239 | Loss: 0.2815\n",
            "Batch 225/239 | Loss: 0.4146\n",
            "Batch 230/239 | Loss: 0.2115\n",
            "Batch 235/239 | Loss: 0.3497\n",
            "Average Training LossL: 0.3065\n",
            "Validation Loss: 0.2955\n",
            "Frame-level moving/not-moving accuracy: 69.46% (threshold=0.5)\n",
            "Summary: train_loss=0.3065, val_loss=0.2955, val_acc=69.46%\n",
            "\n",
            "Epoch 4/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.1255\n",
            "Batch 5/239 | Loss: 0.2571\n",
            "Batch 10/239 | Loss: 0.3510\n",
            "Batch 15/239 | Loss: 0.4083\n",
            "Batch 20/239 | Loss: 0.1835\n",
            "Batch 25/239 | Loss: 0.1347\n",
            "Batch 30/239 | Loss: 0.1901\n",
            "Batch 35/239 | Loss: 0.1501\n",
            "Batch 40/239 | Loss: 0.1730\n",
            "Batch 45/239 | Loss: 0.4483\n",
            "Batch 50/239 | Loss: 0.1310\n",
            "Batch 55/239 | Loss: 0.0938\n",
            "Batch 60/239 | Loss: 0.1578\n",
            "Batch 65/239 | Loss: 0.2577\n",
            "Batch 70/239 | Loss: 0.4189\n",
            "Batch 75/239 | Loss: 0.5299\n",
            "Batch 80/239 | Loss: 0.0452\n",
            "Batch 85/239 | Loss: 0.3160\n",
            "Batch 90/239 | Loss: 0.2227\n",
            "Batch 95/239 | Loss: 0.2894\n",
            "Batch 100/239 | Loss: 0.3276\n",
            "Batch 105/239 | Loss: 0.5006\n",
            "Batch 110/239 | Loss: 0.2933\n",
            "Batch 115/239 | Loss: 0.1369\n",
            "Batch 120/239 | Loss: 0.4443\n",
            "Batch 125/239 | Loss: 0.0501\n",
            "Batch 130/239 | Loss: 0.1529\n",
            "Batch 135/239 | Loss: 0.2712\n",
            "Batch 140/239 | Loss: 0.3005\n",
            "Batch 145/239 | Loss: 0.2347\n",
            "Batch 150/239 | Loss: 0.2338\n",
            "Batch 155/239 | Loss: 0.4871\n",
            "Batch 160/239 | Loss: 0.3155\n",
            "Batch 165/239 | Loss: 0.2432\n",
            "Batch 170/239 | Loss: 0.3451\n",
            "Batch 175/239 | Loss: 0.3004\n",
            "Batch 180/239 | Loss: 0.2853\n",
            "Batch 185/239 | Loss: 0.5582\n",
            "Batch 190/239 | Loss: 0.1248\n",
            "Batch 195/239 | Loss: 0.5264\n",
            "Batch 200/239 | Loss: 0.4585\n",
            "Batch 205/239 | Loss: 0.3813\n",
            "Batch 210/239 | Loss: 0.2965\n",
            "Batch 215/239 | Loss: 0.2642\n",
            "Batch 220/239 | Loss: 0.3567\n",
            "Batch 225/239 | Loss: 0.2961\n",
            "Batch 230/239 | Loss: 0.1906\n",
            "Batch 235/239 | Loss: 0.2066\n",
            "Average Training LossL: 0.2821\n",
            "Validation Loss: 0.3025\n",
            "Frame-level moving/not-moving accuracy: 68.62% (threshold=0.5)\n",
            "Summary: train_loss=0.2821, val_loss=0.3025, val_acc=68.62%\n",
            "\n",
            "Epoch 5/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.2106\n",
            "Batch 5/239 | Loss: 0.3746\n",
            "Batch 10/239 | Loss: 0.3138\n",
            "Batch 15/239 | Loss: 0.3764\n",
            "Batch 20/239 | Loss: 0.2229\n",
            "Batch 25/239 | Loss: 0.3084\n",
            "Batch 30/239 | Loss: 0.1126\n",
            "Batch 35/239 | Loss: 0.1964\n",
            "Batch 40/239 | Loss: 0.0695\n",
            "Batch 45/239 | Loss: 0.1731\n",
            "Batch 50/239 | Loss: 0.1949\n",
            "Batch 55/239 | Loss: 0.1945\n",
            "Batch 60/239 | Loss: 0.0903\n",
            "Batch 65/239 | Loss: 0.1213\n",
            "Batch 70/239 | Loss: 0.0669\n",
            "Batch 75/239 | Loss: 0.2655\n",
            "Batch 80/239 | Loss: 0.2291\n",
            "Batch 85/239 | Loss: 0.1616\n",
            "Batch 90/239 | Loss: 0.4192\n",
            "Batch 95/239 | Loss: 0.0598\n",
            "Batch 100/239 | Loss: 0.3247\n",
            "Batch 105/239 | Loss: 0.2002\n",
            "Batch 110/239 | Loss: 0.3352\n",
            "Batch 115/239 | Loss: 0.1645\n",
            "Batch 120/239 | Loss: 0.2641\n",
            "Batch 125/239 | Loss: 0.1029\n",
            "Batch 130/239 | Loss: 0.2439\n",
            "Batch 135/239 | Loss: 0.1843\n",
            "Batch 140/239 | Loss: 0.2087\n",
            "Batch 145/239 | Loss: 0.4874\n",
            "Batch 150/239 | Loss: 0.2411\n",
            "Batch 155/239 | Loss: 0.1583\n",
            "Batch 160/239 | Loss: 0.1656\n",
            "Batch 165/239 | Loss: 0.3462\n",
            "Batch 170/239 | Loss: 0.0613\n",
            "Batch 175/239 | Loss: 0.2695\n",
            "Batch 180/239 | Loss: 0.3114\n",
            "Batch 185/239 | Loss: 0.0560\n",
            "Batch 190/239 | Loss: 0.4485\n",
            "Batch 195/239 | Loss: 0.4142\n",
            "Batch 200/239 | Loss: 0.0696\n",
            "Batch 205/239 | Loss: 0.3348\n",
            "Batch 210/239 | Loss: 0.2224\n",
            "Batch 215/239 | Loss: 0.1360\n",
            "Batch 220/239 | Loss: 0.1361\n",
            "Batch 225/239 | Loss: 0.2159\n",
            "Batch 230/239 | Loss: 0.2161\n",
            "Batch 235/239 | Loss: 0.3080\n",
            "Average Training LossL: 0.2480\n",
            "Validation Loss: 0.2594\n",
            "Frame-level moving/not-moving accuracy: 75.73% (threshold=0.5)\n",
            "Summary: train_loss=0.2480, val_loss=0.2594, val_acc=75.73%\n",
            "\n",
            "Model saved as 'fasterrcnn_moving_detector_2.pth'.\n"
          ]
        }
      ],
      "source": [
        "#train and save trained model\n",
        "if __name__ == \"__main__\":\n",
        "    video_folder = r\"/content/drive/MyDrive/Raw Videos\" #Change this when swapping devices to your local Raw Video folder\n",
        "    xml_folder   = r\"/content/drive/MyDrive/Annotations\" #\n",
        "\n",
        "    #Collate function for object detection\n",
        "    def collate_fn(batch):\n",
        "        batch = [b for b in batch if b is not None]\n",
        "        frames = [b[0] for b in batch]\n",
        "        targets = [b[1] for b in batch]\n",
        "        return frames, targets\n",
        "\n",
        "    #Load full dataset (videos + XMLs)\n",
        "    full_dataset = BaseballVideoDataset(video_folder, xml_folder, resize=(1280, 720), frame_skip=1, scale_boxes=True)\n",
        "\n",
        "    #print a quick summary\n",
        "    print(f\"\\nDataset contains {len(full_dataset)} annotated frames across videos.\")\n",
        "\n",
        "    #Split train/test\n",
        "    n = len(full_dataset)\n",
        "    split = int(0.8 * n)\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [split, n - split])\n",
        "\n",
        "    #Train the model\n",
        "    trained_model = train_detector(\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        num_classes=3,\n",
        "        epochs=5,\n",
        "        lr=1e-4,\n",
        "        batch_size=4)\n",
        "\n",
        "    #Save trained model\n",
        "    torch.save(trained_model.state_dict(), \"fasterrcnn_moving_detector_2.0.pth\")\n",
        "    print(\"\\nModel saved as 'fasterrcnn_moving_detector_2.pth'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cba8c6c-63d7-4229-d627-61b17571ca4f",
        "id": "hnUCFI2me3Qp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No XML found for IMG_8226_jared.mov\n",
            "No XML found for IMG_8124_joe.mov\n",
            "No XML found for IMG_8028_joel.mov\n",
            "No XML found for IMG_8139_joe.mov\n",
            "No XML found for IMG_8123_scott.mov\n",
            "No XML found for IMG_8027_joel.mov\n",
            "No XML found for IMG_8243_jared.mov\n",
            "No XML found for IMG_8063_scott.mov\n",
            "No XML found for IMG_8030_patrick.mov\n",
            "No XML found for IMG_8255_zach.mov\n",
            "No XML found for IMG_8121_scott.mov\n",
            "No XML found for IMG_8138_joe.mov\n",
            "No XML found for IMG_8242_jared.mov\n",
            "No XML found for IMG_8241_jared.mov\n",
            "No XML found for IMG_8140_joe.mov\n",
            "No XML found for IMG_8122_scott.mov\n",
            "No XML found for IMG_8029_joel.mov\n",
            "No XML found for IMG_7999_joel.mov\n",
            "No XML found for IMG_8252_zach.mov\n",
            "No XML found for IMG_8923_souleymane.mov\n",
            "No XML found for IMG_8947_souleymane.mov\n",
            "No XML found for IMG_8257_zach.mov\n",
            "No XML found for IMG_9198_joel.mov\n",
            "No XML found for IMG_8256_zach.mov\n",
            "No XML found for IMG_8924_souleymane.mov\n",
            "No XML found for IMG_8946_souleymane.mov\n",
            "\n",
            " Found 21 videos with matching XMLs in /content/drive/MyDrive/Raw Videos\n",
            "\n",
            "Preloading videos and indexing frames...\n",
            "\n",
            "Loading video: IMG_7943_khem.mov\n",
            "Loaded 51 frames -> kept 51 after skipping\n",
            "\n",
            "Frame 51 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
            "Frame 52 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
            "Frame 53 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
            "   [1/21] Loaded IMG_7943_khem.mov (51 frames, 54 annotated)\n",
            "\n",
            "Loading video: IMG_7919_dusty.mov\n",
            "Loaded 50 frames -> kept 50 after skipping\n",
            "\n",
            "   [2/21] Loaded IMG_7919_dusty.mov (50 frames, 50 annotated)\n",
            "\n",
            "Loading video: dusty_1.mov\n",
            "Loaded 76 frames -> kept 76 after skipping\n",
            "\n",
            "   [3/21] Loaded dusty_1.mov (76 frames, 76 annotated)\n",
            "\n",
            "Loading video: IMG_7997_khem.mov\n",
            "Loaded 37 frames -> kept 37 after skipping\n",
            "\n",
            "   [4/21] Loaded IMG_7997_khem.mov (37 frames, 37 annotated)\n",
            "\n",
            "Loading video: IMG_8061_patrick.mov\n",
            "Loaded 54 frames -> kept 54 after skipping\n",
            "\n",
            "   [5/21] Loaded IMG_8061_patrick.mov (54 frames, 54 annotated)\n",
            "\n",
            "Loading video: IMG_7998_khem.mov\n",
            "Loaded 55 frames -> kept 55 after skipping\n",
            "\n",
            "   [6/21] Loaded IMG_7998_khem.mov (55 frames, 55 annotated)\n",
            "\n",
            "Loading video: IMG_8141_amarnath.mov\n",
            "Loaded 57 frames -> kept 57 after skipping\n",
            "\n",
            "   [7/21] Loaded IMG_8141_amarnath.mov (57 frames, 29 annotated)\n",
            "\n",
            "Loading video: IMG_8223_amarnath.mov\n",
            "Loaded 52 frames -> kept 52 after skipping\n",
            "\n",
            "   [8/21] Loaded IMG_8223_amarnath.mov (52 frames, 43 annotated)\n",
            "\n",
            "Loading video: IMG_7944_khem.mov\n",
            "Loaded 54 frames -> kept 54 after skipping\n",
            "\n",
            "   [9/21] Loaded IMG_7944_khem.mov (54 frames, 37 annotated)\n",
            "\n",
            "Loading video: IMG_7942_dusty.mov\n",
            "Loaded 57 frames -> kept 57 after skipping\n",
            "\n",
            "   [10/21] Loaded IMG_7942_dusty.mov (57 frames, 57 annotated)\n",
            "\n",
            "Loading video: IMG_7917_dusty.mov\n",
            "Loaded 60 frames -> kept 60 after skipping\n",
            "\n",
            "   [11/21] Loaded IMG_7917_dusty.mov (60 frames, 60 annotated)\n",
            "\n",
            "Loading video: IMG_8225_amarnath.mov\n",
            "Loaded 40 frames -> kept 40 after skipping\n",
            "\n",
            "   [12/21] Loaded IMG_8225_amarnath.mov (40 frames, 40 annotated)\n",
            "\n",
            "Loading video: IMG_8060_patrick.mov\n",
            "Loaded 58 frames -> kept 58 after skipping\n",
            "\n",
            "   [13/21] Loaded IMG_8060_patrick.mov (58 frames, 58 annotated)\n",
            "\n",
            "Loading video: IMG_8062_patrick.mov\n",
            "Loaded 53 frames -> kept 53 after skipping\n",
            "\n",
            "   [14/21] Loaded IMG_8062_patrick.mov (53 frames, 53 annotated)\n",
            "\n",
            "Loading video: IMG_8224_amarnath.mov\n",
            "Loaded 57 frames -> kept 57 after skipping\n",
            "\n",
            "   [15/21] Loaded IMG_8224_amarnath.mov (57 frames, 56 annotated)\n",
            "\n",
            "Loading video: IMG_7918_dusty.mov\n",
            "Loaded 76 frames -> kept 76 after skipping\n",
            "\n",
            "   [16/21] Loaded IMG_7918_dusty.mov (76 frames, 76 annotated)\n",
            "\n",
            "Loading video: IMG_9199_hugo.mov\n",
            "Loaded 81 frames -> kept 81 after skipping\n",
            "\n",
            "   [17/21] Loaded IMG_9199_hugo.mov (81 frames, 81 annotated)\n",
            "\n",
            "Loading video: IMG_9197_hugo.mov\n",
            "Loaded 70 frames -> kept 70 after skipping\n",
            "\n",
            "   [18/21] Loaded IMG_9197_hugo.mov (70 frames, 70 annotated)\n",
            "\n",
            "Loading video: IMG_9607_hugo.mov\n",
            "Loaded 43 frames -> kept 43 after skipping\n",
            "\n",
            "   [19/21] Loaded IMG_9607_hugo.mov (43 frames, 43 annotated)\n",
            "\n",
            "Loading video: IMG_9609_dusty.mov\n",
            "Loaded 102 frames -> kept 102 after skipping\n",
            "\n",
            "   [20/21] Loaded IMG_9609_dusty.mov (102 frames, 102 annotated)\n",
            "\n",
            "Loading video: IMG_9435_hugo.mov\n",
            "Loaded 66 frames -> kept 66 after skipping\n",
            "\n",
            "   [21/21] Loaded IMG_9435_hugo.mov (66 frames, 66 annotated)\n",
            "\n",
            "Finished indexing 1194 annotated frames from 21 videos.\n",
            "\n",
            "\n",
            "Dataset contains 1194 annotated frames across videos.\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 160M/160M [00:00<00:00, 215MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 4.2637\n",
            "Batch 5/239 | Loss: 0.6494\n",
            "Batch 10/239 | Loss: 0.8681\n",
            "Batch 15/239 | Loss: 0.4860\n",
            "Batch 20/239 | Loss: 0.6403\n",
            "Batch 25/239 | Loss: 0.4103\n",
            "Batch 30/239 | Loss: 0.5687\n",
            "Batch 35/239 | Loss: 0.5541\n",
            "Batch 40/239 | Loss: 0.2250\n",
            "Batch 45/239 | Loss: 0.3853\n",
            "Batch 50/239 | Loss: 0.2741\n",
            "Batch 55/239 | Loss: 0.4839\n",
            "Batch 60/239 | Loss: 0.3406\n",
            "Batch 65/239 | Loss: 0.8450\n",
            "Batch 70/239 | Loss: 0.1846\n",
            "Batch 75/239 | Loss: 0.5465\n",
            "Batch 80/239 | Loss: 0.6376\n",
            "Batch 85/239 | Loss: 0.4792\n",
            "Batch 90/239 | Loss: 0.4808\n",
            "Batch 95/239 | Loss: 0.3720\n",
            "Batch 100/239 | Loss: 0.5374\n",
            "Batch 105/239 | Loss: 0.5006\n",
            "Batch 110/239 | Loss: 0.1673\n",
            "Batch 115/239 | Loss: 0.5370\n",
            "Batch 120/239 | Loss: 0.4840\n",
            "Batch 125/239 | Loss: 0.5427\n",
            "Batch 130/239 | Loss: 0.4162\n",
            "Batch 135/239 | Loss: 0.5092\n",
            "Batch 140/239 | Loss: 0.3625\n",
            "Batch 145/239 | Loss: 0.2212\n",
            "Batch 150/239 | Loss: 0.4685\n",
            "Batch 155/239 | Loss: 0.3553\n",
            "Batch 160/239 | Loss: 0.5177\n",
            "Batch 165/239 | Loss: 0.5091\n",
            "Batch 170/239 | Loss: 0.4798\n",
            "Batch 175/239 | Loss: 0.3629\n",
            "Batch 180/239 | Loss: 0.3427\n",
            "Batch 185/239 | Loss: 0.4238\n",
            "Batch 190/239 | Loss: 0.3886\n",
            "Batch 195/239 | Loss: 0.3278\n",
            "Batch 200/239 | Loss: 0.5772\n",
            "Batch 205/239 | Loss: 0.5172\n",
            "Batch 210/239 | Loss: 0.6017\n",
            "Batch 215/239 | Loss: 0.4532\n",
            "Batch 220/239 | Loss: 0.4787\n",
            "Batch 225/239 | Loss: 0.8664\n",
            "Batch 230/239 | Loss: 0.5109\n",
            "Batch 235/239 | Loss: 0.5085\n",
            "Average Training LossL: 0.5098\n",
            "Validation Loss: 0.4220\n",
            "Frame-level moving/not-moving accuracy: 74.90% (threshold=0.5)\n",
            "Summary: train_loss=0.5098, val_loss=0.4220, val_acc=74.90%\n",
            "\n",
            "Epoch 2/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.3555\n",
            "Batch 5/239 | Loss: 0.1399\n",
            "Batch 10/239 | Loss: 0.3075\n",
            "Batch 15/239 | Loss: 0.4084\n",
            "Batch 20/239 | Loss: 0.2107\n",
            "Batch 25/239 | Loss: 0.3618\n",
            "Batch 30/239 | Loss: 0.5036\n",
            "Batch 35/239 | Loss: 0.3018\n",
            "Batch 40/239 | Loss: 0.4460\n",
            "Batch 45/239 | Loss: 0.2276\n",
            "Batch 50/239 | Loss: 0.2407\n",
            "Batch 55/239 | Loss: 0.4885\n",
            "Batch 60/239 | Loss: 0.4398\n",
            "Batch 65/239 | Loss: 0.2358\n",
            "Batch 70/239 | Loss: 0.4188\n",
            "Batch 75/239 | Loss: 0.3330\n",
            "Batch 80/239 | Loss: 0.4451\n",
            "Batch 85/239 | Loss: 0.4220\n",
            "Batch 90/239 | Loss: 0.1454\n",
            "Batch 95/239 | Loss: 0.1896\n",
            "Batch 100/239 | Loss: 0.3122\n",
            "Batch 105/239 | Loss: 0.3576\n",
            "Batch 110/239 | Loss: 0.5488\n",
            "Batch 115/239 | Loss: 0.0676\n",
            "Batch 120/239 | Loss: 0.6167\n",
            "Batch 125/239 | Loss: 0.3187\n",
            "Batch 130/239 | Loss: 0.2493\n",
            "Batch 135/239 | Loss: 0.6379\n",
            "Batch 140/239 | Loss: 0.4254\n",
            "Batch 145/239 | Loss: 0.1544\n",
            "Batch 150/239 | Loss: 0.3168\n",
            "Batch 155/239 | Loss: 0.3811\n",
            "Batch 160/239 | Loss: 0.2450\n",
            "Batch 165/239 | Loss: 0.3268\n",
            "Batch 170/239 | Loss: 0.3094\n",
            "Batch 175/239 | Loss: 0.2611\n",
            "Batch 180/239 | Loss: 0.3694\n",
            "Batch 185/239 | Loss: 0.3546\n",
            "Batch 190/239 | Loss: 0.2567\n",
            "Batch 195/239 | Loss: 0.3692\n",
            "Batch 200/239 | Loss: 0.4356\n",
            "Batch 205/239 | Loss: 0.3019\n",
            "Batch 210/239 | Loss: 0.2370\n",
            "Batch 215/239 | Loss: 0.5888\n",
            "Batch 220/239 | Loss: 0.4578\n",
            "Batch 225/239 | Loss: 0.4938\n",
            "Batch 230/239 | Loss: 0.2490\n",
            "Batch 235/239 | Loss: 0.5619\n",
            "Average Training LossL: 0.3539\n",
            "Validation Loss: 0.3863\n",
            "Frame-level moving/not-moving accuracy: 67.36% (threshold=0.5)\n",
            "Summary: train_loss=0.3539, val_loss=0.3863, val_acc=67.36%\n",
            "\n",
            "Epoch 3/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.2822\n",
            "Batch 5/239 | Loss: 0.1752\n",
            "Batch 10/239 | Loss: 0.4136\n",
            "Batch 15/239 | Loss: 0.5525\n",
            "Batch 20/239 | Loss: 0.3287\n",
            "Batch 25/239 | Loss: 0.2341\n",
            "Batch 30/239 | Loss: 0.3634\n",
            "Batch 35/239 | Loss: 0.2906\n",
            "Batch 40/239 | Loss: 0.3547\n",
            "Batch 45/239 | Loss: 0.4841\n",
            "Batch 50/239 | Loss: 0.3665\n",
            "Batch 55/239 | Loss: 0.3430\n",
            "Batch 60/239 | Loss: 0.3129\n",
            "Batch 65/239 | Loss: 0.1678\n",
            "Batch 70/239 | Loss: 0.4911\n",
            "Batch 75/239 | Loss: 0.2947\n",
            "Batch 80/239 | Loss: 0.1773\n",
            "Batch 85/239 | Loss: 0.3056\n",
            "Batch 90/239 | Loss: 0.4160\n",
            "Batch 95/239 | Loss: 0.2972\n",
            "Batch 100/239 | Loss: 0.2813\n",
            "Batch 105/239 | Loss: 0.4035\n",
            "Batch 110/239 | Loss: 0.5452\n",
            "Batch 115/239 | Loss: 0.2478\n",
            "Batch 120/239 | Loss: 0.5669\n",
            "Batch 125/239 | Loss: 0.2609\n",
            "Batch 130/239 | Loss: 0.4482\n",
            "Batch 135/239 | Loss: 0.2617\n",
            "Batch 140/239 | Loss: 0.1748\n",
            "Batch 145/239 | Loss: 0.1038\n",
            "Batch 150/239 | Loss: 0.0535\n",
            "Batch 155/239 | Loss: 0.7146\n",
            "Batch 160/239 | Loss: 0.0814\n",
            "Batch 165/239 | Loss: 0.0921\n",
            "Batch 170/239 | Loss: 0.2373\n",
            "Batch 175/239 | Loss: 0.4281\n",
            "Batch 180/239 | Loss: 0.2833\n",
            "Batch 185/239 | Loss: 0.4173\n",
            "Batch 190/239 | Loss: 0.2724\n",
            "Batch 195/239 | Loss: 0.0556\n",
            "Batch 200/239 | Loss: 0.2126\n",
            "Batch 205/239 | Loss: 0.1158\n",
            "Batch 210/239 | Loss: 0.1420\n",
            "Batch 215/239 | Loss: 0.6115\n",
            "Batch 220/239 | Loss: 0.2815\n",
            "Batch 225/239 | Loss: 0.4146\n",
            "Batch 230/239 | Loss: 0.2115\n",
            "Batch 235/239 | Loss: 0.3497\n",
            "Average Training LossL: 0.3065\n",
            "Validation Loss: 0.2955\n",
            "Frame-level moving/not-moving accuracy: 69.46% (threshold=0.5)\n",
            "Summary: train_loss=0.3065, val_loss=0.2955, val_acc=69.46%\n",
            "\n",
            "Epoch 4/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.1255\n",
            "Batch 5/239 | Loss: 0.2571\n",
            "Batch 10/239 | Loss: 0.3510\n",
            "Batch 15/239 | Loss: 0.4083\n",
            "Batch 20/239 | Loss: 0.1835\n",
            "Batch 25/239 | Loss: 0.1347\n",
            "Batch 30/239 | Loss: 0.1901\n",
            "Batch 35/239 | Loss: 0.1501\n",
            "Batch 40/239 | Loss: 0.1730\n",
            "Batch 45/239 | Loss: 0.4483\n",
            "Batch 50/239 | Loss: 0.1310\n",
            "Batch 55/239 | Loss: 0.0938\n",
            "Batch 60/239 | Loss: 0.1578\n",
            "Batch 65/239 | Loss: 0.2577\n",
            "Batch 70/239 | Loss: 0.4189\n",
            "Batch 75/239 | Loss: 0.5299\n",
            "Batch 80/239 | Loss: 0.0452\n",
            "Batch 85/239 | Loss: 0.3160\n",
            "Batch 90/239 | Loss: 0.2227\n",
            "Batch 95/239 | Loss: 0.2894\n",
            "Batch 100/239 | Loss: 0.3276\n",
            "Batch 105/239 | Loss: 0.5006\n",
            "Batch 110/239 | Loss: 0.2933\n",
            "Batch 115/239 | Loss: 0.1369\n",
            "Batch 120/239 | Loss: 0.4443\n",
            "Batch 125/239 | Loss: 0.0501\n",
            "Batch 130/239 | Loss: 0.1529\n",
            "Batch 135/239 | Loss: 0.2712\n",
            "Batch 140/239 | Loss: 0.3005\n",
            "Batch 145/239 | Loss: 0.2347\n",
            "Batch 150/239 | Loss: 0.2338\n",
            "Batch 155/239 | Loss: 0.4871\n",
            "Batch 160/239 | Loss: 0.3155\n",
            "Batch 165/239 | Loss: 0.2432\n",
            "Batch 170/239 | Loss: 0.3451\n",
            "Batch 175/239 | Loss: 0.3004\n",
            "Batch 180/239 | Loss: 0.2853\n",
            "Batch 185/239 | Loss: 0.5582\n",
            "Batch 190/239 | Loss: 0.1248\n",
            "Batch 195/239 | Loss: 0.5264\n",
            "Batch 200/239 | Loss: 0.4585\n",
            "Batch 205/239 | Loss: 0.3813\n",
            "Batch 210/239 | Loss: 0.2965\n",
            "Batch 215/239 | Loss: 0.2642\n",
            "Batch 220/239 | Loss: 0.3567\n",
            "Batch 225/239 | Loss: 0.2961\n",
            "Batch 230/239 | Loss: 0.1906\n",
            "Batch 235/239 | Loss: 0.2066\n",
            "Average Training LossL: 0.2821\n",
            "Validation Loss: 0.3025\n",
            "Frame-level moving/not-moving accuracy: 68.62% (threshold=0.5)\n",
            "Summary: train_loss=0.2821, val_loss=0.3025, val_acc=68.62%\n",
            "\n",
            "Epoch 5/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.2106\n",
            "Batch 5/239 | Loss: 0.3746\n",
            "Batch 10/239 | Loss: 0.3138\n",
            "Batch 15/239 | Loss: 0.3764\n",
            "Batch 20/239 | Loss: 0.2229\n",
            "Batch 25/239 | Loss: 0.3084\n",
            "Batch 30/239 | Loss: 0.1126\n",
            "Batch 35/239 | Loss: 0.1964\n",
            "Batch 40/239 | Loss: 0.0695\n",
            "Batch 45/239 | Loss: 0.1731\n",
            "Batch 50/239 | Loss: 0.1949\n",
            "Batch 55/239 | Loss: 0.1945\n",
            "Batch 60/239 | Loss: 0.0903\n",
            "Batch 65/239 | Loss: 0.1213\n",
            "Batch 70/239 | Loss: 0.0669\n",
            "Batch 75/239 | Loss: 0.2655\n",
            "Batch 80/239 | Loss: 0.2291\n",
            "Batch 85/239 | Loss: 0.1616\n",
            "Batch 90/239 | Loss: 0.4192\n",
            "Batch 95/239 | Loss: 0.0598\n",
            "Batch 100/239 | Loss: 0.3247\n",
            "Batch 105/239 | Loss: 0.2002\n",
            "Batch 110/239 | Loss: 0.3352\n",
            "Batch 115/239 | Loss: 0.1645\n",
            "Batch 120/239 | Loss: 0.2641\n",
            "Batch 125/239 | Loss: 0.1029\n",
            "Batch 130/239 | Loss: 0.2439\n",
            "Batch 135/239 | Loss: 0.1843\n",
            "Batch 140/239 | Loss: 0.2087\n",
            "Batch 145/239 | Loss: 0.4874\n",
            "Batch 150/239 | Loss: 0.2411\n",
            "Batch 155/239 | Loss: 0.1583\n",
            "Batch 160/239 | Loss: 0.1656\n",
            "Batch 165/239 | Loss: 0.3462\n",
            "Batch 170/239 | Loss: 0.0613\n",
            "Batch 175/239 | Loss: 0.2695\n",
            "Batch 180/239 | Loss: 0.3114\n",
            "Batch 185/239 | Loss: 0.0560\n",
            "Batch 190/239 | Loss: 0.4485\n",
            "Batch 195/239 | Loss: 0.4142\n",
            "Batch 200/239 | Loss: 0.0696\n",
            "Batch 205/239 | Loss: 0.3348\n",
            "Batch 210/239 | Loss: 0.2224\n",
            "Batch 215/239 | Loss: 0.1360\n",
            "Batch 220/239 | Loss: 0.1361\n",
            "Batch 225/239 | Loss: 0.2159\n",
            "Batch 230/239 | Loss: 0.2161\n",
            "Batch 235/239 | Loss: 0.3080\n",
            "Average Training LossL: 0.2480\n",
            "Validation Loss: 0.2594\n",
            "Frame-level moving/not-moving accuracy: 75.73% (threshold=0.5)\n",
            "Summary: train_loss=0.2480, val_loss=0.2594, val_acc=75.73%\n",
            "\n",
            "Model saved as 'fasterrcnn_moving_detector_2.pth'.\n"
          ]
        }
      ],
      "source": [
        "#train and save trained model\n",
        "if __name__ == \"__main__\":\n",
        "    video_folder = r\"/content/drive/MyDrive/Raw Videos\" #Change this when swapping devices to your local Raw Video folder\n",
        "    xml_folder   = r\"/content/drive/MyDrive/Annotations\"\n",
        "\n",
        "    #Collate function for object detection\n",
        "    def collate_fn(batch):\n",
        "        batch = [b for b in batch if b is not None]\n",
        "        frames = [b[0] for b in batch]\n",
        "        targets = [b[1] for b in batch]\n",
        "        return frames, targets\n",
        "\n",
        "    #Load full dataset (videos + XMLs)\n",
        "    full_dataset = BaseballVideoDataset(video_folder, xml_folder, resize=(1280, 720), frame_skip=1, scale_boxes=True)\n",
        "\n",
        "    #print a quick summary\n",
        "    print(f\"\\nDataset contains {len(full_dataset)} annotated frames across videos.\")\n",
        "\n",
        "    #Split train/test\n",
        "    n = len(full_dataset)\n",
        "    split = int(0.8 * n)\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [split, n - split])\n",
        "\n",
        "    #Train the model\n",
        "    trained_model = train_detector(\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        num_classes=3,\n",
        "        epochs=5,\n",
        "        lr=1e-4,\n",
        "        batch_size=4)\n",
        "\n",
        "    #Save trained model\n",
        "    torch.save(trained_model.state_dict(), \"fasterrcnn_moving_detector_2.0.pth\")\n",
        "    print(\"\\nModel saved as 'fasterrcnn_moving_detector_2.pth'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cba8c6c-63d7-4229-d627-61b17571ca4f",
        "id": "MLfrs90Ge5w5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No XML found for IMG_8226_jared.mov\n",
            "No XML found for IMG_8124_joe.mov\n",
            "No XML found for IMG_8028_joel.mov\n",
            "No XML found for IMG_8139_joe.mov\n",
            "No XML found for IMG_8123_scott.mov\n",
            "No XML found for IMG_8027_joel.mov\n",
            "No XML found for IMG_8243_jared.mov\n",
            "No XML found for IMG_8063_scott.mov\n",
            "No XML found for IMG_8030_patrick.mov\n",
            "No XML found for IMG_8255_zach.mov\n",
            "No XML found for IMG_8121_scott.mov\n",
            "No XML found for IMG_8138_joe.mov\n",
            "No XML found for IMG_8242_jared.mov\n",
            "No XML found for IMG_8241_jared.mov\n",
            "No XML found for IMG_8140_joe.mov\n",
            "No XML found for IMG_8122_scott.mov\n",
            "No XML found for IMG_8029_joel.mov\n",
            "No XML found for IMG_7999_joel.mov\n",
            "No XML found for IMG_8252_zach.mov\n",
            "No XML found for IMG_8923_souleymane.mov\n",
            "No XML found for IMG_8947_souleymane.mov\n",
            "No XML found for IMG_8257_zach.mov\n",
            "No XML found for IMG_9198_joel.mov\n",
            "No XML found for IMG_8256_zach.mov\n",
            "No XML found for IMG_8924_souleymane.mov\n",
            "No XML found for IMG_8946_souleymane.mov\n",
            "\n",
            " Found 21 videos with matching XMLs in /content/drive/MyDrive/Raw Videos\n",
            "\n",
            "Preloading videos and indexing frames...\n",
            "\n",
            "Loading video: IMG_7943_khem.mov\n",
            "Loaded 51 frames -> kept 51 after skipping\n",
            "\n",
            "Frame 51 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
            "Frame 52 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
            "Frame 53 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
            "   [1/21] Loaded IMG_7943_khem.mov (51 frames, 54 annotated)\n",
            "\n",
            "Loading video: IMG_7919_dusty.mov\n",
            "Loaded 50 frames -> kept 50 after skipping\n",
            "\n",
            "   [2/21] Loaded IMG_7919_dusty.mov (50 frames, 50 annotated)\n",
            "\n",
            "Loading video: dusty_1.mov\n",
            "Loaded 76 frames -> kept 76 after skipping\n",
            "\n",
            "   [3/21] Loaded dusty_1.mov (76 frames, 76 annotated)\n",
            "\n",
            "Loading video: IMG_7997_khem.mov\n",
            "Loaded 37 frames -> kept 37 after skipping\n",
            "\n",
            "   [4/21] Loaded IMG_7997_khem.mov (37 frames, 37 annotated)\n",
            "\n",
            "Loading video: IMG_8061_patrick.mov\n",
            "Loaded 54 frames -> kept 54 after skipping\n",
            "\n",
            "   [5/21] Loaded IMG_8061_patrick.mov (54 frames, 54 annotated)\n",
            "\n",
            "Loading video: IMG_7998_khem.mov\n",
            "Loaded 55 frames -> kept 55 after skipping\n",
            "\n",
            "   [6/21] Loaded IMG_7998_khem.mov (55 frames, 55 annotated)\n",
            "\n",
            "Loading video: IMG_8141_amarnath.mov\n",
            "Loaded 57 frames -> kept 57 after skipping\n",
            "\n",
            "   [7/21] Loaded IMG_8141_amarnath.mov (57 frames, 29 annotated)\n",
            "\n",
            "Loading video: IMG_8223_amarnath.mov\n",
            "Loaded 52 frames -> kept 52 after skipping\n",
            "\n",
            "   [8/21] Loaded IMG_8223_amarnath.mov (52 frames, 43 annotated)\n",
            "\n",
            "Loading video: IMG_7944_khem.mov\n",
            "Loaded 54 frames -> kept 54 after skipping\n",
            "\n",
            "   [9/21] Loaded IMG_7944_khem.mov (54 frames, 37 annotated)\n",
            "\n",
            "Loading video: IMG_7942_dusty.mov\n",
            "Loaded 57 frames -> kept 57 after skipping\n",
            "\n",
            "   [10/21] Loaded IMG_7942_dusty.mov (57 frames, 57 annotated)\n",
            "\n",
            "Loading video: IMG_7917_dusty.mov\n",
            "Loaded 60 frames -> kept 60 after skipping\n",
            "\n",
            "   [11/21] Loaded IMG_7917_dusty.mov (60 frames, 60 annotated)\n",
            "\n",
            "Loading video: IMG_8225_amarnath.mov\n",
            "Loaded 40 frames -> kept 40 after skipping\n",
            "\n",
            "   [12/21] Loaded IMG_8225_amarnath.mov (40 frames, 40 annotated)\n",
            "\n",
            "Loading video: IMG_8060_patrick.mov\n",
            "Loaded 58 frames -> kept 58 after skipping\n",
            "\n",
            "   [13/21] Loaded IMG_8060_patrick.mov (58 frames, 58 annotated)\n",
            "\n",
            "Loading video: IMG_8062_patrick.mov\n",
            "Loaded 53 frames -> kept 53 after skipping\n",
            "\n",
            "   [14/21] Loaded IMG_8062_patrick.mov (53 frames, 53 annotated)\n",
            "\n",
            "Loading video: IMG_8224_amarnath.mov\n",
            "Loaded 57 frames -> kept 57 after skipping\n",
            "\n",
            "   [15/21] Loaded IMG_8224_amarnath.mov (57 frames, 56 annotated)\n",
            "\n",
            "Loading video: IMG_7918_dusty.mov\n",
            "Loaded 76 frames -> kept 76 after skipping\n",
            "\n",
            "   [16/21] Loaded IMG_7918_dusty.mov (76 frames, 76 annotated)\n",
            "\n",
            "Loading video: IMG_9199_hugo.mov\n",
            "Loaded 81 frames -> kept 81 after skipping\n",
            "\n",
            "   [17/21] Loaded IMG_9199_hugo.mov (81 frames, 81 annotated)\n",
            "\n",
            "Loading video: IMG_9197_hugo.mov\n",
            "Loaded 70 frames -> kept 70 after skipping\n",
            "\n",
            "   [18/21] Loaded IMG_9197_hugo.mov (70 frames, 70 annotated)\n",
            "\n",
            "Loading video: IMG_9607_hugo.mov\n",
            "Loaded 43 frames -> kept 43 after skipping\n",
            "\n",
            "   [19/21] Loaded IMG_9607_hugo.mov (43 frames, 43 annotated)\n",
            "\n",
            "Loading video: IMG_9609_dusty.mov\n",
            "Loaded 102 frames -> kept 102 after skipping\n",
            "\n",
            "   [20/21] Loaded IMG_9609_dusty.mov (102 frames, 102 annotated)\n",
            "\n",
            "Loading video: IMG_9435_hugo.mov\n",
            "Loaded 66 frames -> kept 66 after skipping\n",
            "\n",
            "   [21/21] Loaded IMG_9435_hugo.mov (66 frames, 66 annotated)\n",
            "\n",
            "Finished indexing 1194 annotated frames from 21 videos.\n",
            "\n",
            "\n",
            "Dataset contains 1194 annotated frames across videos.\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 160M/160M [00:00<00:00, 215MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 4.2637\n",
            "Batch 5/239 | Loss: 0.6494\n",
            "Batch 10/239 | Loss: 0.8681\n",
            "Batch 15/239 | Loss: 0.4860\n",
            "Batch 20/239 | Loss: 0.6403\n",
            "Batch 25/239 | Loss: 0.4103\n",
            "Batch 30/239 | Loss: 0.5687\n",
            "Batch 35/239 | Loss: 0.5541\n",
            "Batch 40/239 | Loss: 0.2250\n",
            "Batch 45/239 | Loss: 0.3853\n",
            "Batch 50/239 | Loss: 0.2741\n",
            "Batch 55/239 | Loss: 0.4839\n",
            "Batch 60/239 | Loss: 0.3406\n",
            "Batch 65/239 | Loss: 0.8450\n",
            "Batch 70/239 | Loss: 0.1846\n",
            "Batch 75/239 | Loss: 0.5465\n",
            "Batch 80/239 | Loss: 0.6376\n",
            "Batch 85/239 | Loss: 0.4792\n",
            "Batch 90/239 | Loss: 0.4808\n",
            "Batch 95/239 | Loss: 0.3720\n",
            "Batch 100/239 | Loss: 0.5374\n",
            "Batch 105/239 | Loss: 0.5006\n",
            "Batch 110/239 | Loss: 0.1673\n",
            "Batch 115/239 | Loss: 0.5370\n",
            "Batch 120/239 | Loss: 0.4840\n",
            "Batch 125/239 | Loss: 0.5427\n",
            "Batch 130/239 | Loss: 0.4162\n",
            "Batch 135/239 | Loss: 0.5092\n",
            "Batch 140/239 | Loss: 0.3625\n",
            "Batch 145/239 | Loss: 0.2212\n",
            "Batch 150/239 | Loss: 0.4685\n",
            "Batch 155/239 | Loss: 0.3553\n",
            "Batch 160/239 | Loss: 0.5177\n",
            "Batch 165/239 | Loss: 0.5091\n",
            "Batch 170/239 | Loss: 0.4798\n",
            "Batch 175/239 | Loss: 0.3629\n",
            "Batch 180/239 | Loss: 0.3427\n",
            "Batch 185/239 | Loss: 0.4238\n",
            "Batch 190/239 | Loss: 0.3886\n",
            "Batch 195/239 | Loss: 0.3278\n",
            "Batch 200/239 | Loss: 0.5772\n",
            "Batch 205/239 | Loss: 0.5172\n",
            "Batch 210/239 | Loss: 0.6017\n",
            "Batch 215/239 | Loss: 0.4532\n",
            "Batch 220/239 | Loss: 0.4787\n",
            "Batch 225/239 | Loss: 0.8664\n",
            "Batch 230/239 | Loss: 0.5109\n",
            "Batch 235/239 | Loss: 0.5085\n",
            "Average Training LossL: 0.5098\n",
            "Validation Loss: 0.4220\n",
            "Frame-level moving/not-moving accuracy: 74.90% (threshold=0.5)\n",
            "Summary: train_loss=0.5098, val_loss=0.4220, val_acc=74.90%\n",
            "\n",
            "Epoch 2/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.3555\n",
            "Batch 5/239 | Loss: 0.1399\n",
            "Batch 10/239 | Loss: 0.3075\n",
            "Batch 15/239 | Loss: 0.4084\n",
            "Batch 20/239 | Loss: 0.2107\n",
            "Batch 25/239 | Loss: 0.3618\n",
            "Batch 30/239 | Loss: 0.5036\n",
            "Batch 35/239 | Loss: 0.3018\n",
            "Batch 40/239 | Loss: 0.4460\n",
            "Batch 45/239 | Loss: 0.2276\n",
            "Batch 50/239 | Loss: 0.2407\n",
            "Batch 55/239 | Loss: 0.4885\n",
            "Batch 60/239 | Loss: 0.4398\n",
            "Batch 65/239 | Loss: 0.2358\n",
            "Batch 70/239 | Loss: 0.4188\n",
            "Batch 75/239 | Loss: 0.3330\n",
            "Batch 80/239 | Loss: 0.4451\n",
            "Batch 85/239 | Loss: 0.4220\n",
            "Batch 90/239 | Loss: 0.1454\n",
            "Batch 95/239 | Loss: 0.1896\n",
            "Batch 100/239 | Loss: 0.3122\n",
            "Batch 105/239 | Loss: 0.3576\n",
            "Batch 110/239 | Loss: 0.5488\n",
            "Batch 115/239 | Loss: 0.0676\n",
            "Batch 120/239 | Loss: 0.6167\n",
            "Batch 125/239 | Loss: 0.3187\n",
            "Batch 130/239 | Loss: 0.2493\n",
            "Batch 135/239 | Loss: 0.6379\n",
            "Batch 140/239 | Loss: 0.4254\n",
            "Batch 145/239 | Loss: 0.1544\n",
            "Batch 150/239 | Loss: 0.3168\n",
            "Batch 155/239 | Loss: 0.3811\n",
            "Batch 160/239 | Loss: 0.2450\n",
            "Batch 165/239 | Loss: 0.3268\n",
            "Batch 170/239 | Loss: 0.3094\n",
            "Batch 175/239 | Loss: 0.2611\n",
            "Batch 180/239 | Loss: 0.3694\n",
            "Batch 185/239 | Loss: 0.3546\n",
            "Batch 190/239 | Loss: 0.2567\n",
            "Batch 195/239 | Loss: 0.3692\n",
            "Batch 200/239 | Loss: 0.4356\n",
            "Batch 205/239 | Loss: 0.3019\n",
            "Batch 210/239 | Loss: 0.2370\n",
            "Batch 215/239 | Loss: 0.5888\n",
            "Batch 220/239 | Loss: 0.4578\n",
            "Batch 225/239 | Loss: 0.4938\n",
            "Batch 230/239 | Loss: 0.2490\n",
            "Batch 235/239 | Loss: 0.5619\n",
            "Average Training LossL: 0.3539\n",
            "Validation Loss: 0.3863\n",
            "Frame-level moving/not-moving accuracy: 67.36% (threshold=0.5)\n",
            "Summary: train_loss=0.3539, val_loss=0.3863, val_acc=67.36%\n",
            "\n",
            "Epoch 3/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.2822\n",
            "Batch 5/239 | Loss: 0.1752\n",
            "Batch 10/239 | Loss: 0.4136\n",
            "Batch 15/239 | Loss: 0.5525\n",
            "Batch 20/239 | Loss: 0.3287\n",
            "Batch 25/239 | Loss: 0.2341\n",
            "Batch 30/239 | Loss: 0.3634\n",
            "Batch 35/239 | Loss: 0.2906\n",
            "Batch 40/239 | Loss: 0.3547\n",
            "Batch 45/239 | Loss: 0.4841\n",
            "Batch 50/239 | Loss: 0.3665\n",
            "Batch 55/239 | Loss: 0.3430\n",
            "Batch 60/239 | Loss: 0.3129\n",
            "Batch 65/239 | Loss: 0.1678\n",
            "Batch 70/239 | Loss: 0.4911\n",
            "Batch 75/239 | Loss: 0.2947\n",
            "Batch 80/239 | Loss: 0.1773\n",
            "Batch 85/239 | Loss: 0.3056\n",
            "Batch 90/239 | Loss: 0.4160\n",
            "Batch 95/239 | Loss: 0.2972\n",
            "Batch 100/239 | Loss: 0.2813\n",
            "Batch 105/239 | Loss: 0.4035\n",
            "Batch 110/239 | Loss: 0.5452\n",
            "Batch 115/239 | Loss: 0.2478\n",
            "Batch 120/239 | Loss: 0.5669\n",
            "Batch 125/239 | Loss: 0.2609\n",
            "Batch 130/239 | Loss: 0.4482\n",
            "Batch 135/239 | Loss: 0.2617\n",
            "Batch 140/239 | Loss: 0.1748\n",
            "Batch 145/239 | Loss: 0.1038\n",
            "Batch 150/239 | Loss: 0.0535\n",
            "Batch 155/239 | Loss: 0.7146\n",
            "Batch 160/239 | Loss: 0.0814\n",
            "Batch 165/239 | Loss: 0.0921\n",
            "Batch 170/239 | Loss: 0.2373\n",
            "Batch 175/239 | Loss: 0.4281\n",
            "Batch 180/239 | Loss: 0.2833\n",
            "Batch 185/239 | Loss: 0.4173\n",
            "Batch 190/239 | Loss: 0.2724\n",
            "Batch 195/239 | Loss: 0.0556\n",
            "Batch 200/239 | Loss: 0.2126\n",
            "Batch 205/239 | Loss: 0.1158\n",
            "Batch 210/239 | Loss: 0.1420\n",
            "Batch 215/239 | Loss: 0.6115\n",
            "Batch 220/239 | Loss: 0.2815\n",
            "Batch 225/239 | Loss: 0.4146\n",
            "Batch 230/239 | Loss: 0.2115\n",
            "Batch 235/239 | Loss: 0.3497\n",
            "Average Training LossL: 0.3065\n",
            "Validation Loss: 0.2955\n",
            "Frame-level moving/not-moving accuracy: 69.46% (threshold=0.5)\n",
            "Summary: train_loss=0.3065, val_loss=0.2955, val_acc=69.46%\n",
            "\n",
            "Epoch 4/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.1255\n",
            "Batch 5/239 | Loss: 0.2571\n",
            "Batch 10/239 | Loss: 0.3510\n",
            "Batch 15/239 | Loss: 0.4083\n",
            "Batch 20/239 | Loss: 0.1835\n",
            "Batch 25/239 | Loss: 0.1347\n",
            "Batch 30/239 | Loss: 0.1901\n",
            "Batch 35/239 | Loss: 0.1501\n",
            "Batch 40/239 | Loss: 0.1730\n",
            "Batch 45/239 | Loss: 0.4483\n",
            "Batch 50/239 | Loss: 0.1310\n",
            "Batch 55/239 | Loss: 0.0938\n",
            "Batch 60/239 | Loss: 0.1578\n",
            "Batch 65/239 | Loss: 0.2577\n",
            "Batch 70/239 | Loss: 0.4189\n",
            "Batch 75/239 | Loss: 0.5299\n",
            "Batch 80/239 | Loss: 0.0452\n",
            "Batch 85/239 | Loss: 0.3160\n",
            "Batch 90/239 | Loss: 0.2227\n",
            "Batch 95/239 | Loss: 0.2894\n",
            "Batch 100/239 | Loss: 0.3276\n",
            "Batch 105/239 | Loss: 0.5006\n",
            "Batch 110/239 | Loss: 0.2933\n",
            "Batch 115/239 | Loss: 0.1369\n",
            "Batch 120/239 | Loss: 0.4443\n",
            "Batch 125/239 | Loss: 0.0501\n",
            "Batch 130/239 | Loss: 0.1529\n",
            "Batch 135/239 | Loss: 0.2712\n",
            "Batch 140/239 | Loss: 0.3005\n",
            "Batch 145/239 | Loss: 0.2347\n",
            "Batch 150/239 | Loss: 0.2338\n",
            "Batch 155/239 | Loss: 0.4871\n",
            "Batch 160/239 | Loss: 0.3155\n",
            "Batch 165/239 | Loss: 0.2432\n",
            "Batch 170/239 | Loss: 0.3451\n",
            "Batch 175/239 | Loss: 0.3004\n",
            "Batch 180/239 | Loss: 0.2853\n",
            "Batch 185/239 | Loss: 0.5582\n",
            "Batch 190/239 | Loss: 0.1248\n",
            "Batch 195/239 | Loss: 0.5264\n",
            "Batch 200/239 | Loss: 0.4585\n",
            "Batch 205/239 | Loss: 0.3813\n",
            "Batch 210/239 | Loss: 0.2965\n",
            "Batch 215/239 | Loss: 0.2642\n",
            "Batch 220/239 | Loss: 0.3567\n",
            "Batch 225/239 | Loss: 0.2961\n",
            "Batch 230/239 | Loss: 0.1906\n",
            "Batch 235/239 | Loss: 0.2066\n",
            "Average Training LossL: 0.2821\n",
            "Validation Loss: 0.3025\n",
            "Frame-level moving/not-moving accuracy: 68.62% (threshold=0.5)\n",
            "Summary: train_loss=0.2821, val_loss=0.3025, val_acc=68.62%\n",
            "\n",
            "Epoch 5/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.2106\n",
            "Batch 5/239 | Loss: 0.3746\n",
            "Batch 10/239 | Loss: 0.3138\n",
            "Batch 15/239 | Loss: 0.3764\n",
            "Batch 20/239 | Loss: 0.2229\n",
            "Batch 25/239 | Loss: 0.3084\n",
            "Batch 30/239 | Loss: 0.1126\n",
            "Batch 35/239 | Loss: 0.1964\n",
            "Batch 40/239 | Loss: 0.0695\n",
            "Batch 45/239 | Loss: 0.1731\n",
            "Batch 50/239 | Loss: 0.1949\n",
            "Batch 55/239 | Loss: 0.1945\n",
            "Batch 60/239 | Loss: 0.0903\n",
            "Batch 65/239 | Loss: 0.1213\n",
            "Batch 70/239 | Loss: 0.0669\n",
            "Batch 75/239 | Loss: 0.2655\n",
            "Batch 80/239 | Loss: 0.2291\n",
            "Batch 85/239 | Loss: 0.1616\n",
            "Batch 90/239 | Loss: 0.4192\n",
            "Batch 95/239 | Loss: 0.0598\n",
            "Batch 100/239 | Loss: 0.3247\n",
            "Batch 105/239 | Loss: 0.2002\n",
            "Batch 110/239 | Loss: 0.3352\n",
            "Batch 115/239 | Loss: 0.1645\n",
            "Batch 120/239 | Loss: 0.2641\n",
            "Batch 125/239 | Loss: 0.1029\n",
            "Batch 130/239 | Loss: 0.2439\n",
            "Batch 135/239 | Loss: 0.1843\n",
            "Batch 140/239 | Loss: 0.2087\n",
            "Batch 145/239 | Loss: 0.4874\n",
            "Batch 150/239 | Loss: 0.2411\n",
            "Batch 155/239 | Loss: 0.1583\n",
            "Batch 160/239 | Loss: 0.1656\n",
            "Batch 165/239 | Loss: 0.3462\n",
            "Batch 170/239 | Loss: 0.0613\n",
            "Batch 175/239 | Loss: 0.2695\n",
            "Batch 180/239 | Loss: 0.3114\n",
            "Batch 185/239 | Loss: 0.0560\n",
            "Batch 190/239 | Loss: 0.4485\n",
            "Batch 195/239 | Loss: 0.4142\n",
            "Batch 200/239 | Loss: 0.0696\n",
            "Batch 205/239 | Loss: 0.3348\n",
            "Batch 210/239 | Loss: 0.2224\n",
            "Batch 215/239 | Loss: 0.1360\n",
            "Batch 220/239 | Loss: 0.1361\n",
            "Batch 225/239 | Loss: 0.2159\n",
            "Batch 230/239 | Loss: 0.2161\n",
            "Batch 235/239 | Loss: 0.3080\n",
            "Average Training LossL: 0.2480\n",
            "Validation Loss: 0.2594\n",
            "Frame-level moving/not-moving accuracy: 75.73% (threshold=0.5)\n",
            "Summary: train_loss=0.2480, val_loss=0.2594, val_acc=75.73%\n",
            "\n",
            "Model saved as 'fasterrcnn_moving_detector_2.pth'.\n"
          ]
        }
      ],
      "source": [
        "#train and save trained model\n",
        "if __name__ == \"__main__\":\n",
        "    video_folder = r\"/content/drive/MyDrive/Raw Videos\" #Change this when swapping devices to your local Raw Video folder\n",
        "    xml_folder   = r\"/content/drive/MyDrive/Annotations\" #\n",
        "\n",
        "    #Collate function for object detection\n",
        "    def collate_fn(batch):\n",
        "        batch = [b for b in batch if b is not None]\n",
        "        frames = [b[0] for b in batch]\n",
        "        targets = [b[1] for b in batch]\n",
        "        return frames, targets\n",
        "\n",
        "    #Load full dataset (videos + XMLs)\n",
        "    full_dataset = BaseballVideoDataset(video_folder, xml_folder, resize=(1280, 720), frame_skip=1, scale_boxes=True)\n",
        "\n",
        "    #print a quick summary\n",
        "    print(f\"\\nDataset contains {len(full_dataset)} annotated frames across videos.\")\n",
        "\n",
        "    #Split train/test\n",
        "    n = len(full_dataset)\n",
        "    split = int(0.8 * n)\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [split, n - split])\n",
        "\n",
        "    #Train the model\n",
        "    trained_model = train_detector(\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        num_classes=3,\n",
        "        epochs=5,\n",
        "        lr=1e-4,\n",
        "        batch_size=4)\n",
        "\n",
        "    #Save trained model\n",
        "    torch.save(trained_model.state_dict(), \"fasterrcnn_moving_detector_2.0.pth\")\n",
        "    print(\"\\nModel saved as 'fasterrcnn_moving_detector_2.pth'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cba8c6c-63d7-4229-d627-61b17571ca4f",
        "id": "Ti6e1Kkse8Ru"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No XML found for IMG_8226_jared.mov\n",
            "No XML found for IMG_8124_joe.mov\n",
            "No XML found for IMG_8028_joel.mov\n",
            "No XML found for IMG_8139_joe.mov\n",
            "No XML found for IMG_8123_scott.mov\n",
            "No XML found for IMG_8027_joel.mov\n",
            "No XML found for IMG_8243_jared.mov\n",
            "No XML found for IMG_8063_scott.mov\n",
            "No XML found for IMG_8030_patrick.mov\n",
            "No XML found for IMG_8255_zach.mov\n",
            "No XML found for IMG_8121_scott.mov\n",
            "No XML found for IMG_8138_joe.mov\n",
            "No XML found for IMG_8242_jared.mov\n",
            "No XML found for IMG_8241_jared.mov\n",
            "No XML found for IMG_8140_joe.mov\n",
            "No XML found for IMG_8122_scott.mov\n",
            "No XML found for IMG_8029_joel.mov\n",
            "No XML found for IMG_7999_joel.mov\n",
            "No XML found for IMG_8252_zach.mov\n",
            "No XML found for IMG_8923_souleymane.mov\n",
            "No XML found for IMG_8947_souleymane.mov\n",
            "No XML found for IMG_8257_zach.mov\n",
            "No XML found for IMG_9198_joel.mov\n",
            "No XML found for IMG_8256_zach.mov\n",
            "No XML found for IMG_8924_souleymane.mov\n",
            "No XML found for IMG_8946_souleymane.mov\n",
            "\n",
            " Found 21 videos with matching XMLs in /content/drive/MyDrive/Raw Videos\n",
            "\n",
            "Preloading videos and indexing frames...\n",
            "\n",
            "Loading video: IMG_7943_khem.mov\n",
            "Loaded 51 frames -> kept 51 after skipping\n",
            "\n",
            "Frame 51 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
            "Frame 52 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
            "Frame 53 out of range for IMG_7943_khem.mov (video has 51 frames) — skipping\n",
            "   [1/21] Loaded IMG_7943_khem.mov (51 frames, 54 annotated)\n",
            "\n",
            "Loading video: IMG_7919_dusty.mov\n",
            "Loaded 50 frames -> kept 50 after skipping\n",
            "\n",
            "   [2/21] Loaded IMG_7919_dusty.mov (50 frames, 50 annotated)\n",
            "\n",
            "Loading video: dusty_1.mov\n",
            "Loaded 76 frames -> kept 76 after skipping\n",
            "\n",
            "   [3/21] Loaded dusty_1.mov (76 frames, 76 annotated)\n",
            "\n",
            "Loading video: IMG_7997_khem.mov\n",
            "Loaded 37 frames -> kept 37 after skipping\n",
            "\n",
            "   [4/21] Loaded IMG_7997_khem.mov (37 frames, 37 annotated)\n",
            "\n",
            "Loading video: IMG_8061_patrick.mov\n",
            "Loaded 54 frames -> kept 54 after skipping\n",
            "\n",
            "   [5/21] Loaded IMG_8061_patrick.mov (54 frames, 54 annotated)\n",
            "\n",
            "Loading video: IMG_7998_khem.mov\n",
            "Loaded 55 frames -> kept 55 after skipping\n",
            "\n",
            "   [6/21] Loaded IMG_7998_khem.mov (55 frames, 55 annotated)\n",
            "\n",
            "Loading video: IMG_8141_amarnath.mov\n",
            "Loaded 57 frames -> kept 57 after skipping\n",
            "\n",
            "   [7/21] Loaded IMG_8141_amarnath.mov (57 frames, 29 annotated)\n",
            "\n",
            "Loading video: IMG_8223_amarnath.mov\n",
            "Loaded 52 frames -> kept 52 after skipping\n",
            "\n",
            "   [8/21] Loaded IMG_8223_amarnath.mov (52 frames, 43 annotated)\n",
            "\n",
            "Loading video: IMG_7944_khem.mov\n",
            "Loaded 54 frames -> kept 54 after skipping\n",
            "\n",
            "   [9/21] Loaded IMG_7944_khem.mov (54 frames, 37 annotated)\n",
            "\n",
            "Loading video: IMG_7942_dusty.mov\n",
            "Loaded 57 frames -> kept 57 after skipping\n",
            "\n",
            "   [10/21] Loaded IMG_7942_dusty.mov (57 frames, 57 annotated)\n",
            "\n",
            "Loading video: IMG_7917_dusty.mov\n",
            "Loaded 60 frames -> kept 60 after skipping\n",
            "\n",
            "   [11/21] Loaded IMG_7917_dusty.mov (60 frames, 60 annotated)\n",
            "\n",
            "Loading video: IMG_8225_amarnath.mov\n",
            "Loaded 40 frames -> kept 40 after skipping\n",
            "\n",
            "   [12/21] Loaded IMG_8225_amarnath.mov (40 frames, 40 annotated)\n",
            "\n",
            "Loading video: IMG_8060_patrick.mov\n",
            "Loaded 58 frames -> kept 58 after skipping\n",
            "\n",
            "   [13/21] Loaded IMG_8060_patrick.mov (58 frames, 58 annotated)\n",
            "\n",
            "Loading video: IMG_8062_patrick.mov\n",
            "Loaded 53 frames -> kept 53 after skipping\n",
            "\n",
            "   [14/21] Loaded IMG_8062_patrick.mov (53 frames, 53 annotated)\n",
            "\n",
            "Loading video: IMG_8224_amarnath.mov\n",
            "Loaded 57 frames -> kept 57 after skipping\n",
            "\n",
            "   [15/21] Loaded IMG_8224_amarnath.mov (57 frames, 56 annotated)\n",
            "\n",
            "Loading video: IMG_7918_dusty.mov\n",
            "Loaded 76 frames -> kept 76 after skipping\n",
            "\n",
            "   [16/21] Loaded IMG_7918_dusty.mov (76 frames, 76 annotated)\n",
            "\n",
            "Loading video: IMG_9199_hugo.mov\n",
            "Loaded 81 frames -> kept 81 after skipping\n",
            "\n",
            "   [17/21] Loaded IMG_9199_hugo.mov (81 frames, 81 annotated)\n",
            "\n",
            "Loading video: IMG_9197_hugo.mov\n",
            "Loaded 70 frames -> kept 70 after skipping\n",
            "\n",
            "   [18/21] Loaded IMG_9197_hugo.mov (70 frames, 70 annotated)\n",
            "\n",
            "Loading video: IMG_9607_hugo.mov\n",
            "Loaded 43 frames -> kept 43 after skipping\n",
            "\n",
            "   [19/21] Loaded IMG_9607_hugo.mov (43 frames, 43 annotated)\n",
            "\n",
            "Loading video: IMG_9609_dusty.mov\n",
            "Loaded 102 frames -> kept 102 after skipping\n",
            "\n",
            "   [20/21] Loaded IMG_9609_dusty.mov (102 frames, 102 annotated)\n",
            "\n",
            "Loading video: IMG_9435_hugo.mov\n",
            "Loaded 66 frames -> kept 66 after skipping\n",
            "\n",
            "   [21/21] Loaded IMG_9435_hugo.mov (66 frames, 66 annotated)\n",
            "\n",
            "Finished indexing 1194 annotated frames from 21 videos.\n",
            "\n",
            "\n",
            "Dataset contains 1194 annotated frames across videos.\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 160M/160M [00:00<00:00, 215MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 4.2637\n",
            "Batch 5/239 | Loss: 0.6494\n",
            "Batch 10/239 | Loss: 0.8681\n",
            "Batch 15/239 | Loss: 0.4860\n",
            "Batch 20/239 | Loss: 0.6403\n",
            "Batch 25/239 | Loss: 0.4103\n",
            "Batch 30/239 | Loss: 0.5687\n",
            "Batch 35/239 | Loss: 0.5541\n",
            "Batch 40/239 | Loss: 0.2250\n",
            "Batch 45/239 | Loss: 0.3853\n",
            "Batch 50/239 | Loss: 0.2741\n",
            "Batch 55/239 | Loss: 0.4839\n",
            "Batch 60/239 | Loss: 0.3406\n",
            "Batch 65/239 | Loss: 0.8450\n",
            "Batch 70/239 | Loss: 0.1846\n",
            "Batch 75/239 | Loss: 0.5465\n",
            "Batch 80/239 | Loss: 0.6376\n",
            "Batch 85/239 | Loss: 0.4792\n",
            "Batch 90/239 | Loss: 0.4808\n",
            "Batch 95/239 | Loss: 0.3720\n",
            "Batch 100/239 | Loss: 0.5374\n",
            "Batch 105/239 | Loss: 0.5006\n",
            "Batch 110/239 | Loss: 0.1673\n",
            "Batch 115/239 | Loss: 0.5370\n",
            "Batch 120/239 | Loss: 0.4840\n",
            "Batch 125/239 | Loss: 0.5427\n",
            "Batch 130/239 | Loss: 0.4162\n",
            "Batch 135/239 | Loss: 0.5092\n",
            "Batch 140/239 | Loss: 0.3625\n",
            "Batch 145/239 | Loss: 0.2212\n",
            "Batch 150/239 | Loss: 0.4685\n",
            "Batch 155/239 | Loss: 0.3553\n",
            "Batch 160/239 | Loss: 0.5177\n",
            "Batch 165/239 | Loss: 0.5091\n",
            "Batch 170/239 | Loss: 0.4798\n",
            "Batch 175/239 | Loss: 0.3629\n",
            "Batch 180/239 | Loss: 0.3427\n",
            "Batch 185/239 | Loss: 0.4238\n",
            "Batch 190/239 | Loss: 0.3886\n",
            "Batch 195/239 | Loss: 0.3278\n",
            "Batch 200/239 | Loss: 0.5772\n",
            "Batch 205/239 | Loss: 0.5172\n",
            "Batch 210/239 | Loss: 0.6017\n",
            "Batch 215/239 | Loss: 0.4532\n",
            "Batch 220/239 | Loss: 0.4787\n",
            "Batch 225/239 | Loss: 0.8664\n",
            "Batch 230/239 | Loss: 0.5109\n",
            "Batch 235/239 | Loss: 0.5085\n",
            "Average Training LossL: 0.5098\n",
            "Validation Loss: 0.4220\n",
            "Frame-level moving/not-moving accuracy: 74.90% (threshold=0.5)\n",
            "Summary: train_loss=0.5098, val_loss=0.4220, val_acc=74.90%\n",
            "\n",
            "Epoch 2/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.3555\n",
            "Batch 5/239 | Loss: 0.1399\n",
            "Batch 10/239 | Loss: 0.3075\n",
            "Batch 15/239 | Loss: 0.4084\n",
            "Batch 20/239 | Loss: 0.2107\n",
            "Batch 25/239 | Loss: 0.3618\n",
            "Batch 30/239 | Loss: 0.5036\n",
            "Batch 35/239 | Loss: 0.3018\n",
            "Batch 40/239 | Loss: 0.4460\n",
            "Batch 45/239 | Loss: 0.2276\n",
            "Batch 50/239 | Loss: 0.2407\n",
            "Batch 55/239 | Loss: 0.4885\n",
            "Batch 60/239 | Loss: 0.4398\n",
            "Batch 65/239 | Loss: 0.2358\n",
            "Batch 70/239 | Loss: 0.4188\n",
            "Batch 75/239 | Loss: 0.3330\n",
            "Batch 80/239 | Loss: 0.4451\n",
            "Batch 85/239 | Loss: 0.4220\n",
            "Batch 90/239 | Loss: 0.1454\n",
            "Batch 95/239 | Loss: 0.1896\n",
            "Batch 100/239 | Loss: 0.3122\n",
            "Batch 105/239 | Loss: 0.3576\n",
            "Batch 110/239 | Loss: 0.5488\n",
            "Batch 115/239 | Loss: 0.0676\n",
            "Batch 120/239 | Loss: 0.6167\n",
            "Batch 125/239 | Loss: 0.3187\n",
            "Batch 130/239 | Loss: 0.2493\n",
            "Batch 135/239 | Loss: 0.6379\n",
            "Batch 140/239 | Loss: 0.4254\n",
            "Batch 145/239 | Loss: 0.1544\n",
            "Batch 150/239 | Loss: 0.3168\n",
            "Batch 155/239 | Loss: 0.3811\n",
            "Batch 160/239 | Loss: 0.2450\n",
            "Batch 165/239 | Loss: 0.3268\n",
            "Batch 170/239 | Loss: 0.3094\n",
            "Batch 175/239 | Loss: 0.2611\n",
            "Batch 180/239 | Loss: 0.3694\n",
            "Batch 185/239 | Loss: 0.3546\n",
            "Batch 190/239 | Loss: 0.2567\n",
            "Batch 195/239 | Loss: 0.3692\n",
            "Batch 200/239 | Loss: 0.4356\n",
            "Batch 205/239 | Loss: 0.3019\n",
            "Batch 210/239 | Loss: 0.2370\n",
            "Batch 215/239 | Loss: 0.5888\n",
            "Batch 220/239 | Loss: 0.4578\n",
            "Batch 225/239 | Loss: 0.4938\n",
            "Batch 230/239 | Loss: 0.2490\n",
            "Batch 235/239 | Loss: 0.5619\n",
            "Average Training LossL: 0.3539\n",
            "Validation Loss: 0.3863\n",
            "Frame-level moving/not-moving accuracy: 67.36% (threshold=0.5)\n",
            "Summary: train_loss=0.3539, val_loss=0.3863, val_acc=67.36%\n",
            "\n",
            "Epoch 3/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.2822\n",
            "Batch 5/239 | Loss: 0.1752\n",
            "Batch 10/239 | Loss: 0.4136\n",
            "Batch 15/239 | Loss: 0.5525\n",
            "Batch 20/239 | Loss: 0.3287\n",
            "Batch 25/239 | Loss: 0.2341\n",
            "Batch 30/239 | Loss: 0.3634\n",
            "Batch 35/239 | Loss: 0.2906\n",
            "Batch 40/239 | Loss: 0.3547\n",
            "Batch 45/239 | Loss: 0.4841\n",
            "Batch 50/239 | Loss: 0.3665\n",
            "Batch 55/239 | Loss: 0.3430\n",
            "Batch 60/239 | Loss: 0.3129\n",
            "Batch 65/239 | Loss: 0.1678\n",
            "Batch 70/239 | Loss: 0.4911\n",
            "Batch 75/239 | Loss: 0.2947\n",
            "Batch 80/239 | Loss: 0.1773\n",
            "Batch 85/239 | Loss: 0.3056\n",
            "Batch 90/239 | Loss: 0.4160\n",
            "Batch 95/239 | Loss: 0.2972\n",
            "Batch 100/239 | Loss: 0.2813\n",
            "Batch 105/239 | Loss: 0.4035\n",
            "Batch 110/239 | Loss: 0.5452\n",
            "Batch 115/239 | Loss: 0.2478\n",
            "Batch 120/239 | Loss: 0.5669\n",
            "Batch 125/239 | Loss: 0.2609\n",
            "Batch 130/239 | Loss: 0.4482\n",
            "Batch 135/239 | Loss: 0.2617\n",
            "Batch 140/239 | Loss: 0.1748\n",
            "Batch 145/239 | Loss: 0.1038\n",
            "Batch 150/239 | Loss: 0.0535\n",
            "Batch 155/239 | Loss: 0.7146\n",
            "Batch 160/239 | Loss: 0.0814\n",
            "Batch 165/239 | Loss: 0.0921\n",
            "Batch 170/239 | Loss: 0.2373\n",
            "Batch 175/239 | Loss: 0.4281\n",
            "Batch 180/239 | Loss: 0.2833\n",
            "Batch 185/239 | Loss: 0.4173\n",
            "Batch 190/239 | Loss: 0.2724\n",
            "Batch 195/239 | Loss: 0.0556\n",
            "Batch 200/239 | Loss: 0.2126\n",
            "Batch 205/239 | Loss: 0.1158\n",
            "Batch 210/239 | Loss: 0.1420\n",
            "Batch 215/239 | Loss: 0.6115\n",
            "Batch 220/239 | Loss: 0.2815\n",
            "Batch 225/239 | Loss: 0.4146\n",
            "Batch 230/239 | Loss: 0.2115\n",
            "Batch 235/239 | Loss: 0.3497\n",
            "Average Training LossL: 0.3065\n",
            "Validation Loss: 0.2955\n",
            "Frame-level moving/not-moving accuracy: 69.46% (threshold=0.5)\n",
            "Summary: train_loss=0.3065, val_loss=0.2955, val_acc=69.46%\n",
            "\n",
            "Epoch 4/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.1255\n",
            "Batch 5/239 | Loss: 0.2571\n",
            "Batch 10/239 | Loss: 0.3510\n",
            "Batch 15/239 | Loss: 0.4083\n",
            "Batch 20/239 | Loss: 0.1835\n",
            "Batch 25/239 | Loss: 0.1347\n",
            "Batch 30/239 | Loss: 0.1901\n",
            "Batch 35/239 | Loss: 0.1501\n",
            "Batch 40/239 | Loss: 0.1730\n",
            "Batch 45/239 | Loss: 0.4483\n",
            "Batch 50/239 | Loss: 0.1310\n",
            "Batch 55/239 | Loss: 0.0938\n",
            "Batch 60/239 | Loss: 0.1578\n",
            "Batch 65/239 | Loss: 0.2577\n",
            "Batch 70/239 | Loss: 0.4189\n",
            "Batch 75/239 | Loss: 0.5299\n",
            "Batch 80/239 | Loss: 0.0452\n",
            "Batch 85/239 | Loss: 0.3160\n",
            "Batch 90/239 | Loss: 0.2227\n",
            "Batch 95/239 | Loss: 0.2894\n",
            "Batch 100/239 | Loss: 0.3276\n",
            "Batch 105/239 | Loss: 0.5006\n",
            "Batch 110/239 | Loss: 0.2933\n",
            "Batch 115/239 | Loss: 0.1369\n",
            "Batch 120/239 | Loss: 0.4443\n",
            "Batch 125/239 | Loss: 0.0501\n",
            "Batch 130/239 | Loss: 0.1529\n",
            "Batch 135/239 | Loss: 0.2712\n",
            "Batch 140/239 | Loss: 0.3005\n",
            "Batch 145/239 | Loss: 0.2347\n",
            "Batch 150/239 | Loss: 0.2338\n",
            "Batch 155/239 | Loss: 0.4871\n",
            "Batch 160/239 | Loss: 0.3155\n",
            "Batch 165/239 | Loss: 0.2432\n",
            "Batch 170/239 | Loss: 0.3451\n",
            "Batch 175/239 | Loss: 0.3004\n",
            "Batch 180/239 | Loss: 0.2853\n",
            "Batch 185/239 | Loss: 0.5582\n",
            "Batch 190/239 | Loss: 0.1248\n",
            "Batch 195/239 | Loss: 0.5264\n",
            "Batch 200/239 | Loss: 0.4585\n",
            "Batch 205/239 | Loss: 0.3813\n",
            "Batch 210/239 | Loss: 0.2965\n",
            "Batch 215/239 | Loss: 0.2642\n",
            "Batch 220/239 | Loss: 0.3567\n",
            "Batch 225/239 | Loss: 0.2961\n",
            "Batch 230/239 | Loss: 0.1906\n",
            "Batch 235/239 | Loss: 0.2066\n",
            "Average Training LossL: 0.2821\n",
            "Validation Loss: 0.3025\n",
            "Frame-level moving/not-moving accuracy: 68.62% (threshold=0.5)\n",
            "Summary: train_loss=0.2821, val_loss=0.3025, val_acc=68.62%\n",
            "\n",
            "Epoch 5/5\n",
            "----------------------------\n",
            "Batch 0/239 | Loss: 0.2106\n",
            "Batch 5/239 | Loss: 0.3746\n",
            "Batch 10/239 | Loss: 0.3138\n",
            "Batch 15/239 | Loss: 0.3764\n",
            "Batch 20/239 | Loss: 0.2229\n",
            "Batch 25/239 | Loss: 0.3084\n",
            "Batch 30/239 | Loss: 0.1126\n",
            "Batch 35/239 | Loss: 0.1964\n",
            "Batch 40/239 | Loss: 0.0695\n",
            "Batch 45/239 | Loss: 0.1731\n",
            "Batch 50/239 | Loss: 0.1949\n",
            "Batch 55/239 | Loss: 0.1945\n",
            "Batch 60/239 | Loss: 0.0903\n",
            "Batch 65/239 | Loss: 0.1213\n",
            "Batch 70/239 | Loss: 0.0669\n",
            "Batch 75/239 | Loss: 0.2655\n",
            "Batch 80/239 | Loss: 0.2291\n",
            "Batch 85/239 | Loss: 0.1616\n",
            "Batch 90/239 | Loss: 0.4192\n",
            "Batch 95/239 | Loss: 0.0598\n",
            "Batch 100/239 | Loss: 0.3247\n",
            "Batch 105/239 | Loss: 0.2002\n",
            "Batch 110/239 | Loss: 0.3352\n",
            "Batch 115/239 | Loss: 0.1645\n",
            "Batch 120/239 | Loss: 0.2641\n",
            "Batch 125/239 | Loss: 0.1029\n",
            "Batch 130/239 | Loss: 0.2439\n",
            "Batch 135/239 | Loss: 0.1843\n",
            "Batch 140/239 | Loss: 0.2087\n",
            "Batch 145/239 | Loss: 0.4874\n",
            "Batch 150/239 | Loss: 0.2411\n",
            "Batch 155/239 | Loss: 0.1583\n",
            "Batch 160/239 | Loss: 0.1656\n",
            "Batch 165/239 | Loss: 0.3462\n",
            "Batch 170/239 | Loss: 0.0613\n",
            "Batch 175/239 | Loss: 0.2695\n",
            "Batch 180/239 | Loss: 0.3114\n",
            "Batch 185/239 | Loss: 0.0560\n",
            "Batch 190/239 | Loss: 0.4485\n",
            "Batch 195/239 | Loss: 0.4142\n",
            "Batch 200/239 | Loss: 0.0696\n",
            "Batch 205/239 | Loss: 0.3348\n",
            "Batch 210/239 | Loss: 0.2224\n",
            "Batch 215/239 | Loss: 0.1360\n",
            "Batch 220/239 | Loss: 0.1361\n",
            "Batch 225/239 | Loss: 0.2159\n",
            "Batch 230/239 | Loss: 0.2161\n",
            "Batch 235/239 | Loss: 0.3080\n",
            "Average Training LossL: 0.2480\n",
            "Validation Loss: 0.2594\n",
            "Frame-level moving/not-moving accuracy: 75.73% (threshold=0.5)\n",
            "Summary: train_loss=0.2480, val_loss=0.2594, val_acc=75.73%\n",
            "\n",
            "Model saved as 'fasterrcnn_moving_detector_2.pth'.\n"
          ]
        }
      ],
      "source": [
        "#train and save trained model\n",
        "if __name__ == \"__main__\":\n",
        "    video_folder = r\"/content/drive/MyDrive/Raw Videos\" #Change this when swapping devices to your local Raw Video folder\n",
        "    xml_folder   = r\"/content/drive/MyDrive/Annotations\" #Change this when swapping devices to your local Annotations folder\n",
        "\n",
        "    #Collate function for object detection\n",
        "    def collate_fn(batch):\n",
        "        batch = [b for b in batch if b is not None]\n",
        "        frames = [b[0] for b in batch]\n",
        "        targets = [b[1] for b in batch]\n",
        "        return frames, targets\n",
        "\n",
        "    #Load full dataset (videos + XMLs)\n",
        "    full_dataset = BaseballVideoDataset(video_folder, xml_folder, resize=(1280, 720), frame_skip=1, scale_boxes=True)\n",
        "\n",
        "    #print a quick summary\n",
        "    print(f\"\\nDataset contains {len(full_dataset)} annotated frames across videos.\")\n",
        "\n",
        "    #Split train/test\n",
        "    n = len(full_dataset)\n",
        "    split = int(0.8 * n)\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [split, n - split])\n",
        "\n",
        "    #Train the model\n",
        "    trained_model = train_detector(\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        num_classes=3,\n",
        "        epochs=5,\n",
        "        lr=1e-4,\n",
        "        batch_size=4)\n",
        "\n",
        "    #Save trained model\n",
        "    torch.save(trained_model.state_dict(), \"fasterrcnn_moving_detector_2.0.pth\")\n",
        "    print(\"\\nModel saved as 'fasterrcnn_moving_detector_2.pth'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKpgQM9h8KdT"
      },
      "outputs": [],
      "source": [
        "#link to .pth file in one drive, was too large to upload to github\n",
        "#https://uofnebraska-my.sharepoint.com/:u:/g/personal/51628718_nebraska_edu/IQCJ0x1cdpvdQJXmvP5vf1XLAcN6AUHbCMoGgel_l8_oEMM?e=qqaj9l\n",
        "\n",
        "#import script for trained model\n",
        "def load_trained_model(weights_path, num_classes=3, device=None):\n",
        "\n",
        "    # Select device\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Loading model on device: {device}\")\n",
        "\n",
        "    # Recreate model architecture\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # Load saved weights\n",
        "    state_dict = torch.load(weights_path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    # Move model to device and set eval mode\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Model loaded successfully from '{weights_path}'\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Hl90uoPC_AD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W8l07Cb7GH0"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q opencv-python-headless\n",
        "!pip install -q pillow\n",
        "!pip install -q numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCPd7PmOKsb1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df5f995f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow   # ← this line enables cv2_imshow in Colab\n",
        "import torchvision.transforms as T\n",
        "from google.colab import files\n",
        "\n",
        "# Provided function to load the model\n",
        "def load_trained_model(weights_path, num_classes=3, device=None):\n",
        "    # Select device\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Loading model on device: {device}\")\n",
        "\n",
        "    # Recreate model architecture\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # Load saved weights\n",
        "    state_dict = torch.load(weights_path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    # Move model to device and set eval mode\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Model loaded successfully from '{weights_path}'\")\n",
        "    return model\n",
        "\n",
        "# Function to process the video and draw boxes around moving baseball\n",
        "def process_video(video_path, output_path, weights_path=\"fasterrcnn_moving_detector.pth\"):\n",
        "    # Load the model\n",
        "    model = load_trained_model(weights_path)\n",
        "\n",
        "    # Open the input video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error opening video file: {video_path}\")\n",
        "        return\n",
        "\n",
        "    # Get video properties\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    print(f\"Original video size: {orig_width}x{orig_height}, FPS: {fps}\")\n",
        "\n",
        "    # Define resized dimensions\n",
        "    new_width = 1280\n",
        "    new_height = 720\n",
        "\n",
        "    # Create output video writer (using mp4v codec for .mp4; change to 'MOV ' if needing .mov)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(output_path, fourcc, fps, (new_width, new_height))\n",
        "\n",
        "    # Device from model\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Preprocessing transforms\n",
        "    transform = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    frame_count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "        print(f\"Processing frame {frame_count}...\")\n",
        "\n",
        "        # Resize the frame\n",
        "        frame_resized = cv2.resize(frame, (new_width, new_height))\n",
        "\n",
        "        # Convert to RGB and PIL Image\n",
        "        rgb_frame = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
        "        pil_image = Image.fromarray(rgb_frame)\n",
        "\n",
        "        # Apply transforms and move to device\n",
        "        image_tensor = transform(pil_image).to(device)\n",
        "\n",
        "        # Run inference\n",
        "        with torch.no_grad():\n",
        "            predictions = model([image_tensor])[0]\n",
        "\n",
        "        # Extract predictions\n",
        "        boxes = predictions['boxes'].cpu().numpy()\n",
        "        labels = predictions['labels'].cpu().numpy()\n",
        "        scores = predictions['scores'].cpu().numpy()\n",
        "\n",
        "        # Draw boxes for moving baseball (assume label 2 = moving; adjust if needed)\n",
        "        for i in range(len(scores)):\n",
        "            if scores[i] > 0.5 and labels[i] == 2:\n",
        "                x1, y1, x2, y2 = map(int, boxes[i])\n",
        "                # Draw red rectangle (BGR color)\n",
        "                cv2.rectangle(frame_resized, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)\n",
        "\n",
        "        # Write the annotated frame to output\n",
        "        writer.write(frame_resized)\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "    print(f\"Processing complete. Output saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1coldDcOK0S5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "207193ae"
      },
      "outputs": [],
      "source": [
        "def process_video_colab2(\n",
        "                       video_path,\n",
        "                       output_path=\"output_with_boxes_v2.mp4\",\n",
        "                       weights_path=\"fasterrcnn_moving_detector_2.pth\",\n",
        "                       general_conf_threshold=0.3,\n",
        "                       thrown_ball_label=3,\n",
        "                       thrown_ball_min_score=0.35): #thrown_ball_min_score to make the detection more sensitive.\n",
        "\n",
        "    # Load model (your existing function)\n",
        "    model = load_trained_model(weights_path, num_classes=3)\n",
        "    device = next(model.parameters()).device\n",
        "    print(f\"Model loaded on {device}\")\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error opening video\")\n",
        "        return\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    # Get original video dimensions\n",
        "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Use original dimensions for output video\n",
        "    width  = orig_width\n",
        "    height = orig_height\n",
        "\n",
        "    # Video writer to save the result\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    transform = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    print(\"Starting processing... We will display every frame and print detection info if a thrown ball is found.\")\n",
        "\n",
        "    frame_idx = 0\n",
        "    found_thrown_ball = False\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_idx += 1\n",
        "        # Do not resize the frame, use original\n",
        "        display_frame = frame.copy() # Use original frame for display and writing\n",
        "\n",
        "        # Prepare input (use original frame for inference as well)\n",
        "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        pil_image = Image.fromarray(rgb)\n",
        "        img_tensor = transform(pil_image).unsqueeze(0).to(device)\n",
        "\n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            pred = model(img_tensor)[0]\n",
        "\n",
        "        boxes  = pred['boxes'].cpu().numpy()\n",
        "        labels = pred['labels'].cpu().numpy()\n",
        "        scores = pred['scores'].cpu().numpy()\n",
        "\n",
        "        thrown_ball_detections_this_frame = []\n",
        "\n",
        "        for i, (box, label, score) in enumerate(zip(boxes, labels, scores)):\n",
        "            # Apply general confidence threshold for drawing all boxes\n",
        "            if score > general_conf_threshold:\n",
        "                x1, y1, x2, y2 = map(int, box)\n",
        "\n",
        "                color = (0, 255, 0) # Default green for other detections\n",
        "                label_text = f\"Class {label} ({score:.2f})\"\n",
        "\n",
        "                if label == thrown_ball_label:\n",
        "                    color = (0, 0, 255) # Red for thrown ball\n",
        "                    if score > thrown_ball_min_score:\n",
        "                        thrown_ball_detections_this_frame.append((box, score))\n",
        "                        found_thrown_ball = True\n",
        "\n",
        "                # Draw rectangle and put text for all confident detections\n",
        "                cv2.rectangle(display_frame, (x1, y1), (x2, y2), color, 3)\n",
        "                cv2.putText(display_frame, label_text, (x1, y1-8),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "\n",
        "        # Display every frame\n",
        "        cv2_imshow(display_frame)\n",
        "\n",
        "        # Print detection info only if a thrown ball is found in this frame\n",
        "        if thrown_ball_detections_this_frame:\n",
        "            # Get max score for the thrown ball label in this frame\n",
        "            max_thrown_ball_score = max([s for _, s in thrown_ball_detections_this_frame] or [0])\n",
        "            print(f\"Frame {frame_idx:4d} → Thrown ball (Class {thrown_ball_label}) detected with confidence: {max_thrown_ball_score:.3f}\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "        # Always write frame to output video\n",
        "        writer.write(display_frame)\n",
        "\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "    print(f\"\\nFinished! Video saved as → {output_path}\")\n",
        "    if found_thrown_ball:\n",
        "        print(\"Thrown ball (Class 2) successfully detected in some frames!\")\n",
        "    else:\n",
        "        print(\"Warning: No confident thrown ball (Class 2) found with the given thresholds. Try lowering `thrown_ball_min_score` or check if `thrown_ball_label` is correct.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "82c1b909",
        "outputId": "6bc1b265-ace8-4d1e-cd8d-c5fc16f59906"
      },
      "outputs": [],
      "source": [
        "# Example call for the new function\n",
        "process_video_colab2(\n",
        "    video_path=\"/content/drive/MyDrive/Raw Videos/IMG_8060_patrick.mov\",\n",
        "    output_path=\"IMG_8060.Box.mp4\",\n",
        "    weights_path=\"/content/drive/MyDrive/fasterrcnn_moving_detector_2.0.pth\",\n",
        "    general_conf_threshold=0.5,  # General threshold for any detection\n",
        "    thrown_ball_label=2,          # class for the 'thrown ball'\n",
        "    thrown_ball_min_score=0.5    # Specific confidence for the thrown ball\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}